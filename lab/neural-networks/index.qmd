---
title: An Introduction to Neural Networks
author: Jan Emmanuel Samson
email: jgsamson@up.edu.ph
date: 2024/10/16
last-modified: 2024/10/16
description: "Learning the building blocks of AI and machine learning."
categories:
  - machine learning
  - statistics
image: ../../assets/mamba-logo.png
toc: true
---

```{python}
#| echo: false
#| evaluate: true
import numpy as np
import matplotlib.pyplot as plt
from scipy.interpolate import make_interp_spline, BSpline
```

## Neurons

### Perceptrons

A perceptron can be viewed as a simple function that takes several binary inputs and produces a single bingary output:

![](http://neuralnetworksanddeeplearning.com/images/tikz0.png){fig-align="center"}

The three inputs $x_1$, $x_2$, $x_3$ are connected to the neuron by individual weights. The dot product of the input and weight vectors are computed and a certain threshold is used to decide if the neuron will output a 0 or a 1.


$$
\begin{equation}
\text{output} = 
  \begin{cases}
    0  & \text{if $\sum_j w_j x_j \le$ threshold} \\
    1  & \text{if $\sum_j w_j x_j$ > threshold} \\
  \end{cases}
\end{equation}
$$

### Sigmoid Neurons

The sigmoid neuron is conceptually similar to a perceptron, but allows for the network to adjust how it chooses the output by making smalling changes in its weights and biases. 

![](http://neuralnetworksanddeeplearning.com/images/tikz8.png){fig-align="center"}

Similar to the perceptron, the sigmoid neuron takes an vector of inputs associated with a set of weights. However, the output can take on a continuous value between 0 and 1 as described in the function below:

$$ σ(z) \equiv \frac{1}{1 + ℯ^{-z}} $$

The variable _z_ denotes the dot product between a vector of inputs and their corresponding weights in the network. As the value of _z_ approaches infinity, the output converges to zero. Conversely, the output approaches 1 as _z_ becomes more negative.

```{python}
#| echo: false
def sigmoid(z):
  return 1 / (1 + np.exp(-z))

vsigmoid = np.vectorize(sigmoid)

vmin = -5
vmax = 5
x = np.arange(vmin, vmax+1)
y = vsigmoid(x)
xnew = np.linspace(min(x), max(x), 300)

# interpolate values
spl = make_interp_spline(x, y, k=3)
smooth = spl(xnew)

fig, ax = plt.subplots(figsize=(8, 4.5))
plt.subplots_adjust(left=0.1, right=0.9)

# plot sigmoid function
ax.plot(xnew, smooth)
# ax.set_ylabel("output")
ax.set_xlabel("z")
plt.show()
```
## Cost Function

The cost function captures the error in the model. It gives us an idea of how accurate the current trained model is. A simple way to measure error is by computing the **mean squared error** (MSE), a quadratic function.

$$ C(w,b) \equiv \frac{1}{2n} \sum_x || y(x) - a ||^2$$

## Gradient Descent

Gradient descent is an algorithm used to minimize the computed cost/error of a network while training. This is done by tweaking the various weights and biases of the network by computing the first-order derivative of each weight/bias with respect to the cost value.

**Stochastic gradient descent** addresses the computational overhead of GD by spliting the dataset into subsets called _mini-batches_ and computing the gradient for these subsets.
