[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "BioHub",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "lab/edirect/index.html",
    "href": "lab/edirect/index.html",
    "title": "Entrez Utilities Quick Start",
    "section": "",
    "text": "The Entrez Programming Utilities (E-utilities) are a set of server-side programs that provide a stable interface into the Entrez query and database system at the NCBI. It provides a REST API for programmatic access to data hosted in the 38 databases covering a variety of biomedical data, including nucleotide and protein sequences, gene records, molecular structures, and biomedical literature.\n\n\n\n\n\n\n\nThe official help manual can be access here.\n\n\n\n\n\n\n\n\nUnique Identifiers\n\n\n\nEach data record stored in an Entrez database is associated with a unique identifier (UID).\n\n\n\nEntrez Unique Identifiers (UIDs) for selected databases.\n\n\nEntrez Database\nUID Common Name\nE-utility Database Name\n\n\n\n\nBioProject\nBioProject ID\nbioproject\n\n\nBioSample\nBioSample ID\nbiosample\n\n\nBooks\nBook ID\nbooks\n\n\nConserved Domains\nPSSM-ID\ncdd\n\n\ndbGaP\ndbGaP ID\ngap\n\n\ndbVar\ndbVar ID\ndbvar\n\n\nGene\nGene ID\ngene\n\n\nGEO Datasets\nGDS ID\ngds\n\n\nGEO Profiles\nGEO ID\ngeoprofiles\n\n\nHomoloGene\nHomoloGene ID\nhomologene\n\n\nMeSH\nMeSH ID\nmesh\n\n\nNCBI C++ Toolkit\nToolkit ID\ntoolkit\n\n\nNLM Catalog\nNLM Catalog ID\nnlmcatalog\n\n\nNucleotide\nGI number\nnuccore\n\n\nPopSet\nPopSet ID\npopset\n\n\nProbe\nProbe ID\nprobe\n\n\nProteins\nGI number\nproteins\n\n\nProtein Clusters\nProtein Cluser ID\nproteinclusters\n\n\nPubChem BioAssay\nAID\npcassay\n\n\nPubChem Compound\nCID\npccompound\n\n\nPubChem Substance\nSID\npcsubstance\n\n\nPubMed\nPMID\npubmed\n\n\nPubMed Central\nPMCID\npmc\n\n\nSNP\nrs number\nsnp\n\n\nSRA\nSRA ID\nsra\n\n\nStructure\nMMDB-ID\nstructure\n\n\nTaxonomy\nTaxID\ntaxonomy\n\n\n\nThe list of E-utility database names can also be retrieved by running the einfo command:\neinfo -email $EMAIL -dbs | sort"
  },
  {
    "objectID": "lab/edirect/index.html#introduction-to-the-e-utilities",
    "href": "lab/edirect/index.html#introduction-to-the-e-utilities",
    "title": "Entrez Utilities Quick Start",
    "section": "",
    "text": "The Entrez Programming Utilities (E-utilities) are a set of server-side programs that provide a stable interface into the Entrez query and database system at the NCBI. It provides a REST API for programmatic access to data hosted in the 38 databases covering a variety of biomedical data, including nucleotide and protein sequences, gene records, molecular structures, and biomedical literature.\n\n\n\n\n\n\n\nThe official help manual can be access here.\n\n\n\n\n\n\n\n\nUnique Identifiers\n\n\n\nEach data record stored in an Entrez database is associated with a unique identifier (UID).\n\n\n\nEntrez Unique Identifiers (UIDs) for selected databases.\n\n\nEntrez Database\nUID Common Name\nE-utility Database Name\n\n\n\n\nBioProject\nBioProject ID\nbioproject\n\n\nBioSample\nBioSample ID\nbiosample\n\n\nBooks\nBook ID\nbooks\n\n\nConserved Domains\nPSSM-ID\ncdd\n\n\ndbGaP\ndbGaP ID\ngap\n\n\ndbVar\ndbVar ID\ndbvar\n\n\nGene\nGene ID\ngene\n\n\nGEO Datasets\nGDS ID\ngds\n\n\nGEO Profiles\nGEO ID\ngeoprofiles\n\n\nHomoloGene\nHomoloGene ID\nhomologene\n\n\nMeSH\nMeSH ID\nmesh\n\n\nNCBI C++ Toolkit\nToolkit ID\ntoolkit\n\n\nNLM Catalog\nNLM Catalog ID\nnlmcatalog\n\n\nNucleotide\nGI number\nnuccore\n\n\nPopSet\nPopSet ID\npopset\n\n\nProbe\nProbe ID\nprobe\n\n\nProteins\nGI number\nproteins\n\n\nProtein Clusters\nProtein Cluser ID\nproteinclusters\n\n\nPubChem BioAssay\nAID\npcassay\n\n\nPubChem Compound\nCID\npccompound\n\n\nPubChem Substance\nSID\npcsubstance\n\n\nPubMed\nPMID\npubmed\n\n\nPubMed Central\nPMCID\npmc\n\n\nSNP\nrs number\nsnp\n\n\nSRA\nSRA ID\nsra\n\n\nStructure\nMMDB-ID\nstructure\n\n\nTaxonomy\nTaxID\ntaxonomy\n\n\n\nThe list of E-utility database names can also be retrieved by running the einfo command:\neinfo -email $EMAIL -dbs | sort"
  },
  {
    "objectID": "lab/edirect/index.html#einfo",
    "href": "lab/edirect/index.html#einfo",
    "title": "Entrez Utilities Quick Start",
    "section": "EInfo",
    "text": "EInfo\nAccessed through the einfo command to display a list of NCBI databases. It can also display summary statistics and information about indexed fields and links in a specific database.\n\nArguments\nThe einfo command allows the following arguments:\n\n-dbs: Outputs a text list of NCBI databases which can be accessed by E-utilities.\n-db: Allow you to specify a database name about which you wish to receive information.\n-fields: Overrides the default XML output and prints a text list of indexed fields for the database specified by the -db parameter.\n-links: Override the default XML output and prints a text list of links (for use with the ELink utility) for the database specified in the -db parameter\n\n\n\nExamples\nGet a list of all available databases (truncated to the first 10), sorted alphabetically:\neinfo -dbs | sort | head -n 10\nDisplay a list of PubMed indexed fields:\neinfo -db pubmed -fields"
  },
  {
    "objectID": "lab/edirect/index.html#esearch",
    "href": "lab/edirect/index.html#esearch",
    "title": "Entrez Utilities Quick Start",
    "section": "ESearch",
    "text": "ESearch\nUses the esearch command to search an NCBI database for a query and finds the unique identifiers for all records that match the search query.\n\nArguments\nThe esearch command allows the following arguments:\n\n-db: The database to be searched.\n-query: The search string enclosed in double quotes.\n-sort: Specifies the order in which your results will be sorted.\n-datetype: When limiting by date, specifies which of the several date fields on a record is used to limit.\n-days: Limits results to records with dates no more than the specified number of days in the past, based on the date field specified in the -datetype argument.\n-mindate/-maxdate: Limits results to records with dates in a certain range, based on the date field specified in the -datetype argument.\n-spell: Corrects misspellings in your search query.\n-log: Also show the full E-utilities URL and the full query translation.\n\n\n\nExamples\nSearch for articles on the African Swine Fever Virus:\nesearch -db pubmed -query \"African Swine Fever Virus\"\nThe query returns XML-formatted output where the &lt;Count&gt;3064&lt;/Count&gt; tag indicates the number of relevant results.\nAdd a date filter and sort by the Relevance field:\nesearch -db pubmed -query \"African Swine Fever Virus\" \\\n  -datetype PDAT -mindate 2020 -maxdate 2024 \\\n  -sort \"Relevance\""
  },
  {
    "objectID": "lab/edirect/index.html#efetch",
    "href": "lab/edirect/index.html#efetch",
    "title": "Entrez Utilities Quick Start",
    "section": "EFetch",
    "text": "EFetch\nThe efetch command is used by the EFetch utility to download records from an NCBI database in a specified format.\n\nArguments\nThe efetch command allows the following arguments:\n\n-db: The database from which to retrieve records.\n-id: One or more UIDs, separated by commas.\n-format: Specifies the format in which you wish to display the records.\n\nValid formats include:\n\nuid: display a list of UIDs\nabstract: displays the Abstract view, formatted in plain text\nmedline: displays the MEDLINE view, including the field indicators\nxml: displays the full PubMed XML\n\n\n\n\n\nExamples\nRetrieve 10 articles on ASFV:\nesearch -db pubmed -query \"African Swine Fever Virus\" \\\n  | efetch -format uid | head -n 20 \\\n  | esummary -db pubmed \\\n  | xtract -pattern DocumentSummary -element Id Title -year PubDate \\\n  | align-columns\nRetrieve a first 20 UIDs of available ASFV genomes:\nesearch -db nuccore -query \"African Swine Fever Virus genome\" \\\n  | efetch -format uid | head"
  },
  {
    "objectID": "lab/edirect/index.html#accessory-programs",
    "href": "lab/edirect/index.html#accessory-programs",
    "title": "Entrez Utilities Quick Start",
    "section": "Accessory Programs",
    "text": "Accessory Programs\nAdditional helper programs are included in the EDirect package. They help eliminate the need to write custom scripts to further filter out query results.\n\nnquire\n\nRetrieves data from remote servers with URLs constructed from command line arguments.\n\ntramsute\n\nConverts a concatenate stream of JSON objects or other structured formats into XML.\n\nxtract\n\nUses waypoints to navigate a complex XML hierarchy and obtain data values by field name."
  },
  {
    "objectID": "lab/edirect/sections/databases.html",
    "href": "lab/edirect/sections/databases.html",
    "title": "BioHub",
    "section": "",
    "text": "The Entrez System Identifies Database Records Using UIDs\n\n\n\n\n\n\nUnique Identifiers\n\n\n\nEach data record stored in an Entrez database is associated with a unique identifier (UID).\n\n\n\nEntrez Unique Identifiers (UIDs) for selected databases.\n\n\nEntrez Database\nUID Common Name\nE-utility Database Name\n\n\n\n\nBioProject\nBioProject ID\nbioproject\n\n\nBioSample\nBioSample ID\nbiosample\n\n\nBooks\nBook ID\nbooks\n\n\nConserved Domains\nPSSM-ID\ncdd\n\n\ndbGaP\ndbGaP ID\ngap\n\n\ndbVar\ndbVar ID\ndbvar\n\n\nGene\nGene ID\ngene\n\n\nGEO Datasets\nGDS ID\ngds\n\n\nGEO Profiles\nGEO ID\ngeoprofiles\n\n\nHomoloGene\nHomoloGene ID\nhomologene\n\n\nMeSH\nMeSH ID\nmesh\n\n\nNCBI C++ Toolkit\nToolkit ID\ntoolkit\n\n\nNLM Catalog\nNLM Catalog ID\nnlmcatalog\n\n\nNucleotide\nGI number\nnuccore\n\n\nPopSet\nPopSet ID\npopset\n\n\nProbe\nProbe ID\nprobe\n\n\nProteins\nGI number\nproteins\n\n\nProtein Clusters\nProtein Cluser ID\nproteinclusters\n\n\nPubChem BioAssay\nAID\npcassay\n\n\nPubChem Compound\nCID\npccompound\n\n\nPubChem Substance\nSID\npcsubstance\n\n\nPubMed\nPMID\npubmed\n\n\nPubMed Central\nPMCID\npmc\n\n\nSNP\nrs number\nsnp\n\n\nSRA\nSRA ID\nsra\n\n\nStructure\nMMDB-ID\nstructure\n\n\nTaxonomy\nTaxID\ntaxonomy\n\n\n\nThe list of E-utility database names can also be retrieved by running the einfo command:\neinfo -email $EMAIL -dbs | sort\n\n\n\n\n Back to top"
  },
  {
    "objectID": "lab/edirect/sections/installation.html",
    "href": "lab/edirect/sections/installation.html",
    "title": "BioHub",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "lab/neural_networks/index.html",
    "href": "lab/neural_networks/index.html",
    "title": "An Introduction to Neural Networks",
    "section": "",
    "text": "A perceptron can be viewed as a simple function that takes several binary inputs and produces a single bingary output:\n\n\n\n\n\nThe three inputs \\(x_1\\), \\(x_2\\), \\(x_3\\) are connected to the neuron by individual weights. The dot product of the input and weight vectors are computed and a certain threshold is used to decide if the neuron will output a 0 or a 1.\n\\[\n\\begin{equation}\n\\text{output} =\n  \\begin{cases}\n    0  & \\text{if $\\sum_j w_j x_j \\le$ threshold} \\\\\n    1  & \\text{if $\\sum_j w_j x_j$ &gt; threshold} \\\\\n  \\end{cases}\n\\end{equation}\n\\]\n\n\n\nThe sigmoid neuron is conceptually similar to a perceptron, but allows for the network to adjust how it chooses the output by making smalling changes in its weights and biases.\n\n\n\n\n\nSimilar to the perceptron, the sigmoid neuron takes an vector of inputs associated with a set of weights. However, the output can take on a continuous value between 0 and 1 as described in the function below:\n\\[ σ(z) \\equiv \\frac{1}{1 + ℯ^{-z}} \\]\nThe variable z denotes the dot product between a vector of inputs and their corresponding weights in the network. As the value of z approaches infinity, the output converges to zero. Conversely, the output approaches 1 as z becomes more negative."
  },
  {
    "objectID": "lab/neural_networks/index.html#neurons",
    "href": "lab/neural_networks/index.html#neurons",
    "title": "An Introduction to Neural Networks",
    "section": "",
    "text": "A perceptron can be viewed as a simple function that takes several binary inputs and produces a single bingary output:\n\n\n\n\n\nThe three inputs \\(x_1\\), \\(x_2\\), \\(x_3\\) are connected to the neuron by individual weights. The dot product of the input and weight vectors are computed and a certain threshold is used to decide if the neuron will output a 0 or a 1.\n\\[\n\\begin{equation}\n\\text{output} =\n  \\begin{cases}\n    0  & \\text{if $\\sum_j w_j x_j \\le$ threshold} \\\\\n    1  & \\text{if $\\sum_j w_j x_j$ &gt; threshold} \\\\\n  \\end{cases}\n\\end{equation}\n\\]\n\n\n\nThe sigmoid neuron is conceptually similar to a perceptron, but allows for the network to adjust how it chooses the output by making smalling changes in its weights and biases.\n\n\n\n\n\nSimilar to the perceptron, the sigmoid neuron takes an vector of inputs associated with a set of weights. However, the output can take on a continuous value between 0 and 1 as described in the function below:\n\\[ σ(z) \\equiv \\frac{1}{1 + ℯ^{-z}} \\]\nThe variable z denotes the dot product between a vector of inputs and their corresponding weights in the network. As the value of z approaches infinity, the output converges to zero. Conversely, the output approaches 1 as z becomes more negative."
  },
  {
    "objectID": "lab/neural_networks/index.html#cost-function",
    "href": "lab/neural_networks/index.html#cost-function",
    "title": "An Introduction to Neural Networks",
    "section": "Cost Function",
    "text": "Cost Function\nThe cost function captures the error in the model. It gives us an idea of how accurate the current trained model is. A simple way to measure error is by computing the mean squared error (MSE), a quadratic function.\n\\[ C(w,b) \\equiv \\frac{1}{2n} \\sum_x || y(x) - a ||^2\\]"
  },
  {
    "objectID": "lab/neural_networks/index.html#gradient-descent",
    "href": "lab/neural_networks/index.html#gradient-descent",
    "title": "An Introduction to Neural Networks",
    "section": "Gradient Descent",
    "text": "Gradient Descent\nGradient descent is an algorithm used to minimize the computed cost/error of a network while training. This is done by tweaking the various weights and biases of the network by computing the first-order derivative of each weight/bias with respect to the cost value.\nStochastic gradient descent addresses the computational overhead of GD by spliting the dataset into subsets called mini-batches and computing the gradient for these subsets."
  },
  {
    "objectID": "notebook/retrieving_biological_data/index.html",
    "href": "notebook/retrieving_biological_data/index.html",
    "title": "Retrieving Biological Data",
    "section": "",
    "text": "What are databases?\nDatabases are organized collections of structured information retrieved from various sources such as experiments, sensors, telemetry data and publications, to name a few. Information within the database is physically stored in computer systems and a set of programs manage how data flows in and out of the database. A server actively listens for requests sent by other computers to access the data. Upon validation, the server retrieves the data from the database and sends the data (response) to the requesting computer (or client).\nBiological databases\nIn biology, high-throughput experiments had led to the generation of data in the scale of petabytes. An organizational scheme is needed to easily retrieve the data used for the generation of actionable insights, which is the ultimate goal of any analysis. Biological databases contain data generated from experiments in the fields as genomics, proteomics, metabolomics, epigenetics, and many more. Each data point reflects a particular attribute of a biological entity such as the function of a gene, structure of a protein, expression of a transcript, localization of mutation, etc.\nWhy should I care?\nPrior to any analysis, data pertinent to the research question must first be collected in a useable form. Retrieving data may be as simple a going to NCBI, searching for a particular accession or query, and clicking a button to download a file in a specific format (e.g., FASTA, GFF). However, this approach is not scalable when attempting to process largescale data. As such, bioinformaticians have developed infrastructure for streamlining the process of data collection, allowing researchers from all backgrounds to retrieve the information they require in a straighforward manner.\nThe aim of this write-up is to introduce you to a few command-line-based tools for downloading biological data."
  },
  {
    "objectID": "notebook/retrieving_biological_data/index.html#sources-of-biological-data",
    "href": "notebook/retrieving_biological_data/index.html#sources-of-biological-data",
    "title": "Retrieving Biological Data",
    "section": "",
    "text": "What are databases?\nDatabases are organized collections of structured information retrieved from various sources such as experiments, sensors, telemetry data and publications, to name a few. Information within the database is physically stored in computer systems and a set of programs manage how data flows in and out of the database. A server actively listens for requests sent by other computers to access the data. Upon validation, the server retrieves the data from the database and sends the data (response) to the requesting computer (or client).\nBiological databases\nIn biology, high-throughput experiments had led to the generation of data in the scale of petabytes. An organizational scheme is needed to easily retrieve the data used for the generation of actionable insights, which is the ultimate goal of any analysis. Biological databases contain data generated from experiments in the fields as genomics, proteomics, metabolomics, epigenetics, and many more. Each data point reflects a particular attribute of a biological entity such as the function of a gene, structure of a protein, expression of a transcript, localization of mutation, etc.\nWhy should I care?\nPrior to any analysis, data pertinent to the research question must first be collected in a useable form. Retrieving data may be as simple a going to NCBI, searching for a particular accession or query, and clicking a button to download a file in a specific format (e.g., FASTA, GFF). However, this approach is not scalable when attempting to process largescale data. As such, bioinformaticians have developed infrastructure for streamlining the process of data collection, allowing researchers from all backgrounds to retrieve the information they require in a straighforward manner.\nThe aim of this write-up is to introduce you to a few command-line-based tools for downloading biological data."
  },
  {
    "objectID": "notebook/retrieving_biological_data/index.html#how-to-get-genome-data",
    "href": "notebook/retrieving_biological_data/index.html#how-to-get-genome-data",
    "title": "Retrieving Biological Data",
    "section": "How to get genome data",
    "text": "How to get genome data\nData is distributed via various repositories. The most commonly used ones are:\n\n\n\n\n\n\nflowchart LR\n  1A[\"`**NCBI**\n  Genbank/RefSeq`\"]\n  1B[\"`Use numbers\n  **bio, efetch**`\"]\n  \n  2A[\"`**Ensembl**\n  **UCSC**\n  **FlyBase, WormBase**\n  Releases`\"]\n  2B[\"`Use URL\n  **curl, wget, aria2**\n  **genomepy, refgenie**`\"]\n  \n  3A[\"`**NCBI Assemblies**\n  RefSeq genomes`\"]\n  3B[\"`Use accession\n  **datasets**`\"]\n  \n  4A[\"`**Independent\n  Data Tools**`\"]\n  4B[\"`**genomepy**\n  **refgenie**`\"]\n  \n  5[\"`**File Formats**\n  FASTA, GFF, GTF, BED`\"]\n  \n  1A --&gt; 1B --&gt; 5\n  2A --&gt; 2B --&gt; 5\n  3A --&gt; 3B --&gt; 5\n  4A --&gt; 4B --&gt; 5\n\n\n\n\n\n\n\nIn addition, there are software packages such as refgenie and genomepy that can be used to download and manage reference genomes.\n\nSearch for metadata\nThe bio package is a CLI-based tool used for bioinformatics exploration. It contains commands for downloading, manipulating, and transforming sequence data. If you have an NCBI-based accession number, you can use the bio search command to get information on it.\n# Use a GenBank accession\n1bio search AF086833\n\n# Use an SRA accession\n2bio search SRR1553425\n\n# Use a RefSeq assembly ID\n3bio search GCA_000005845\n\n# Use a query string\nbio search ecoli\n\n1\n\nSearches GenBank\n\n2\n\nSearches SRA\n\n3\n\nSearches NCBI Genome\n\n\nRunning the bio search command will return a JSON-formatted string which contains the metadata for a particular record. Use the --csv flag to output the metadata in a comma-delimited format. Similarly, use the --csv if you want tab-delimited data.\nRunning bio search SRR1553425 would produce the following:\n\n\n[\n    {\n        \"run_accession\": \"SRR1553425\",\n        \"sample_accession\": \"SAMN02951957\",\n        \"sample_alias\": \"EM110\",\n        \"sample_description\": \"Zaire ebolavirus genome sequencing from 2014 outbreak in Sierra Leone\",\n        \"first_public\": \"2015-06-05\",\n        \"country\": \"Sierra Leone\",\n        \"scientific_name\": \"Zaire ebolavirus\",\n        \"fastq_bytes\": \"111859282;119350609\",\n        \"base_count\": \"360534650\",\n        \"read_count\": \"1784825\",\n        \"library_name\": \"EM110_r1.ADXX\",\n        \"library_strategy\": \"RNA-Seq\",\n        \"library_source\": \"TRANSCRIPTOMIC\",\n        \"library_layout\": \"PAIRED\",\n        \"instrument_platform\": \"ILLUMINA\",\n        \"instrument_model\": \"Illumina HiSeq 2500\",\n        \"study_title\": \"Zaire ebolavirus Genome sequencing\",\n        \"fastq_url\": [\n            \"https://ftp.sra.ebi.ac.uk/vol1/fastq/SRR155/005/SRR1553425/SRR1553425_1.fastq.gz\",\n            \"https://ftp.sra.ebi.ac.uk/vol1/fastq/SRR155/005/SRR1553425/SRR1553425_2.fastq.gz\"\n        ],\n        \"info\": \"112 MB, 119 MB files; 1.8 million reads; 360.5 million sequenced bases\"\n    }\n]\n\n\nWe can then use the jq processing tool to extract fields of interest. Extract the fastq_url list by running:\n\n! bio search SRR1553425 | jq \".[].fastq_url[]\"\n\n\"https://ftp.sra.ebi.ac.uk/vol1/fastq/SRR155/005/SRR1553425/SRR1553425_1.fastq.gz\"\n\"https://ftp.sra.ebi.ac.uk/vol1/fastq/SRR155/005/SRR1553425/SRR1553425_2.fastq.gz\"\n\n\n\n\nAccessing Genbank\nGenBank is the NIH genetic sequence database, an annotated collection of publicly available DNA sequences. If your data has a GenBank accession number such as AF086833, use the bio fetch command. By default, data is printed to stdout. Override this behavior by specifying the output filename with the -o flag or redirect the output to a file with the &gt; operator.\n# Accession id pointing to the record.\n1ACC=AF086833\n\n# Specify output with a flag.\n2bio fetch ${ACC} --format fasta -o ${ACC}.fa\n\n# Redirect the output to a file.\n3bio fetch ${ACC} --format gff &gt; ${ACC}.gff\n\n1\n\nStore accesssion ID as a variable.\n\n2\n\nDownload the sequence (FASTA) file.\n\n3\n\nDownload the annotation (GFF) file.\n\n\nLet us verify the download by viewing the first ten lines of the annotation file:\n\n\n##gff-version 3\n#!gff-spec-version 1.21\n#!processor NCBI annotwriter\n##sequence-region AF086833.2 1 18959\n##species https://www.ncbi.nlm.nih.gov/Taxonomy/Browser/wwwtax.cgi?id=128952\nAF086833.2  Genbank region  1   18959   .   +   .   ID=AF086833.2:1..18959;Dbxref=taxon:128952;gb-acronym=EBOV-May;gbkey=Src;mol_type=viral cRNA;strain=Mayinga\nAF086833.2  Genbank five_prime_UTR  1   55  .   +   .   ID=id-AF086833.2:1..55;Note=putative leader region;function=regulation or initiation of RNA replication;gbkey=5'UTR\nAF086833.2  Genbank gene    56  3026    .   +   .   ID=gene-NP;Name=NP;gbkey=Gene;gene=NP;gene_biotype=protein_coding\nAF086833.2  Genbank mRNA    56  3026    .   +   .   ID=rna-NP;Parent=gene-NP;gbkey=mRNA;gene=NP;product=nucleoprotein\nAF086833.2  Genbank exon    56  3026    .   +   .   ID=exon-NP-1;Parent=rna-NP;gbkey=mRNA;gene=NP;product=nucleoprotein\n\n\n\n\nDownload via URL\nIf you know the URL of a resource, you may use wget or curl to download the file. First, save the URL to a variable for referencing:\nURL=http://ftp.ensembl.org/pub/release-104/fasta/homo_sapiens/dna/Homo_sapiens.GRCh38.dna.chromosome.22.fa.gz\nUse curl:\ncurl ${URL} -o chr22.fa.gz\nOr use wget:\nwget -nc ${URL} -o chr22.fa.gz\nThe -nc flag would skip the download altogether if the file already exists. Use this flag to ensure that large files are not overwritten.\nFor large files, aria2 can be used for faster, multi-segmented downloads. The tool also supports checkpoints which allow you to restart interrupted downloads. Download aria2 from their website or use conda to install in an environment.\naria2c ${URL} \\ \n1  -x 5 \\\n2  -o chr22.fa.gz\n\n1\n\nConnect to the server with x connections.\n\n2\n\nSave the output to a file.\n\n\n\n\nHow to use NCBI datasets\nNCBI Datasets is a new resource designed by NCBI to gather data from across NCBI databases. The main entry point of the tool is the datasets command. Subcommands such as download or summary is then specified, followed by more subcommands to specify your query.\nIt seems to be the direction that NCBI wants to shepherd users towards. However, the nested structure of running the tool makes its use complicated and convoluted compared to other resources. NCBI is kind enough to give us a diagram for navigating the subcommands:\n\n\n\nSwiftwater hydra (Hydra vulgaris) has a taxonomy id of 6087 and RefSeq id of GCF_038396675.1. We can download its genome by running the following:\ndatasets download genome accession GCF_038396675.1\nBy default, genome is downloaded as a zipped file named ncbi_dataset. The structure of the directory is seen below.\nncbi_dataset\n└── data\n    ├── assembly_data_report.jsonl\n    ├── dataset_catalog.json\n    └── GCF_038396675.1\n        └── GCF_038396675.1_HydraT2T_AEP_genomic.fn\n\n\nHow to access Ensembl\nEnsembl operates on numbered releases. For example, release 104 was published on March 30, 2021. Data can be retrieved by navigating the file tree in the provided FTP server. Alternatively, you can invoke curl, wget, or aria2c directly on each file.\n# Get the FASTA file from chromosome 22 of the human genome\nURL=http://ftp.ensembl.org/pub/release-104/fasta/homo_sapiens/dna/Homo_sapiens.GRCh38.dna.chromosome.22.fa.gz\n\n# Use curl go download the data\n1curl ${URL} | gunzip -c &gt; refs/chr22.fa\n\n1\n\nDownload from URL and decompress the FASTA file."
  },
  {
    "objectID": "notebook/retrieving_biological_data/index.html#how-to-use-refgenie",
    "href": "notebook/retrieving_biological_data/index.html#how-to-use-refgenie",
    "title": "Retrieving Biological Data",
    "section": "How to use refgenie",
    "text": "How to use refgenie\nRefgenie is a command-line tool that can be used to download and manage reference genomes, and to build and manage custom genome assets. It also provides a Python interface for programmatic access to genome assets.\n\n\n\n\n\n\nflowchart LR\n  list[\"`**refgenie**\n  list`\"]\n  pull[\"`**refgenie**\n  pull`\"]\n  seek[\"`**refgenie**\n  seek`\"]\n  list --&gt; pull --&gt; seek\n\n\n\n\n\n\n\n\nInstallation\nRefgenie can be installed as a Python package using pip:\n# Install using pip.\npip install refgenie\n\n# Install using pipx.\npipx install refgenie\nor conda:\nconda install -c conda-forge refgenie\n\n# Use mamba/micromamba instead of conda\nmicromamba install refgenie\n\n\nCreate a config file\nrefgenie requires a configuration file that lists the resources in the form of a yaml file. For that you need to select a directory that will store the downloaded resources. The path to the config file is saved as the REFGENIE shell environment variable which will be used for initialization:\n# Path pointing to refgenie config file.\nexport REFGENIE=~/refs/config.yaml\n\n# Load the REFGENIE variable when launching a shell instance\necho \"export REFGENIE=~/refs/config.yml\" &gt;&gt; ~/.bashrc\nsource ~/.bashrc\n\n# Run initialization\nrefgenie init\nThe refgenie tool is now ready to be used to download and manage reference genomes.\n\n\nUsing refgenie\nA list of pre-built assets from a remote server can be displayed with listr:\n\n! refgenie listr\n\nzsh:1: command not found: refgenie\n\n\nGenome data is fetched using the pull command:\nrefgenie pull hg38/bwa_index\nThe seek command displays the path of the downloaded file:\nrefgenie seek hg38/bwa_index\nList local genome assets:\nrefgenie list\nUse command substitution to store the genome path to a variable:\n# Retrieve the human reference genome\nrefgenie pull hg38/fasta\n\n# Save path of reference genome to a variable\nREF=$(refgenie seek hg38/fasta)\nThen use the resulting path in downstream tools:\n# Generate statistics for the human reference genome\nseqkit stats ${REF}\nSubscribe to the iGenomes server which hosts additional reference genomes and genome assets.\nrefgenie subscribe -s http://igenomes.databio.org/"
  },
  {
    "objectID": "notebook/retrieving_biological_data/index.html#using-genomepy",
    "href": "notebook/retrieving_biological_data/index.html#using-genomepy",
    "title": "Retrieving Biological Data",
    "section": "Using genomepy",
    "text": "Using genomepy\n\nHow to use genomepy\ngenomepy is another tool designed for searching and downloading genomic data. It can be used to:\n\nsearch available data\nshow the available metadata\nautomatically download, preprocess\ngenerate optional aligned indexes\n\nCurrently, genomepy supports Ensembl, UCSC, NCBI, and GENCODE.\n\n\n\n\n\n\nflowchart LR\n  genomepy[\"`**genomepy**`\"]\n  commands{\"`**search**\n  **install**\n  **annotation**`\"}\n  storage[\"`files stored in\n  **$home/local/share/genomes**`\"]\n  \n  genomepy --&gt; commands --&gt; storage\n\n\n\n\n\n\n\nSee also: genomepy documentation\n\n\nInstallation\nInstall using micromamba:\nmicromamba install genomepy\nInstall using pip or pipx:\npipx install genomepy\n\n\nUsing genomepy\nUse the search command to query genomes by name or accession:\ngenomepy search ecoli &gt; ecoli_query_results.txt\nA genome index will be downloaded upon invoking the search command for the first time. Hence, the initial search may take a while depending on your connection speed. As seen from the log below, assembly summaries are fetched from multiple databases (GENCODE, UCSC, Ensembl, NCBI).\n05:28:31 | INFO | Downloading assembly summaries from GENCODE\n05:29:54 | INFO | Downloading assembly summaries from UCSC\n05:30:05 | INFO | Downloading assembly summaries from Ensembl\n05:30:43 | INFO | Downloading assembly summaries from NCBI, this will take a while...\ngenbank_historical: 73.0k genomes [00:06, 11.1k genomes/s]\nrefseq_historical: 85.6k genomes [00:05, 16.6k genomes/s]\ngenbank: 2.39M genomes [02:11, 18.1k genomes/s]\nrefseq: 378k genomes [00:28, 13.0k genomes/s] \nThe results look like so:\nname                 provider accession         tax_id annotation species                                  other_info                              \n                                                        n r e k   &lt;- UCSC options (see help)                                                       \nEcoliT22_2.0         NCBI     GCF_000247665.3      562     ✓      Escherichia coli O157:H43 str. T22       BAYGEN                                  \nEcoli.O104:H4.LB226692_2.0 NCBI     GCA_000215685.3      562     ✓      Escherichia coli O104:H4 str. LB226692   Life Technologies                       \nEcoli.O104:H4.01-09591_1.0 NCBI     GCA_000221065.2      562     ✓      Escherichia coli O104:H4 str. 01-09591   Life Technologies                       \nEcoli_C227-11_1.0    NCBI     GCA_000220805.2      562     ✓      Escherichia coli O104:H4 str. C227-11    PacBio                                  \necoli009             NCBI     GCA_900607665.1      562     ✓      Escherichia coli                         BIOZENTRUM, UNIVERSITY OF BASEL         \necoli006             NCBI     GCA_900607465.1      562     ✓      Escherichia coli                         BIOZENTRUM, UNIVERSITY OF BASEL         \necoli008             NCBI     GCA_900607535.1      562     ✓      Escherichia coli                         BIOZENTRUM, UNIVERSITY OF BASEL         \necoli017             NCBI     GCA_900608025.1      562     ✓      Escherichia coli                         BIOZENTRUM, UNIVERSITY OF BASEL         \necoli025             NCBI     GCA_900608175.1      562     ✓      Escherichia coli                         BIOZENTRUM, UNIVERSITY OF BASEL         \necoli022             NCBI     GCA_900608105.1      562     ✓      Escherichia coli                         BIOZENTRUM, UNIVERSITY OF BASEL         \necoli015             NCBI     GCA_900607975.1      562     ✓      Escherichia coli                         BIOZENTRUM, UNIVERSITY OF BASEL         \necoli013             NCBI     GCA_900607805.1      562     ✓      Escherichia coli                         BIOZENTRUM, UNIVERSITY OF BASEL       \nEntries under the name field can be used to download the genome:\ngenomepy install ecoli009\nBy default, the downloaded genomes will be found in the ~/.local/share/genomes directory. For our example, the directory named ecoli009 contains the genome data and other relevant files:\n\n\n/home/dagsdags/.local/share/genomes\n└── ecoli009\n    ├── assembly_report.txt\n    ├── ecoli009.fa\n    ├── ecoli009.fa.fai\n    ├── ecoli009.fa.sizes\n    ├── ecoli009.gaps.bed\n    └── README.txt\n\n1 directory, 6 files"
  },
  {
    "objectID": "notebook/retrieving_biological_data/index.html#how-to-get-fastq-data",
    "href": "notebook/retrieving_biological_data/index.html#how-to-get-fastq-data",
    "title": "Retrieving Biological Data",
    "section": "How to get FASTQ data",
    "text": "How to get FASTQ data\nPublibashed FASTQ files are stored in the bashort Read Archive (SRA). Access to SRA can be diagrammed like so:\n\n\n\n\n\n\nflowchart LR\n  fq[\"`**FASTQ FILES**`\"]\n  srr[\"`**SRR number**`\"] --&gt; srr2[\"`Find URL and metadata\n  web, **bio**, **ffq**`\"]\n  sra[\"`**SRA**\n  bashort Read Archive`\"] --&gt; sra2[\"`Use SRR number\n  **fastq-dump**`\"] --&gt; fq\n  ensembl[\"`**Ensembl**\n  Sequence archive`\"] --&gt; ensembl2[\"`Find URL\n  **curl**, **wget**, **aria2**`\"]\n  com[\"`**Commercial**\n  Google, Amazon\n  Users pay to download`\"] --&gt; com2[\"`Custom tools\n  **gsutil**, **aws**`\"] --&gt; fq\n\n\n\n\n\n\n\nThe sratools suite from NCBI provides fastq-dump and fasterq-dump to download read data from SRA accessions. In later versions of sratools, fasterq-dump is the preferred tool for fetching read data as demonstrated below:\n# Store accession number and number of reads\nACC=SRR1553425\nN=10000\n\n# Create reads directory\nmkdir reads\n\n# Fetch reads from accession\nfasterq-dump --split-3 -X ${N} -O reads ${ACC}\nHowever this method is clunky and fragile, often failing to fetch the required data due to errors that are cryptically communicated to the user. An alternative method is to retrieve the URLs that point to the data and download locally using wget, curl or aria2. Use bio search to retrieve metadata on the SRA accession and parse using jq.\n\n! bio search SRR1553425 | jq -r '.[].fastq_url[]'\n\nhttps://ftp.sra.ebi.ac.uk/vol1/fastq/SRR155/005/SRR1553425/SRR1553425_1.fastq.gz\nhttps://ftp.sra.ebi.ac.uk/vol1/fastq/SRR155/005/SRR1553425/SRR1553425_2.fastq.gz\n\n\n\nUsing the SRA Explorer\nThe SRA Explorer is a web-based tool developed by Phil Lewis aimed to make SRA data more accessible. It allows you to search for datasets and view metadata. The link can be accessed here:\n\nhttps://sra-explorer.info/\n\n\n\nUsing the NCBI website\nYou can also visit NCBI’s SRA repository here to download sequencing read data.\n\n\nHow to download multiple runs\nAll data from a project can be queried using bio search, parsed using csvcut, and concurrently downloaded using parallel or aria2c:\nThe project id encapsulates all the details in a sequencing experiment. Pass the project id as an argument to the bio search command to view the SRR accessions.\n# Access the project metadata and save as a CSV file\nbio search PRJNA257197 --csv &gt; project.csv\nThe truncated output is as follows:\n\n\nSRR1553426,SAMN02951957,EM110,Zaire ebolavirus genome sequencing from 2014 outbreak in Sierra Leone,2015-06-05,Sierra Leone,Zaire ebolavirus,156165683;174523161,494259458,2446829,EM110.FCH9,RNA-Seq,TRANSCRIPTOMIC,PAIRED,ILLUMINA,Illumina HiSeq 2500,Zaire ebolavirus Genome sequencing,\"['https://ftp.sra.ebi.ac.uk/vol1/fastq/SRR155/006/SRR1553426/SRR1553426_1.fastq.gz', 'https://ftp.sra.ebi.ac.uk/vol1/fastq/SRR155/006/SRR1553426/SRR1553426_2.fastq.gz']\",\"156 MB, 175 MB files; 2.4 million reads; 494.3 million sequenced bases\"\nSRR1553428,SAMN02951958,EM111,Zaire ebolavirus genome sequencing from 2014 outbreak in Sierra Leone,2015-06-05,Sierra Leone,Zaire ebolavirus,46711942;48176644,147915308,732254,EM111.FCH9,RNA-Seq,TRANSCRIPTOMIC,PAIRED,ILLUMINA,Illumina HiSeq 2500,Zaire ebolavirus Genome sequencing,\"['https://ftp.sra.ebi.ac.uk/vol1/fastq/SRR155/008/SRR1553428/SRR1553428_1.fastq.gz', 'https://ftp.sra.ebi.ac.uk/vol1/fastq/SRR155/008/SRR1553428/SRR1553428_2.fastq.gz']\",\"47 MB, 48 MB files; 0.7 million reads; 147.9 million sequenced bases\"\nSRR1553432,SAMN02951960,EM113,Zaire ebolavirus genome sequencing from 2014 outbreak in Sierra Leone,2015-06-05,Sierra Leone,Zaire ebolavirus,36110922;36939803,97376524,482062,EM113.FCH9,RNA-Seq,TRANSCRIPTOMIC,PAIRED,ILLUMINA,Illumina HiSeq 2500,Zaire ebolavirus Genome sequencing,\"['https://ftp.sra.ebi.ac.uk/vol1/fastq/SRR155/002/SRR1553432/SRR1553432_1.fastq.gz', 'https://ftp.sra.ebi.ac.uk/vol1/fastq/SRR155/002/SRR1553432/SRR1553432_2.fastq.gz']\",\"36 MB, 37 MB files; 0.5 million reads; 97.4 million sequenced bases\"\nSRR1553441,SAMN02951965,EM124.1,Zaire ebolavirus genome sequencing from 2014 outbreak in Sierra Leone,2015-06-05,Sierra Leone,Zaire ebolavirus,34581102;38122466,107291088,531144,EM124.1_r1.ADXX,RNA-Seq,TRANSCRIPTOMIC,PAIRED,ILLUMINA,Illumina HiSeq 2500,Zaire ebolavirus Genome sequencing,\"['https://ftp.sra.ebi.ac.uk/vol1/fastq/SRR155/001/SRR1553441/SRR1553441_1.fastq.gz', 'https://ftp.sra.ebi.ac.uk/vol1/fastq/SRR155/001/SRR1553441/SRR1553441_2.fastq.gz']\",\"35 MB, 38 MB files; 0.5 million reads; 107.3 million sequenced bases\"\nSRR1553442,SAMN02951965,EM124.1,Zaire ebolavirus genome sequencing from 2014 outbreak in Sierra Leone,2015-06-05,Sierra Leone,Zaire ebolavirus,69550566;70497774,214233928,1060564,EM124.1.FCH9,RNA-Seq,TRANSCRIPTOMIC,PAIRED,ILLUMINA,Illumina HiSeq 2500,Zaire ebolavirus Genome sequencing,\"['https://ftp.sra.ebi.ac.uk/vol1/fastq/SRR155/002/SRR1553442/SRR1553442_1.fastq.gz', 'https://ftp.sra.ebi.ac.uk/vol1/fastq/SRR155/002/SRR1553442/SRR1553442_2.fastq.gz']\",\"70 MB, 70 MB files; 1.1 million reads; 214.2 million sequenced bases\"\n\n\nOnly the accession numbers are needed to download the reads. From the project file, we extract the first column corresponding to the accession, and use this as input to fastq-dump. The parallel tool enables us to simultaneously download multiple accession at once.\n# Extract the first column and download concurrently using parallel\n1cat project.csv | \\\n2    csvcut -c 1 | \\\n3    head -n 3 | \\\n4    parallel \"fastq-dump -F --split-files -O data {}\"\n\n1\n\nPrint to standard output.\n\n2\n\nFilter only the first column.\n\n3\n\nFilter first three rows.\n\n4\n\nDownload reads for each of the three accessions."
  },
  {
    "objectID": "notebook/retrieving_biological_data/sections/url_download.html",
    "href": "notebook/retrieving_biological_data/sections/url_download.html",
    "title": "BioHub",
    "section": "",
    "text": "Download via URL\nIf you know the URL of a resource, you may use wget or curl to download the file. First, save the URL to a variable for referencing:\nURL=http://ftp.ensembl.org/pub/release-104/fasta/homo_sapiens/dna/Homo_sapiens.GRCh38.dna.chromosome.22.fa.gz\nUse curl:\ncurl ${URL} -o chr22.fa.gz\nOr use wget:\nwget -nc ${URL} -o chr22.fa.gz\nThe -nc flag would skip the download altogether if the file already exists. Use this flag to ensure that large files are not overwritten.\nFor large files, aria2 can be used for faster, multi-segmented downloads. The tool also supports checkpoints which allow you to restart interrupted downloads. Download aria2 from their website or use conda to install in an environment.\naria2c ${URL} \\ \n1  -x 5 \\\n2  -o chr22.fa.gz\n\n1\n\nConnect to the server with x connections.\n\n2\n\nSave the output to a file.\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "notebook/retrieving_biological_data/sections/overview.html",
    "href": "notebook/retrieving_biological_data/sections/overview.html",
    "title": "Sources of biological data",
    "section": "",
    "text": "What are databases?\nDatabases are organized collections of structured information retrieved from various sources such as experiments, sensors, telemetry data and publications, to name a few. Information within the database is physically stored in computer systems and a set of programs manage how data flows in and out of the database. A server actively listens for requests sent by other computers to access the data. Upon validation, the server retrieves the data from the database and sends the data (response) to the requesting computer (or client).\nBiological databases\nIn biology, high-throughput experiments had led to the generation of data in the scale of petabytes. An organizational scheme is needed to easily retrieve the data used for the generation of actionable insights, which is the ultimate goal of any analysis. Biological databases contain data generated from experiments in the fields as genomics, proteomics, metabolomics, epigenetics, and many more. Each data point reflects a particular attribute of a biological entity such as the function of a gene, structure of a protein, expression of a transcript, localization of mutation, etc.\nWhy should I care?\nPrior to any analysis, data pertinent to the research question must first be collected in a useable form. Retrieving data may be as simple a going to NCBI, searching for a particular accession or query, and clicking a button to download a file in a specific format (e.g., FASTA, GFF). However, this approach is not scalable when attempting to process largescale data. As such, bioinformaticians have developed infrastructure for streamlining the process of data collection, allowing researchers from all backgrounds to retrieve the information they require in a straighforward manner.\nThe aim of this write-up is to introduce you to a few command-line-based tools for downloading biological data."
  },
  {
    "objectID": "notebook/retrieving_biological_data/sections/overview.html#how-to-get-genome-data",
    "href": "notebook/retrieving_biological_data/sections/overview.html#how-to-get-genome-data",
    "title": "Sources of biological data",
    "section": "How to get genome data",
    "text": "How to get genome data\nData is distributed via various repositories. The most commonly used ones are:\n\n\n\n\n\n\nflowchart LR\n  1A[\"`**NCBI**\n  Genbank/RefSeq`\"]\n  1B[\"`Use numbers\n  **bio, efetch**`\"]\n  \n  2A[\"`**Ensembl**\n  **UCSC**\n  **FlyBase, WormBase**\n  Releases`\"]\n  2B[\"`Use URL\n  **curl, wget, aria2**\n  **genomepy, refgenie**`\"]\n  \n  3A[\"`**NCBI Assemblies**\n  RefSeq genomes`\"]\n  3B[\"`Use accession\n  **datasets**`\"]\n  \n  4A[\"`**Independent\n  Data Tools**`\"]\n  4B[\"`**genomepy**\n  **refgenie**`\"]\n  \n  5[\"`**File Formats**\n  FASTA, GFF, GTF, BED`\"]\n  \n  1A --&gt; 1B --&gt; 5\n  2A --&gt; 2B --&gt; 5\n  3A --&gt; 3B --&gt; 5\n  4A --&gt; 4B --&gt; 5\n\n\n\n\n\n\n\nIn addition, there are software packages such as refgenie and genomepy that can be used to download and manage reference genomes.\n\nSearch for metadata\nThe bio package is a CLI-based tool used for bioinformatics exploration. It contains commands for downloading, manipulating, and transforming sequence data. If you have an NCBI-based accession number, you can use the bio search command to get information on it.\n# Use a GenBank accession\n1bio search AF086833\n\n# Use an SRA accession\n2bio search SRR1553425\n\n# Use a RefSeq assembly ID\n3bio search GCA_000005845\n\n# Use a query string\nbio search ecoli\n\n1\n\nSearches GenBank\n\n2\n\nSearches SRA\n\n3\n\nSearches NCBI Genome\n\n\nRunning the bio search command will return a JSON-formatted string which contains the metadata for a particular record. Use the --csv flag to output the metadata in a comma-delimited format. Similarly, use the --csv if you want tab-delimited data.\nRunning bio search SRR1553425 would produce the following:\n\n\n[\n    {\n        \"run_accession\": \"SRR1553425\",\n        \"sample_accession\": \"SAMN02951957\",\n        \"sample_alias\": \"EM110\",\n        \"sample_description\": \"Zaire ebolavirus genome sequencing from 2014 outbreak in Sierra Leone\",\n        \"first_public\": \"2015-06-05\",\n        \"country\": \"Sierra Leone\",\n        \"scientific_name\": \"Zaire ebolavirus\",\n        \"fastq_bytes\": \"111859282;119350609\",\n        \"base_count\": \"360534650\",\n        \"read_count\": \"1784825\",\n        \"library_name\": \"EM110_r1.ADXX\",\n        \"library_strategy\": \"RNA-Seq\",\n        \"library_source\": \"TRANSCRIPTOMIC\",\n        \"library_layout\": \"PAIRED\",\n        \"instrument_platform\": \"ILLUMINA\",\n        \"instrument_model\": \"Illumina HiSeq 2500\",\n        \"study_title\": \"Zaire ebolavirus Genome sequencing\",\n        \"fastq_url\": [\n            \"https://ftp.sra.ebi.ac.uk/vol1/fastq/SRR155/005/SRR1553425/SRR1553425_1.fastq.gz\",\n            \"https://ftp.sra.ebi.ac.uk/vol1/fastq/SRR155/005/SRR1553425/SRR1553425_2.fastq.gz\"\n        ],\n        \"info\": \"112 MB, 119 MB files; 1.8 million reads; 360.5 million sequenced bases\"\n    }\n]\n\n\nWe can then use the jq processing tool to extract fields of interest. Extract the fastq_url list by running:\n\n! bio search SRR1553425 | jq \".[].fastq_url[]\"\n\n\"https://ftp.sra.ebi.ac.uk/vol1/fastq/SRR155/005/SRR1553425/SRR1553425_1.fastq.gz\"\n\"https://ftp.sra.ebi.ac.uk/vol1/fastq/SRR155/005/SRR1553425/SRR1553425_2.fastq.gz\""
  },
  {
    "objectID": "notebook/retrieving_biological_data/sections/ncbi_datasets.html",
    "href": "notebook/retrieving_biological_data/sections/ncbi_datasets.html",
    "title": "BioHub",
    "section": "",
    "text": "How to use NCBI datasets\nNCBI Datasets is a new resource designed by NCBI to gather data from across NCBI databases. The main entry point of the tool is the datasets command. Subcommands such as download or summary is then specified, followed by more subcommands to specify your query.\nIt seems to be the direction that NCBI wants to shepherd users towards. However, the nested structure of running the tool makes its use complicated and convoluted compared to other resources. NCBI is kind enough to give us a diagram for navigating the subcommands:\n\n\n\nSwiftwater hydra (Hydra vulgaris) has a taxonomy id of 6087 and RefSeq id of GCF_038396675.1. We can download its genome by running the following:\ndatasets download genome accession GCF_038396675.1\nBy default, genome is downloaded as a zipped file named ncbi_dataset. The structure of the directory is seen below.\nncbi_dataset\n└── data\n    ├── assembly_data_report.jsonl\n    ├── dataset_catalog.json\n    └── GCF_038396675.1\n        └── GCF_038396675.1_HydraT2T_AEP_genomic.fn\n\n\n\n\n Back to top"
  },
  {
    "objectID": "notebook/retrieving_biological_data/sections/ensembl.html",
    "href": "notebook/retrieving_biological_data/sections/ensembl.html",
    "title": "BioHub",
    "section": "",
    "text": "How to access Ensembl\nEnsembl operates on numbered releases. For example, release 104 was published on March 30, 2021. Data can be retrieved by navigating the file tree in the provided FTP server. Alternatively, you can invoke curl, wget, or aria2c directly on each file.\n# Get the FASTA file from chromosome 22 of the human genome\nURL=http://ftp.ensembl.org/pub/release-104/fasta/homo_sapiens/dna/Homo_sapiens.GRCh38.dna.chromosome.22.fa.gz\n\n# Use curl go download the data\n1curl ${URL} | gunzip -c &gt; refs/chr22.fa\n\n1\n\nDownload from URL and decompress the FASTA file.\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "notebook/motif_finding/index.html",
    "href": "notebook/motif_finding/index.html",
    "title": "Finding Motifs in a Genetic Haystack",
    "section": "",
    "text": "In bacteria, short regions in the genome called DnaA boxes act as the binding site for DnaA proteins to initiate replication. The function of these segments of DNA can be experimentally verified by wet-lab scientists. However, the process of motif discovery is often costly and is prone to systematic errors.\n\n\n\n\n\n\n\nRetrieved from\nEmerging Pathogens Institute\nThe observation that regulatory regions often contain short motifs make discovery amenable to computation techniques. I will be using the Vibrio cholerae genome to illustrate some approaches to motif discovery.\n\n\nThe genome for Vibrio cholerae can be downloaded here.\nLoad and inspect the genome:\n\nfilepath = \"/home/dagsdags/gh-repos/biohub/data/vibrio-cholerae.txt\"\nwith open(filepath, \"r\") as handle:\n  genome = handle.read().strip()\n  n = 80\n  lines = [genome[i:i+n] for i in range(0, 300, n)]\n  for line in lines:\n    print(line)\n\nACAATGAGGTCACTATGTTCGAGCTCTTCAAACCGGCTGCGCATACGCAGCGGCTGCCATCCGATAAGGTGGACAGCGTC\nTATTCACGCCTTCGTTGGCAACTTTTCATCGGTATTTTTGTTGGCTATGCAGGCTACTATTTGGTTCGTAAGAACTTTAG\nCTTGGCAATGCCTTACCTGATTGAACAAGGCTTTAGTCGTGGCGATCTGGGTGTGGCTCTCGGTGCGGTTTCAATCGCGT\nATGGTCTGTCTAAATTTTTGATGGGGAACGTCTCTGACCGTTCTAACCCGCGCTACTTTCTGAGTGCAGGTCTACTCCTT\n\n\n\nexecute-dir: project\n\n\n\n/home/dagsdags/gh-repos/biohub\n\n\n\n\n\nA k-mer is a string of length k. It is a substring derived from a longer sequence. For example, the string AGTAGT has three unique 3-mers: AGT, GTA, and TAG. The 3-mer AGT occurs twice in the sequence while the rest are only found once.\nA simple way to check for motifs is to find frequently-occurring k-mers in a region of the genome. If the count of a particular k-mer is surprisingly large, we can argue that the k-mer pattern may be of biological significance.\nTo count the frequency of a particular k-mer, we can create a window of size k and slide it across a query sequence. At each position, we compare the the current window with the k-mer and increment a counter if they match.\n\ndef pattern_count(text: str, pattern: str) -&gt; int:\n  \"\"\"Counts the number of times `pattern` occurs as a substring of `text`.\"\"\"\n  count: int = 0\n  k = len(pattern)\n  for i in range(len(text)-k+1):\n    if text[i:i+k] == pattern:\n      count += 1\n  return count\n\nA more generalized approach is to set the k parameter and count all k-mers in the query sequence. We then extract the most frequent k-mer by filtering for the substring with the highest count.\n\ndef frequency_table(text: str, k: int) -&gt; dict[str, int]:\n  \"\"\"Returns a frequency table containing all k-mer counts.\"\"\"\n  kmer_counts = {}\n  for i in range(len(text)-k+1):\n    kmer = text[i:i+k]\n    if kmer in kmer_counts:\n      kmer_counts[kmer] += 1\n    else:\n      kmer_counts[kmer] = 1\n  # Return all k-mers sorted by count\n  return kmer_counts\n\ndef get_most_frequent_kmer(freq_map: dict[str, int]) -&gt; tuple[int, list[str]]:\n  \"\"\"Returns the most frequent k-mer and its counts from a frequency table. \n  If there are multiple most-frequent k-mers, return them in a list.\"\"\"\n  max_count = max(freq_map.values())\n  most_freq_kmers = []\n  for kmer, count in freq_map.items():\n    if count == max_count:\n      most_freq_kmers.append(kmer)\n  return (max_count, most_freq_kmers)\n\nUsing the V. cholerae origin of replication:\n\natcaatgatcaacgtaagcttctaagcatgatcaaggtgctcacacagtttatccacaac\nctgagtggatgacatcaagataggtcgttgtatctccttcctctcgtactctcatgacca\ncggaaagatgatcaagagaggatgatttcttggccatatcgcaatgaatacttgtgactt\ngtgcttccaattgacatcttcagcgccatattgcgctggccaaggtgacggagcgggatt\nacgaaagcatgatcatggctgttgttctgtttatcttgttttgactgagacttgttagga\ntagacggtttttcatcactgactagccaaagccttactctgcctgacatcgaccgtaaat\ntgataatgaatttacatgcttccgcgacgatttacctcttgatcatcgatccgattgaag\natcttcaattgttaattctcttgcctcgactcatagccatgatgagctcttgatcatgtt\ntccttaaccctctattttttacggaagaatgatcaagctgctgctcttgatcatcgtttc\n\nLet us find the most frequent frequent k-mers in the range \\(k = 3\\) and \\(k = 9\\):\n\nk_list = list(range(3, 10))\nkmer_counts = {}\nfor k in k_list:\n  freq_map = frequency_table(ori, k)\n  kmer_counts[k] = get_most_frequent_kmer(freq_map)\n\n\n\n{25: 'TGA', 12: 'ATGA', 8: 'TGATCA', 5: 'ATGATCA', 4: 'ATGATCAA', 3: 'ATGATCAAG, CTCTTGATC, TCTTGATCA, CTTGATCAT'}\n\n\nThe 3-mer TGA is observed in 25 unique positions of the bacterial genome. Is this a suprising observation?\nAssuming that the occurrence of a nucleotide at a given position is independent of all other position, we can expect the frequency of a k-mer to decrease as k increases. Given our frequency table, it is a bit surprising that four 9-mers occur three times within a 500-bp window. Furthermore, it seems that the the 9-mers ATGATCAAG and CTTGATCAT are reverse complements of each other.\nSince DNA has a sense of directionality, a protein factor may bind to either forward or reverse strand of a regulatory region. In this case, ATGATCAAG and CTTGATCAT may represent the same protein binding site. Finding a 9-mer that appears six times in a DNA string of length 500 is far more surprising than finding a 9-mer that appears three time, thus supporting the working hypothesis that this region represents the DnaA box in V. cholerae.\n\n\nWe can update our couting function to collapse a k-mer and its reverse complement to a single count. First, we write a subroutine for getting the reverse complement of an input sequence.\n\ndef rev_comp(seq: str) -&gt; str:\n  \"\"\"Computes the reverse complement of a given DNA sequence.\"\"\"\n  rc = \"\"\n  # Map N to N to deal with invalid base calls\n  comp_map = {\"A\": \"T\", \"T\": \"A\", \"G\": \"C\", \"C\": \"G\", \"N\": \"N\"}\n  # Prepend the complement for each base.\n  for base in seq:\n    rc = comp_map[\"base\"] + rc\n  return rc\n\nWhen counting k-mers we will only keep the sequence that comes first when ordered lexicographically. For example, the 4-mer AGTC is the reverse complement of GACT. Since AGTC comes before GACT lexicographyically, we will store only the count of AGTC.\n\ndef frequent_kmers_with_rc(text: str, k: int) -&gt; set:\n  \"\"\"Returns a frequency table containing counts of all k-mers and \n  their reverse complement.\"\"\"\n  kmer_counts = {}\n  for i in range(len(text)-k+1):\n    kmer = text[i:i+k]\n    kmer_rc = rev_comp(kmer)\n    # Only add kmer_ref to the dictionary\n    kmer_ref = None\n    if kmer &gt; kmer_rc:\n      kmer_ref = kmer\n    if (kmer in kmer_counts) or (kmer_rc in kmer_counts):\n      kmer_counts[kmer_ref] += 1\n    else:\n      kmer_counts[kmer_ref] = 1\n  # Return all k-mers sorted by count\n  return kmer_counts\n\n\n\n\nAside from computing the k-mer frequencies, we are also interested in knowning where common k-mers are located in the genome. Given an input pattern or k-mer, we keep a running list of indices at which the pattern occurs as a substring of the genome.\n\ndef pattern_matching(pattern: str, genome: str) -&gt; list[int]:\n  \"\"\"Find all occurrences of a pattern in a genome.\n  Returns a list of indices that indicate the start of each match.\"\"\"\n  matches = []\n  k = len(pattern)\n  for i in range(len(genome)-k+1):\n    if genome[i:i+k] == pattern:\n      matches.append(i)\n  return matches\n\nRunning the pattern_matching function to search all occurrences of ATGATCAAG in the V. cholerae genome would yield the following positions:\n\n\n116556 149355 151913 152013 152394 186189 194276 200076 224527 307692 479770 610980 653338 679985 768828 878903 985368\n\n\nOur motif ATGATCAAG appears 17 times in the bacterial genome. Is this sufficient evidence to confirm that this region represents a signal to bind DnaA for initiating replication?\n\n\n\nWe can validate our hypothesis by checking if the same motif appears in known ori regions from other bacteria. This verifies that the clumping of ATGATCAAG/CTTGATCAT is not merely a result of circumstance.\nLet us check the proposed ori region of Thermotoga petrophila, an extremophile that lives in very hot environments:\n\naactctatacctcctttttgtcgaatttgtgtgatttatagagaaaatcttattaactga\naactaaaatggtaggtttggtggtaggttttgtgtacattttgtagtatctgatttttaa\nttacataccgtatattgtattaaattgacgaacaattgcatggaattgaatatatgcaaa\nacaaacctaccaccaaactctgtattgaccattttaggacaacttcagggtggtaggttt\nctgaagctctcatcaatagactattttagtctttacaaacaatattaccgttcagattca\nagattctacaacgctgttttaatgggcgttgcagaaaacttaccacctaaaatccagtat\nccaagccgatttcagagaaacctaccacttacctaccacttacctaccacccgggtggta\nagttgcagacattattaaaaacctcatcagaagcttgttcaaaaatttcaatactcgaaa\ncctaccacctgcgtcccctattatttactactactaataatagcagtataattgatctga\n\n\nt_petrophila_ori_path = root / Path(\"data/thermotoga_petrophila_ori.txt\")\nt_petrophila_ori = load_text(t_petrophila_ori_path)\nv_cholerae_motifs = [\"ATGATCAAG\", \"CTTGATCAT\"]\nfor motif in v_cholerae_motifs:\n  result = pattern_matching(motif, t_petrophila_ori)\n  if result:\n    print(result)\n\nUpon checking, neither ATGATCAGG nor its reverse complement appear once in the ori site of T. petrophila. One possible explanation is that different motifs are responsible for initiating replicaton in different bacteria.\n\n\n\n\nInstead of finding clumps of a specific k-mer, we can find every k-mer that forms a clump in the genome.\nThe idea is to slide a window of fixed length L along the genome, looking for a region where a k-mer appears several times in short succession. The value of L can vary, however empirical data suggests a length of 500 which reflects the typical length of ori in bacterial genomes.\n\n\n\n\n\n\nClump Finding Problem\n\n\n\nFind patterns forming clumps in a string.\n\nInput: A string text, and integers k, L, and t.\nOutput: All distinct k-mers forming (L, t)-clumps in text.\n\n\n\n\ndef find_clumps(text: str, k: int, L: int, t: int) -&gt; list[str]:\n  \"\"\"Return a list of k-mers that occur at least t times in a region of length L.\"\"\"\n  patterns = set()\n  for i in range(len(text)-L+1):\n    window = text[i:i+L]\n    freq_map = frequency_table(window, k)\n    for kmer in freq_map:\n      if freq_map[kmer] &gt;= t:\n        patterns.add(kmer) \n  return list(patterns)\n\nLet’s find (500,3)-clumps with a k-mer size of 9 in the genome of V cholerae:\n\n# clumps = find_clumps(genome, 9, 500, 3)\n# print(clumps)\n\n\n\n\nPrior to replication, DNA is unwound by the helicase enzyme to separate the two sister strands. This allows other proteins such as primase, ligase, and polymerase to interact with each individual strand and carry out the duplication process.\nWhen DNA is single-stranded, cytosine has the tendency to mutate into thymine in a process called deamination.\n\n\n\n\n\n\n\nBorrowed from a thesis by Elsa Call\nThe converted thymine is then paired with an adenine, leading to a discrepancy in the GC content of the reverse half-strand. This is known as the GC skew.\nHow can we use this to identify the ori site of bacteria?\nThe idea stems from the observation that the forward and reverse half-strands of a bacterial genome is demarcated by the origin and terminus of replication. The forward half-strand traverses the genome in the 5’ ⟶ 3’ direction while the reverse half-strand follows the opposite direction (3’ ⟶ 5’). Due to the asymmetry of the replication process, deamination occurs more frequently in the reverse half-strands resulting in an increase in cytosine. By dividing the genome into equally-sized bins and computing the difference in the counts of guanine and cytosine, we can get an idea of where we are in the genome.\n\n\n\n\n\n\n\nBorrowed from Dr. Pavel Pevzner’s course Bioinformatics Algorithms.\n\n\nThe skew array keeps a running measurement of the skew value while “walking” the length of a genome. The skew value is computed as follows:\n\\[ \\text{skew} = \\text{count}_G - \\text{count}_C \\]\nIf the skew value is increasing, then we guess that we are on the forward half-strand. Otherwise, we guess that we are on the reverse half-strand.\n\n\n\n\n\n\n\nBorrowed from Dr. Pavel Pevzner’s course Bioinformatics Algorithms.\nAs an example, consider the DNA string CATGGGCATCGGCCATACGCC. We compute the skew values starting from the first position up to the length of the string. At position zero, we initialize the skew value as follows:\n\\[\n\\begin{equation}\n\\text{skew}_0 =\n  \\begin{cases}\n    0  & \\text{if $\\text{genome}_0 \\in \\{A,T\\}$} \\\\\n    +1 & \\text{if $\\text{genome}_0 = G$} \\\\  \n    -1 & \\text{if $\\text{genome}_0 = C$} \\\\  \n  \\end{cases}\n\\end{equation}\n\\]\nSince our sequence starts with C, we set \\(\\text{skew}_0 = -1\\). Computing the skew at the \\(i^{th}\\) position can be simplified by considering the skew of the previous position and updating the value based on the following conditions:\n\\[\n\\begin{equation}\n  \\text{skew}_i =\n  \\begin{cases}\n    \\text{skew}_{i-1} & \\text{if $\\text{genome}_i \\in \\{A,T\\}$} \\\\\n    \\text{skew}_{i-1} + 1 & \\text{if $\\text{genome}_i = G$} \\\\\n    \\text{skew}_{i-1} - 1 & \\text{if $\\text{genome}_i = C$} \\\\\n  \\end{cases}\n\\end{equation}\n\\]\nBased on our rules, the first five elements of the skew array can be computed as [-1, -1, -1, 0, 1]. The code implementation is provided below.\n\ndef skew_array(text: str) -&gt; list[int]:\n  \"\"\"Computes the total count difference of G and C.\"\"\"\n  skew_values = [0]\n  for base in text.upper():\n    if base == \"G\":\n      skew_values.append(skew_values[-1]+1)\n    elif base == \"C\":\n      skew_values.append(skew_values[-1]-1)\n    else:\n      skew_values.append(skew_values[-1])\n  return skew_values\n\n\n\n\nThe skew array can be visualized by plotting the skew values against the current genome position. Applying this to the linearized genome of E. coli, we get the following graph:\n\n\n\n\n\nBy finding the range where the skew value transitions from decreasing to increasing, we can identify the position of the ori site of E. coli. Another way to think about the problem is to find the genome position corresponding to the global minimum of the skew array.\nSimilarly, the ter region (terminus of replication) can be framed as a maximization problem. To solve this, we find the genomic location at which we observe the highest skew value in the array.\n\n\n\n\n\n\n\n\n\n\nIn Progress"
  },
  {
    "objectID": "notebook/motif_finding/index.html#genome-data",
    "href": "notebook/motif_finding/index.html#genome-data",
    "title": "Finding Motifs in a Genetic Haystack",
    "section": "",
    "text": "The genome for Vibrio cholerae can be downloaded here.\nLoad and inspect the genome:\n\nfilepath = \"/home/dagsdags/gh-repos/biohub/data/vibrio-cholerae.txt\"\nwith open(filepath, \"r\") as handle:\n  genome = handle.read().strip()\n  n = 80\n  lines = [genome[i:i+n] for i in range(0, 300, n)]\n  for line in lines:\n    print(line)\n\nACAATGAGGTCACTATGTTCGAGCTCTTCAAACCGGCTGCGCATACGCAGCGGCTGCCATCCGATAAGGTGGACAGCGTC\nTATTCACGCCTTCGTTGGCAACTTTTCATCGGTATTTTTGTTGGCTATGCAGGCTACTATTTGGTTCGTAAGAACTTTAG\nCTTGGCAATGCCTTACCTGATTGAACAAGGCTTTAGTCGTGGCGATCTGGGTGTGGCTCTCGGTGCGGTTTCAATCGCGT\nATGGTCTGTCTAAATTTTTGATGGGGAACGTCTCTGACCGTTCTAACCCGCGCTACTTTCTGAGTGCAGGTCTACTCCTT\n\n\n\nexecute-dir: project\n\n\n\n/home/dagsdags/gh-repos/biohub"
  },
  {
    "objectID": "notebook/motif_finding/index.html#k-mer-counting",
    "href": "notebook/motif_finding/index.html#k-mer-counting",
    "title": "Finding Motifs in a Genetic Haystack",
    "section": "",
    "text": "A k-mer is a string of length k. It is a substring derived from a longer sequence. For example, the string AGTAGT has three unique 3-mers: AGT, GTA, and TAG. The 3-mer AGT occurs twice in the sequence while the rest are only found once.\nA simple way to check for motifs is to find frequently-occurring k-mers in a region of the genome. If the count of a particular k-mer is surprisingly large, we can argue that the k-mer pattern may be of biological significance.\nTo count the frequency of a particular k-mer, we can create a window of size k and slide it across a query sequence. At each position, we compare the the current window with the k-mer and increment a counter if they match.\n\ndef pattern_count(text: str, pattern: str) -&gt; int:\n  \"\"\"Counts the number of times `pattern` occurs as a substring of `text`.\"\"\"\n  count: int = 0\n  k = len(pattern)\n  for i in range(len(text)-k+1):\n    if text[i:i+k] == pattern:\n      count += 1\n  return count\n\nA more generalized approach is to set the k parameter and count all k-mers in the query sequence. We then extract the most frequent k-mer by filtering for the substring with the highest count.\n\ndef frequency_table(text: str, k: int) -&gt; dict[str, int]:\n  \"\"\"Returns a frequency table containing all k-mer counts.\"\"\"\n  kmer_counts = {}\n  for i in range(len(text)-k+1):\n    kmer = text[i:i+k]\n    if kmer in kmer_counts:\n      kmer_counts[kmer] += 1\n    else:\n      kmer_counts[kmer] = 1\n  # Return all k-mers sorted by count\n  return kmer_counts\n\ndef get_most_frequent_kmer(freq_map: dict[str, int]) -&gt; tuple[int, list[str]]:\n  \"\"\"Returns the most frequent k-mer and its counts from a frequency table. \n  If there are multiple most-frequent k-mers, return them in a list.\"\"\"\n  max_count = max(freq_map.values())\n  most_freq_kmers = []\n  for kmer, count in freq_map.items():\n    if count == max_count:\n      most_freq_kmers.append(kmer)\n  return (max_count, most_freq_kmers)\n\nUsing the V. cholerae origin of replication:\n\natcaatgatcaacgtaagcttctaagcatgatcaaggtgctcacacagtttatccacaac\nctgagtggatgacatcaagataggtcgttgtatctccttcctctcgtactctcatgacca\ncggaaagatgatcaagagaggatgatttcttggccatatcgcaatgaatacttgtgactt\ngtgcttccaattgacatcttcagcgccatattgcgctggccaaggtgacggagcgggatt\nacgaaagcatgatcatggctgttgttctgtttatcttgttttgactgagacttgttagga\ntagacggtttttcatcactgactagccaaagccttactctgcctgacatcgaccgtaaat\ntgataatgaatttacatgcttccgcgacgatttacctcttgatcatcgatccgattgaag\natcttcaattgttaattctcttgcctcgactcatagccatgatgagctcttgatcatgtt\ntccttaaccctctattttttacggaagaatgatcaagctgctgctcttgatcatcgtttc\n\nLet us find the most frequent frequent k-mers in the range \\(k = 3\\) and \\(k = 9\\):\n\nk_list = list(range(3, 10))\nkmer_counts = {}\nfor k in k_list:\n  freq_map = frequency_table(ori, k)\n  kmer_counts[k] = get_most_frequent_kmer(freq_map)\n\n\n\n{25: 'TGA', 12: 'ATGA', 8: 'TGATCA', 5: 'ATGATCA', 4: 'ATGATCAA', 3: 'ATGATCAAG, CTCTTGATC, TCTTGATCA, CTTGATCAT'}\n\n\nThe 3-mer TGA is observed in 25 unique positions of the bacterial genome. Is this a suprising observation?\nAssuming that the occurrence of a nucleotide at a given position is independent of all other position, we can expect the frequency of a k-mer to decrease as k increases. Given our frequency table, it is a bit surprising that four 9-mers occur three times within a 500-bp window. Furthermore, it seems that the the 9-mers ATGATCAAG and CTTGATCAT are reverse complements of each other.\nSince DNA has a sense of directionality, a protein factor may bind to either forward or reverse strand of a regulatory region. In this case, ATGATCAAG and CTTGATCAT may represent the same protein binding site. Finding a 9-mer that appears six times in a DNA string of length 500 is far more surprising than finding a 9-mer that appears three time, thus supporting the working hypothesis that this region represents the DnaA box in V. cholerae.\n\n\nWe can update our couting function to collapse a k-mer and its reverse complement to a single count. First, we write a subroutine for getting the reverse complement of an input sequence.\n\ndef rev_comp(seq: str) -&gt; str:\n  \"\"\"Computes the reverse complement of a given DNA sequence.\"\"\"\n  rc = \"\"\n  # Map N to N to deal with invalid base calls\n  comp_map = {\"A\": \"T\", \"T\": \"A\", \"G\": \"C\", \"C\": \"G\", \"N\": \"N\"}\n  # Prepend the complement for each base.\n  for base in seq:\n    rc = comp_map[\"base\"] + rc\n  return rc\n\nWhen counting k-mers we will only keep the sequence that comes first when ordered lexicographically. For example, the 4-mer AGTC is the reverse complement of GACT. Since AGTC comes before GACT lexicographyically, we will store only the count of AGTC.\n\ndef frequent_kmers_with_rc(text: str, k: int) -&gt; set:\n  \"\"\"Returns a frequency table containing counts of all k-mers and \n  their reverse complement.\"\"\"\n  kmer_counts = {}\n  for i in range(len(text)-k+1):\n    kmer = text[i:i+k]\n    kmer_rc = rev_comp(kmer)\n    # Only add kmer_ref to the dictionary\n    kmer_ref = None\n    if kmer &gt; kmer_rc:\n      kmer_ref = kmer\n    if (kmer in kmer_counts) or (kmer_rc in kmer_counts):\n      kmer_counts[kmer_ref] += 1\n    else:\n      kmer_counts[kmer_ref] = 1\n  # Return all k-mers sorted by count\n  return kmer_counts\n\n\n\n\nAside from computing the k-mer frequencies, we are also interested in knowning where common k-mers are located in the genome. Given an input pattern or k-mer, we keep a running list of indices at which the pattern occurs as a substring of the genome.\n\ndef pattern_matching(pattern: str, genome: str) -&gt; list[int]:\n  \"\"\"Find all occurrences of a pattern in a genome.\n  Returns a list of indices that indicate the start of each match.\"\"\"\n  matches = []\n  k = len(pattern)\n  for i in range(len(genome)-k+1):\n    if genome[i:i+k] == pattern:\n      matches.append(i)\n  return matches\n\nRunning the pattern_matching function to search all occurrences of ATGATCAAG in the V. cholerae genome would yield the following positions:\n\n\n116556 149355 151913 152013 152394 186189 194276 200076 224527 307692 479770 610980 653338 679985 768828 878903 985368\n\n\nOur motif ATGATCAAG appears 17 times in the bacterial genome. Is this sufficient evidence to confirm that this region represents a signal to bind DnaA for initiating replication?\n\n\n\nWe can validate our hypothesis by checking if the same motif appears in known ori regions from other bacteria. This verifies that the clumping of ATGATCAAG/CTTGATCAT is not merely a result of circumstance.\nLet us check the proposed ori region of Thermotoga petrophila, an extremophile that lives in very hot environments:\n\naactctatacctcctttttgtcgaatttgtgtgatttatagagaaaatcttattaactga\naactaaaatggtaggtttggtggtaggttttgtgtacattttgtagtatctgatttttaa\nttacataccgtatattgtattaaattgacgaacaattgcatggaattgaatatatgcaaa\nacaaacctaccaccaaactctgtattgaccattttaggacaacttcagggtggtaggttt\nctgaagctctcatcaatagactattttagtctttacaaacaatattaccgttcagattca\nagattctacaacgctgttttaatgggcgttgcagaaaacttaccacctaaaatccagtat\nccaagccgatttcagagaaacctaccacttacctaccacttacctaccacccgggtggta\nagttgcagacattattaaaaacctcatcagaagcttgttcaaaaatttcaatactcgaaa\ncctaccacctgcgtcccctattatttactactactaataatagcagtataattgatctga\n\n\nt_petrophila_ori_path = root / Path(\"data/thermotoga_petrophila_ori.txt\")\nt_petrophila_ori = load_text(t_petrophila_ori_path)\nv_cholerae_motifs = [\"ATGATCAAG\", \"CTTGATCAT\"]\nfor motif in v_cholerae_motifs:\n  result = pattern_matching(motif, t_petrophila_ori)\n  if result:\n    print(result)\n\nUpon checking, neither ATGATCAGG nor its reverse complement appear once in the ori site of T. petrophila. One possible explanation is that different motifs are responsible for initiating replicaton in different bacteria."
  },
  {
    "objectID": "notebook/motif_finding/index.html#clump-finding",
    "href": "notebook/motif_finding/index.html#clump-finding",
    "title": "Finding Motifs in a Genetic Haystack",
    "section": "",
    "text": "Instead of finding clumps of a specific k-mer, we can find every k-mer that forms a clump in the genome.\nThe idea is to slide a window of fixed length L along the genome, looking for a region where a k-mer appears several times in short succession. The value of L can vary, however empirical data suggests a length of 500 which reflects the typical length of ori in bacterial genomes.\n\n\n\n\n\n\nClump Finding Problem\n\n\n\nFind patterns forming clumps in a string.\n\nInput: A string text, and integers k, L, and t.\nOutput: All distinct k-mers forming (L, t)-clumps in text.\n\n\n\n\ndef find_clumps(text: str, k: int, L: int, t: int) -&gt; list[str]:\n  \"\"\"Return a list of k-mers that occur at least t times in a region of length L.\"\"\"\n  patterns = set()\n  for i in range(len(text)-L+1):\n    window = text[i:i+L]\n    freq_map = frequency_table(window, k)\n    for kmer in freq_map:\n      if freq_map[kmer] &gt;= t:\n        patterns.add(kmer) \n  return list(patterns)\n\nLet’s find (500,3)-clumps with a k-mer size of 9 in the genome of V cholerae:\n\n# clumps = find_clumps(genome, 9, 500, 3)\n# print(clumps)"
  },
  {
    "objectID": "notebook/motif_finding/index.html#gc-skew",
    "href": "notebook/motif_finding/index.html#gc-skew",
    "title": "Finding Motifs in a Genetic Haystack",
    "section": "",
    "text": "Prior to replication, DNA is unwound by the helicase enzyme to separate the two sister strands. This allows other proteins such as primase, ligase, and polymerase to interact with each individual strand and carry out the duplication process.\nWhen DNA is single-stranded, cytosine has the tendency to mutate into thymine in a process called deamination.\n\n\n\n\n\n\n\nBorrowed from a thesis by Elsa Call\nThe converted thymine is then paired with an adenine, leading to a discrepancy in the GC content of the reverse half-strand. This is known as the GC skew.\nHow can we use this to identify the ori site of bacteria?\nThe idea stems from the observation that the forward and reverse half-strands of a bacterial genome is demarcated by the origin and terminus of replication. The forward half-strand traverses the genome in the 5’ ⟶ 3’ direction while the reverse half-strand follows the opposite direction (3’ ⟶ 5’). Due to the asymmetry of the replication process, deamination occurs more frequently in the reverse half-strands resulting in an increase in cytosine. By dividing the genome into equally-sized bins and computing the difference in the counts of guanine and cytosine, we can get an idea of where we are in the genome.\n\n\n\n\n\n\n\nBorrowed from Dr. Pavel Pevzner’s course Bioinformatics Algorithms.\n\n\nThe skew array keeps a running measurement of the skew value while “walking” the length of a genome. The skew value is computed as follows:\n\\[ \\text{skew} = \\text{count}_G - \\text{count}_C \\]\nIf the skew value is increasing, then we guess that we are on the forward half-strand. Otherwise, we guess that we are on the reverse half-strand.\n\n\n\n\n\n\n\nBorrowed from Dr. Pavel Pevzner’s course Bioinformatics Algorithms.\nAs an example, consider the DNA string CATGGGCATCGGCCATACGCC. We compute the skew values starting from the first position up to the length of the string. At position zero, we initialize the skew value as follows:\n\\[\n\\begin{equation}\n\\text{skew}_0 =\n  \\begin{cases}\n    0  & \\text{if $\\text{genome}_0 \\in \\{A,T\\}$} \\\\\n    +1 & \\text{if $\\text{genome}_0 = G$} \\\\  \n    -1 & \\text{if $\\text{genome}_0 = C$} \\\\  \n  \\end{cases}\n\\end{equation}\n\\]\nSince our sequence starts with C, we set \\(\\text{skew}_0 = -1\\). Computing the skew at the \\(i^{th}\\) position can be simplified by considering the skew of the previous position and updating the value based on the following conditions:\n\\[\n\\begin{equation}\n  \\text{skew}_i =\n  \\begin{cases}\n    \\text{skew}_{i-1} & \\text{if $\\text{genome}_i \\in \\{A,T\\}$} \\\\\n    \\text{skew}_{i-1} + 1 & \\text{if $\\text{genome}_i = G$} \\\\\n    \\text{skew}_{i-1} - 1 & \\text{if $\\text{genome}_i = C$} \\\\\n  \\end{cases}\n\\end{equation}\n\\]\nBased on our rules, the first five elements of the skew array can be computed as [-1, -1, -1, 0, 1]. The code implementation is provided below.\n\ndef skew_array(text: str) -&gt; list[int]:\n  \"\"\"Computes the total count difference of G and C.\"\"\"\n  skew_values = [0]\n  for base in text.upper():\n    if base == \"G\":\n      skew_values.append(skew_values[-1]+1)\n    elif base == \"C\":\n      skew_values.append(skew_values[-1]-1)\n    else:\n      skew_values.append(skew_values[-1])\n  return skew_values\n\n\n\n\nThe skew array can be visualized by plotting the skew values against the current genome position. Applying this to the linearized genome of E. coli, we get the following graph:\n\n\n\n\n\nBy finding the range where the skew value transitions from decreasing to increasing, we can identify the position of the ori site of E. coli. Another way to think about the problem is to find the genome position corresponding to the global minimum of the skew array.\nSimilarly, the ter region (terminus of replication) can be framed as a maximization problem. To solve this, we find the genomic location at which we observe the highest skew value in the array."
  },
  {
    "objectID": "notebook/motif_finding/index.html#approximate-matching",
    "href": "notebook/motif_finding/index.html#approximate-matching",
    "title": "Finding Motifs in a Genetic Haystack",
    "section": "",
    "text": "In Progress"
  },
  {
    "objectID": "notebook/motif_finding/sections/intro.html",
    "href": "notebook/motif_finding/sections/intro.html",
    "title": "Hidden Messages in the Replication Origin",
    "section": "",
    "text": "In bacteria, short regions in the genome called DnaA boxes act as the binding site for DnaA proteins to initiate replication. The function of these segments of DNA can be experimentally verified by wet-lab scientists. However, the process of motif discovery is often costly and is prone to systematic errors.\nThe observation that regulatory regions often contain short motifs make discovery amenable to computation techniques. I will be using the Vibrio cholerae genome to illustrate some approaches to motif discovery."
  },
  {
    "objectID": "notebook/motif_finding/sections/intro.html#genome-data",
    "href": "notebook/motif_finding/sections/intro.html#genome-data",
    "title": "Hidden Messages in the Replication Origin",
    "section": "Genome Data",
    "text": "Genome Data\nThe genome for Vibrio cholerae can be downloaded here.\nLoad and inspect the genome:\n\nfilepath = \"/home/dagsdags/gh-repos/biohub/data/vibrio-cholerae.txt\"\nwith open(filepath, \"r\") as handle:\n  genome = handle.read().strip()\n  n = 80\n  lines = [genome[i:i+n] for i in range(0, 300, n)]\n  for line in lines:\n    print(line)\n\nACAATGAGGTCACTATGTTCGAGCTCTTCAAACCGGCTGCGCATACGCAGCGGCTGCCATCCGATAAGGTGGACAGCGTC\nTATTCACGCCTTCGTTGGCAACTTTTCATCGGTATTTTTGTTGGCTATGCAGGCTACTATTTGGTTCGTAAGAACTTTAG\nCTTGGCAATGCCTTACCTGATTGAACAAGGCTTTAGTCGTGGCGATCTGGGTGTGGCTCTCGGTGCGGTTTCAATCGCGT\nATGGTCTGTCTAAATTTTTGATGGGGAACGTCTCTGACCGTTCTAACCCGCGCTACTTTCTGAGTGCAGGTCTACTCCTT"
  },
  {
    "objectID": "notebook/motif_finding/sections/kmer_counting.html",
    "href": "notebook/motif_finding/sections/kmer_counting.html",
    "title": "K-mer Counting",
    "section": "",
    "text": "/home/dagsdags/gh-repos/biohub\n\n\nA k-mer is a string of length k. It is a substring derived from a longer sequence. For example, the string AGTAGT has three unique 3-mers: AGT, GTA, and TAG. The 3-mer AGT occurs twice in the sequence while the rest are only found once.\nA simple way to check for motifs is to find frequently-occurring k-mers in a region of the genome. If the count of a particular k-mer is surprisingly large, we can argue that the k-mer pattern may be of biological significance.\nTo count the frequency of a particular k-mer, we can create a window of size k and slide it across a query sequence. At each position, we compare the the current window with the k-mer and increment a counter if they match.\n\ndef pattern_count(text: str, pattern: str) -&gt; int:\n  \"\"\"Counts the number of times `pattern` occurs as a substring of `text`.\"\"\"\n  count: int = 0\n  k = len(pattern)\n  for i in range(len(text)-k+1):\n    if text[i:i+k] == pattern:\n      count += 1\n  return count\n\nA more generalized approach is to set the k parameter and count all k-mers in the query sequence. We then extract the most frequent k-mer by filtering for the substring with the highest count.\n\ndef frequency_table(text: str, k: int) -&gt; dict[str, int]:\n  \"\"\"Returns a frequency table containing all k-mer counts.\"\"\"\n  kmer_counts = {}\n  for i in range(len(text)-k+1):\n    kmer = text[i:i+k]\n    if kmer in kmer_counts:\n      kmer_counts[kmer] += 1\n    else:\n      kmer_counts[kmer] = 1\n  # Return all k-mers sorted by count\n  return kmer_counts\n\ndef get_most_frequent_kmer(freq_map: dict[str, int]) -&gt; tuple[int, list[str]]:\n  \"\"\"Returns the most frequent k-mer and its counts from a frequency table. \n  If there are multiple most-frequent k-mers, return them in a list.\"\"\"\n  max_count = max(freq_map.values())\n  most_freq_kmers = []\n  for kmer, count in freq_map.items():\n    if count == max_count:\n      most_freq_kmers.append(kmer)\n  return (max_count, most_freq_kmers)\n\nUsing the V. cholerae origin of replication:\n\natcaatgatcaacgtaagcttctaagcatgatcaaggtgctcacacagtttatccacaac\nctgagtggatgacatcaagataggtcgttgtatctccttcctctcgtactctcatgacca\ncggaaagatgatcaagagaggatgatttcttggccatatcgcaatgaatacttgtgactt\ngtgcttccaattgacatcttcagcgccatattgcgctggccaaggtgacggagcgggatt\nacgaaagcatgatcatggctgttgttctgtttatcttgttttgactgagacttgttagga\ntagacggtttttcatcactgactagccaaagccttactctgcctgacatcgaccgtaaat\ntgataatgaatttacatgcttccgcgacgatttacctcttgatcatcgatccgattgaag\natcttcaattgttaattctcttgcctcgactcatagccatgatgagctcttgatcatgtt\ntccttaaccctctattttttacggaagaatgatcaagctgctgctcttgatcatcgtttc\n\nLet us find the most frequent frequent k-mers in the range \\(k = 3\\) and \\(k = 9\\):\n\nk_list = list(range(3, 10))\nkmer_counts = {}\nfor k in k_list:\n  freq_map = frequency_table(ori, k)\n  kmer_counts[k] = get_most_frequent_kmer(freq_map)\n\n\n\n{25: 'TGA', 12: 'ATGA', 8: 'TGATCA', 5: 'ATGATCA', 4: 'ATGATCAA', 3: 'ATGATCAAG, CTCTTGATC, TCTTGATCA, CTTGATCAT'}\n\n\nThe 3-mer TGA is observed in 25 unique positions of the bacterial genome. Is this a suprising observation?\nAssuming that the occurrence of a nucleotide at a given position is independent of all other position, we can expect the frequency of a k-mer to decrease as k increases. Given our frequency table, it is a bit surprising that four 9-mers occur three times within a 500-bp window. Furthermore, it seems that the the 9-mers ATGATCAAG and CTTGATCAT are reverse complements of each other.\nSince DNA has a sense of directionality, a protein factor may bind to either forward or reverse strand of a regulatory region. In this case, ATGATCAAG and CTTGATCAT may represent the same protein binding site. Finding a 9-mer that appears six times in a DNA string of length 500 is far more surprising than finding a 9-mer that appears three time, thus supporting the working hypothesis that this region represents the DnaA box in V. cholerae.\n\nIncluding Reverse Complements\nWe can update our couting function to collapse a k-mer and its reverse complement to a single count. First, we write a subroutine for getting the reverse complement of an input sequence.\n\ndef rev_comp(seq: str) -&gt; str:\n  \"\"\"Computes the reverse complement of a given DNA sequence.\"\"\"\n  rc = \"\"\n  # Map N to N to deal with invalid base calls\n  comp_map = {\"A\": \"T\", \"T\": \"A\", \"G\": \"C\", \"C\": \"G\", \"N\": \"N\"}\n  # Prepend the complement for each base.\n  for base in seq:\n    rc = comp_map[\"base\"] + rc\n  return rc\n\nWhen counting k-mers we will only keep the sequence that comes first when ordered lexicographically. For example, the 4-mer AGTC is the reverse complement of GACT. Since AGTC comes before GACT lexicographyically, we will store only the count of AGTC.\n\ndef frequent_kmers_with_rc(text: str, k: int) -&gt; set:\n  \"\"\"Returns a frequency table containing counts of all k-mers and \n  their reverse complement.\"\"\"\n  kmer_counts = {}\n  for i in range(len(text)-k+1):\n    kmer = text[i:i+k]\n    kmer_rc = rev_comp(kmer)\n    # Only add kmer_ref to the dictionary\n    kmer_ref = None\n    if kmer &gt; kmer_rc:\n      kmer_ref = kmer\n    if (kmer in kmer_counts) or (kmer_rc in kmer_counts):\n      kmer_counts[kmer_ref] += 1\n    else:\n      kmer_counts[kmer_ref] = 1\n  # Return all k-mers sorted by count\n  return kmer_counts\n\n\n\nPattern Matching\nAside from computing the k-mer frequencies, we are also interested in knowning where common k-mers are located in the genome. Given an input pattern or k-mer, we keep a running list of indices at which the pattern occurs as a substring of the genome.\n\ndef pattern_matching(pattern: str, genome: str) -&gt; list[int]:\n  \"\"\"Find all occurrences of a pattern in a genome.\n  Returns a list of indices that indicate the start of each match.\"\"\"\n  matches = []\n  k = len(pattern)\n  for i in range(len(genome)-k+1):\n    if genome[i:i+k] == pattern:\n      matches.append(i)\n  return matches\n\nRunning the pattern_matching function to search all occurrences of ATGATCAAG in the V. cholerae genome would yield the following positions:\n\n\n116556 149355 151913 152013 152394 186189 194276 200076 224527 307692 479770 610980 653338 679985 768828 878903 985368\n\n\nOur motif ATGATCAAG appears 17 times in the bacterial genome. Is this sufficient evidence to confirm that this region represents a signal to bind DnaA for initiating replication?\n\n\nVerifying with Other Bacterial Genomes\nWe can validate our hypothesis by checking if the same motif appears in known ori regions from other bacteria. This verifies that the clumping of ATGATCAAG/CTTGATCAT is not merely a result of circumstance.\nLet us check the proposed ori region of Thermotoga petrophila, an extremophile that lives in very hot environments:\n\naactctatacctcctttttgtcgaatttgtgtgatttatagagaaaatcttattaactga\naactaaaatggtaggtttggtggtaggttttgtgtacattttgtagtatctgatttttaa\nttacataccgtatattgtattaaattgacgaacaattgcatggaattgaatatatgcaaa\nacaaacctaccaccaaactctgtattgaccattttaggacaacttcagggtggtaggttt\nctgaagctctcatcaatagactattttagtctttacaaacaatattaccgttcagattca\nagattctacaacgctgttttaatgggcgttgcagaaaacttaccacctaaaatccagtat\nccaagccgatttcagagaaacctaccacttacctaccacttacctaccacccgggtggta\nagttgcagacattattaaaaacctcatcagaagcttgttcaaaaatttcaatactcgaaa\ncctaccacctgcgtcccctattatttactactactaataatagcagtataattgatctga\n\n\nt_petrophila_ori_path = root / Path(\"data/thermotoga_petrophila_ori.txt\")\nt_petrophila_ori = load_text(t_petrophila_ori_path)\nv_cholerae_motifs = [\"ATGATCAAG\", \"CTTGATCAT\"]\nfor motif in v_cholerae_motifs:\n  result = pattern_matching(motif, t_petrophila_ori)\n  if result:\n    print(result)\n\nUpon checking, neither ATGATCAGG nor its reverse complement appear once in the ori site of T. petrophila. One possible explanation is that different motifs are responsible for initiating replicaton in different bacteria.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "notebook/sequence_assembly/index.html",
    "href": "notebook/sequence_assembly/index.html",
    "title": "Assembling Genomes",
    "section": "",
    "text": "Currently, technologies with the capacity to sequence human-sized genomes in one go do not exist. This fragments the sequence alignment process into two steps:\n\nSequencing of reads\nAssembly\n\n\n\nIn sequencing, DNA is extracted and purified from an organism. It is then sheared into many short piece and placed into a reaction tube for PCR amplification. Popular platforms for short-read sequencing include Illumina, SOLiD (Life Technologies), and Ion Torrent (ThermoFisher). For long-read sequencing, commonly used platforms include Oxford’s Nanopore sequencer and Pacific Biosciences’ SMRT sequencer.\n\n\nPolymerase Chain Reaction (PCR) is a laboratory technique designed by Kary Mullis to amplify DNA sequences.\n\n\n\nSteps in DNA sequencing. Retrieved from the FJC.\n\n\n\n\n\nThe problem of sequence assembly is synonymous to reconstructing a book that has been blasted into a million pieces. Rebuilding the book from its fragment is not a straightforward task for many reasons. For one, multiple fragments may contain the same word making it hard to exactly know its relative position in the book. It is also possible that the blast completely obliterated some parts of the book resulting to information loss. The difficulty can also be attributed to the intrinsic properties of DNA and its sequencing tools:\n\nDNA is double-stranded: cannot know if the read came from the forward or reverse strand\nSequencing machines can generate errors\nAmplification bias: some regions of the genome may not be covered by any reads\n\n\n\n\nTwo approaches to sequence assembly. Retrieved from Wikipedia.\n\n\nIn an ideal scenario, there exists a copy of the same book which can serve as a reference to guide us in reconstruction. This allows us to reduce the assembly into a mapping problem. Generated reads are mapped to the reference and optimized based on an alignment scoring function. The mapped reads are then collapsed into longer contiguous segments called contigs to simplify the alignment. This is the reference-guided approach to sequence assembly.\nIt can also be the case that the blasted book is the only copy we have, in which case we need to be more clever and design a systematic way to go rebuilding the book. An intuitive approach is to try to join overlapping pairs of reads until you form sufficiently long contigs. This is the de novo approach to sequence assembly.\n\n\n\n\n\n\nString Composition Problem:\n\n\n\nGenerate the k-mer composition of a string.\n\nInput: An integer k and a string text.\nOutput: The k-mer composition of text."
  },
  {
    "objectID": "notebook/sequence_assembly/index.html#how-do-we-assemble-genomes",
    "href": "notebook/sequence_assembly/index.html#how-do-we-assemble-genomes",
    "title": "Assembling Genomes",
    "section": "",
    "text": "Currently, technologies with the capacity to sequence human-sized genomes in one go do not exist. This fragments the sequence alignment process into two steps:\n\nSequencing of reads\nAssembly\n\n\n\nIn sequencing, DNA is extracted and purified from an organism. It is then sheared into many short piece and placed into a reaction tube for PCR amplification. Popular platforms for short-read sequencing include Illumina, SOLiD (Life Technologies), and Ion Torrent (ThermoFisher). For long-read sequencing, commonly used platforms include Oxford’s Nanopore sequencer and Pacific Biosciences’ SMRT sequencer.\n\n\nPolymerase Chain Reaction (PCR) is a laboratory technique designed by Kary Mullis to amplify DNA sequences.\n\n\n\nSteps in DNA sequencing. Retrieved from the FJC.\n\n\n\n\n\nThe problem of sequence assembly is synonymous to reconstructing a book that has been blasted into a million pieces. Rebuilding the book from its fragment is not a straightforward task for many reasons. For one, multiple fragments may contain the same word making it hard to exactly know its relative position in the book. It is also possible that the blast completely obliterated some parts of the book resulting to information loss. The difficulty can also be attributed to the intrinsic properties of DNA and its sequencing tools:\n\nDNA is double-stranded: cannot know if the read came from the forward or reverse strand\nSequencing machines can generate errors\nAmplification bias: some regions of the genome may not be covered by any reads\n\n\n\n\nTwo approaches to sequence assembly. Retrieved from Wikipedia.\n\n\nIn an ideal scenario, there exists a copy of the same book which can serve as a reference to guide us in reconstruction. This allows us to reduce the assembly into a mapping problem. Generated reads are mapped to the reference and optimized based on an alignment scoring function. The mapped reads are then collapsed into longer contiguous segments called contigs to simplify the alignment. This is the reference-guided approach to sequence assembly.\nIt can also be the case that the blasted book is the only copy we have, in which case we need to be more clever and design a systematic way to go rebuilding the book. An intuitive approach is to try to join overlapping pairs of reads until you form sufficiently long contigs. This is the de novo approach to sequence assembly.\n\n\n\n\n\n\nString Composition Problem:\n\n\n\nGenerate the k-mer composition of a string.\n\nInput: An integer k and a string text.\nOutput: The k-mer composition of text."
  },
  {
    "objectID": "notebook/micromamba/index.html",
    "href": "notebook/micromamba/index.html",
    "title": "Getting Started with Micromamba",
    "section": "",
    "text": "Micromamba is a package manager used to install programs in their own isolated environments. It is a light-weight, standalone version of Mamba written purely in C++. Upon installation, the main executable can be accessed by running the micromamba command."
  },
  {
    "objectID": "notebook/micromamba/index.html#what-is-micromamba",
    "href": "notebook/micromamba/index.html#what-is-micromamba",
    "title": "Getting Started with Micromamba",
    "section": "",
    "text": "Micromamba is a package manager used to install programs in their own isolated environments. It is a light-weight, standalone version of Mamba written purely in C++. Upon installation, the main executable can be accessed by running the micromamba command."
  },
  {
    "objectID": "notebook/micromamba/index.html#a-primer-on-using-micromamba",
    "href": "notebook/micromamba/index.html#a-primer-on-using-micromamba",
    "title": "Getting Started with Micromamba",
    "section": "A Primer on Using Micromamba",
    "text": "A Primer on Using Micromamba\n\nTerminology\n\n\n\n\n\n\n\nPrefix\n\nA fully self-contained and portable installation.\n\nEnvironment\n\nAnother name for a target prefix.\n\nActivation\n\nThe activation of an environment makes all its contents available to your shell.\n\n\n\n\n\nDeactivation\n\nThe deactivation is the opposite operation of activation, remove from your shell what makes the environment content accessible.\n\nPackages Repository\n\nA generic way to describe a storage location for software packages; also called repo.\n\nChannel\n\nAn independent and isolated repo structure that is used to classify and administrate more easily a packages server.\n\n\n\n\n\n\n\nInstalling Micromamba\nFor macOS users, micromamba can be installed using Homebrew:\nbrew install micromamba\nFor Linux users, run the following command:\n\"${SHELL}\" &lt;(curl -L micro.mamba.pm/install.sh)\nFor Windows Powershell, invoke:\nInvoke-Expression ((Invoke-WebRequest -Uri https://micro.mamba.pm/install.ps1).Content)\nAfter installation, we make sure that we have an updated version of the software by invoking:\nmicromamba self-update\nIf you encounter an error during installation, refer to micromamba’s troublshooting page for a list of common errors.\n\n\n\n\n\n\nTry it out\n\n\n\nInvoke micromamba with the -h or --help flag to view all available subcommands.\n\n\n\n\nConfiguration\nThere are two ways to configure the behavior of micromamba in your current system. The first method is to generate a text file containing all the configurations in YAML format. This file is often suffixed as .mambarc or .condarc. The second, more simpler method is to update the current configuration through the CLI. This is what we’ll be doing for setting up our environment.\nSince we are concerned with installing bioinformatics packages, we need to add the bioconda channel which is simply an online repository where we will fetch our programs. We will also add the conda-forge channel as an alternative source of packages.\n\n\nFor a full list of available tools hosted in bioconda, consult the following sites:\n\nBioconda Package Index\nBioconda Packages (Anaconda)\n\nRun the following two commands:\nmicromamba config append channels bioconda\nmicromamba config append channels conda-forge\nNext, we will tell micromamba to first look at the bioconda channel before querying the conda-forge channel when downloading packages:\nmicromamba config set channel_priority strict\nWe can view our current configuration as follows:\nmicromamba config list\nIt should output something like this:\nchannels:\n  - bioconda\n  - conda-forge\nchannel_priority: strict\n\n\nCreating a Local Environment\nThe benefit of using micromamba is that it allows us to create isolated environments. As such, software packages that conflict with one another can be run concurrently in the same system. In the case of bioinformatics workflows, packages that have many dependencies often cause other packages to fail. To create a new environment, we invoke the create subcommand:\nmicromamba create -n &lt;environment_name&gt;\nThe -n flag (short for --name) tells the program to label our environment as &lt;environment_name&gt;. As a good practice, keep the name short and descriptive.\nAfter creating the environment, we need to activate it by invoking the activate subcommand followed by the environment name:\nmicromamba activate &lt;environment_name&gt;\nWe are now in the current environment! This should be reflected by a change in the shell prompt. To exit the environment, simply run:\nmicromamba deactivate\n\n\nInstalling Packages\nNow that we are in our newly-created environment, we can now install packages that are only installed within the current activated environment. The command for installing packages is:\nmicromamba install -c &lt;channel_name&gt; &lt;package_name&gt;\nThe -c command tells the program which channel we want to retrieve the packages to be installed. If not specified, micromamba will refer back to the channel priority in the configuration file. Package names are given as positional arguments. Multiple packages can be downloaded at once by listing all package names separated by whitespaces. Additionally, package vesions can be specified as follows:\n# Download a specific version of the package\nmicromamba install -c conda-forge python=3.12.0\n# Download multiple packages at once\nmicromamba install -c bioconda mafft raxml\n# Another way to install packages\nmicromamba install bioconda::mafft bioconda::raxml\n\n\n\n\n\n\n\n\nTip\n\n\n\nMicromamba can also be used for managing Python packages from PIP, as well as R packages from CRAN.\n\n\nList currently install packages:\nmicromamba list"
  },
  {
    "objectID": "notebook/micromamba/index.html#setting-up-our-environment",
    "href": "notebook/micromamba/index.html#setting-up-our-environment",
    "title": "Getting Started with Micromamba",
    "section": "Setting Up Our Environment",
    "text": "Setting Up Our Environment\nFor the second hands-on session, we would need the following programs:\n\n\n\n\n\n\n\n\n\n\nProgram\nVersion\nFunction\nLink\n\n\n\n\nSPAdes\n3.15.5\nAssembler\nhttps://github.com/ablab/spades\n\n\nQUAST\n5.0.2\nAssembly QA\nhttps://github.com/ablab/quast\n\n\nBowtie2\n2.5.4\nRead Mapper\nhttps://bowtie-bio.sourceforge.net/bowtie2/index.shtml\n\n\nGATK\n4.3.0.0\nPreprocessing, Variant Calling\nhttps://gatk.broadinstitute.org/hc/en-us\n\n\nSnpEff\n5.2\nVariant Annotation\nhttps://pcingola.github.io/SnpEff/\n\n\nsamtools\n1.9\nSAM/BAM Indexing and Manipulation\nhttps://github.com/samtools/samtools\n\n\n\n\n\n\nRead the documentation:\n\nSPAdes\nQUAST\nBowtie2\nGATK\nSnpEff\n\nAll of these tools are available in the bioconda repository. The following code snippet creates a new micromamba environment named bioinfo221, activates it on your local machine, and installs a few bioinformatics tools from the bioconda channel:\n1micromamba create -n bioinfo221\n2micromamba activate bioinfo221\n3micromamba install -c bioconda spades quast bowtie2 gatk4 snpeff samtools\n\n1\n\nCreate a micromamba environment named bioinfo221\n\n2\n\nActivate the environment\n\n3\n\nInstall selected tools from the bioconda channel\n\n\nIf you try to install the latest version of each tool, package dependencies will clash with one another. By not specifying the versions, micromamba automatically resolves the versioning of each tool, making sure that one plays well with the others. We can now verify that each tool is installed properly in our system by running their base command.\nspades.py --help\nquast --help\ngatk --help\nsnpEff --help\nbowtie2 --help\nsamtools --help\n\nA More Reproducible Way to Create an Environment\n1. Create a YAML file containing the environment name, channels, and packages.\n\n\nenv.yml\n\nname: bioinfo221\n1channels:\n  - bioconda\n  - conda-forge\n2dependencies:\n  - openjdk=11.0.1\n  - spades\n  - quast\n  - gatk\n  - snpeff\n  - bowtie2\n  - samtools\n\n\n1\n\nA list of channels to pull packages from\n\n2\n\nA list of packages to be installed\n\n\n\n\n Download YAML file \n2. Create a micromamba environment from the YAML file using env create, making to to specify the -f or --file flag.\nmicromamba env create -f env.yml\n3. Activate the environment and install additional packages.\nmicromamba activate bioinfo221\nmicromamba install -c bioconda mafft raxml\n4. Export the updated environment using env export.\nmicromamba env export &gt; updated_env.yml\nThe newly installed packages (mafft, raxml) are now reflected in the new YAML file (updated_env.yml). This is a simple text file that can be sent to other collaborators, allowing them to easily reproduce the environment used for analysis."
  },
  {
    "objectID": "pages/notebook.html",
    "href": "pages/notebook.html",
    "title": "Notebook",
    "section": "",
    "text": "Assembling Genomes\n\n\n\n\n\n\nalgorithms\n\n\nsequence analysis\n\n\n\nBorrowing concepts in graph theory to go from reads to contigs to genomes.\n\n\n\n\n\nOct 15, 2024\n\n\nJan Emmanuel Samson\n\n\n\n\n\n\n\n\n\n\n\n\nFinding Motifs in a Genetic Haystack\n\n\n\n\n\n\nalgorithms\n\n\nsequence analysis\n\n\n\nInterrogating patterns in the genome with biological significance.\n\n\n\n\n\nOct 13, 2024\n\n\nJan Emmanuel Samson\n\n\n\n\n\n\n\n\n\n\n\n\nGetting Started with Micromamba\n\n\n\n\n\n\nCLI\n\n\nContainerization\n\n\n\nSetting up local environments to support reproducible workflows in Bioinformatics.\n\n\n\n\n\nOct 3, 2024\n\n\nJan Emmanuel Samson\n\n\n\n\n\n\n\n\n\n\n\n\nModeling Evolutionary Relationships\n\n\n\n\n\n\nPhylogeny and Evolution\n\n\nAlignment\n\n\n\nBuild trees to infer phylogenetic relationships across species.\n\n\n\n\n\nAug 28, 2024\n\n\nJan Emmanuel Samson\n\n\n\n\n\n\n\n\n\n\n\n\nLocal and Global Sequence Alignment\n\n\n\n\n\n\nAlgorithms\n\n\nAlignment\n\n\n\nFind highly conserved regions in biological strings using techniques in dynamic programming.\n\n\n\n\n\nAug 27, 2024\n\n\nJan Emmanuel Samson\n\n\n\n\n\n\n\n\n\n\n\n\nRetrieving Biological Data\n\n\n\n\n\n\nCLI\n\n\nAPI\n\n\n\nTools for downloading genomic data using the command line.\n\n\n\n\n\nJul 23, 2024\n\n\nJan Emmanuel Samson\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "pages/about.html",
    "href": "pages/about.html",
    "title": "Jan Emmanuel Samson",
    "section": "",
    "text": "Jan Emmanuel Samson is a current MS Bioinformatics student interested in computational drug discovery, molecular virology, and data engineering. His current research focuses on the prediction of protein-protein interactions between swine and the African Swine Fever Virus (ASFV)."
  },
  {
    "objectID": "pages/about.html#education",
    "href": "pages/about.html#education",
    "title": "Jan Emmanuel Samson",
    "section": "Education",
    "text": "Education\nUniversity of the Philippines Diliman\nMS Bioinformatics | Aug 2023 - Aug 2025\nUniversity of the Philippines Los Baños\nBS Biology, major in Genetics | Aug 2018 - Aug 2022"
  },
  {
    "objectID": "pages/about.html#experience",
    "href": "pages/about.html#experience",
    "title": "Jan Emmanuel Samson",
    "section": "Experience",
    "text": "Experience\nSoftware Developer Trainee | Vilage88 | Jan 2023 - May 2023"
  },
  {
    "objectID": "pages/rosalind.html",
    "href": "pages/rosalind.html",
    "title": "Rosalind Challenges",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nDate\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\n2024-07-30\n\n\nConsensus and Profile\n\n\nJan Emmanuel Samson\n\n\n\n\n2024-07-30\n\n\nFinding a Motif in DNA\n\n\nJan Emmanuel Samson\n\n\n\n\n2024-07-29\n\n\nComputing GC Content\n\n\nJan Emmanuel Samson\n\n\n\n\n2024-07-29\n\n\nCounting Point Mutations\n\n\nJan Emmanuel Samson\n\n\n\n\n2024-07-29\n\n\nIntroduction to Mendelian Inheritance\n\n\nJan Emmanuel Samson\n\n\n\n\n2024-07-29\n\n\nRabbits and Recurrence Relations\n\n\nJan Emmanuel Samson\n\n\n\n\n2024-07-29\n\n\nTranslating RNA into Protein\n\n\nJan Emmanuel Samson\n\n\n\n\n2024-07-28\n\n\nCounting DNA Nucleotides\n\n\nJan Emmanuel Samson\n\n\n\n\n2024-07-28\n\n\nComplementing a Strand of DNA\n\n\nJan Emmanuel Samson\n\n\n\n\n2024-07-28\n\n\nTranscribing DNA into RNA\n\n\nJan Emmanuel Samson\n\n\n\n\n\nNo matching items\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "pages/algorithms.html",
    "href": "pages/algorithms.html",
    "title": "Algorithms",
    "section": "",
    "text": "The Z-Algorithm\n\n\n\n\n\n\nAlgorithms\n\n\nAlignment\n\n\nData Structures\n\n\n\nA string preprocessing algorithm for efficient pattern matching.\n\n\n\n\n\nAug 26, 2024\n\n\nJan Emmanuel Samson\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "pages/lab.html",
    "href": "pages/lab.html",
    "title": "Lab",
    "section": "",
    "text": "Quality Control and Adapter Trimming\n\n\n\n\n\n\nsequence-analysis\n\n\npreprocessing\n\n\nCLI\n\n\n\nPreprocessing raw sequencing reads for downstream analysis\n\n\n\n\n\nJan 2, 2025\n\n\nJan Emmanuel Samson\n\n\n\n\n\n\n\n\n\n\n\n\nEntrez Utilities Quick Start\n\n\n\n\n\n\nCLI\n\n\nAPI\n\n\n\nQuery and programmatically access the various databases hosted at NCBI\n\n\n\n\n\nNov 27, 2024\n\n\nJan Emmanuel Samson\n\n\n\n\n\n\n\n\n\n\n\n\nAn Introduction to Neural Networks\n\n\n\n\n\n\nmachine learning\n\n\nstatistics\n\n\n\nLearning the building blocks of AI and machine learning.\n\n\n\n\n\nOct 16, 2024\n\n\nJan Emmanuel Samson\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "notebook/sequence_assembly/sections/intro.html",
    "href": "notebook/sequence_assembly/sections/intro.html",
    "title": "BioHub",
    "section": "",
    "text": "Currently, technologies with the capacity to sequence human-sized genomes in one go do not exist. This fragments the sequence alignment process into two steps:\n\nSequencing of reads\nAssembly\n\n\n\nIn sequencing, DNA is extracted and purified from an organism. It is then sheared into many short piece and placed into a reaction tube for PCR amplification. Popular platforms for short-read sequencing include Illumina, SOLiD (Life Technologies), and Ion Torrent (ThermoFisher). For long-read sequencing, commonly used platforms include Oxford’s Nanopore sequencer and Pacific Biosciences’ SMRT sequencer.\n\n\nPolymerase Chain Reaction (PCR) is a laboratory technique designed by Kary Mullis to amplify DNA sequences.\n\n\n\nSteps in DNA sequencing. Retrieved from the FJC.\n\n\n\n\n\nThe problem of sequence assembly is synonymous to reconstructing a book that has been blasted into a million pieces. Rebuilding the book from its fragment is not a straightforward task for many reasons. For one, multiple fragments may contain the same word making it hard to exactly know its relative position in the book. It is also possible that the blast completely obliterated some parts of the book resulting to information loss. The difficulty can also be attributed to the intrinsic properties of DNA and its sequencing tools:\n\nDNA is double-stranded: cannot know if the read came from the forward or reverse strand\nSequencing machines can generate errors\nAmplification bias: some regions of the genome may not be covered by any reads\n\n\n\n\nTwo approaches to sequence assembly. Retrieved from Wikipedia.\n\n\nIn an ideal scenario, there exists a copy of the same book which can serve as a reference to guide us in reconstruction. This allows us to reduce the assembly into a mapping problem. Generated reads are mapped to the reference and optimized based on an alignment scoring function. The mapped reads are then collapsed into longer contiguous segments called contigs to simplify the alignment. This is the reference-guided approach to sequence assembly.\nIt can also be the case that the blasted book is the only copy we have, in which case we need to be more clever and design a systematic way to go rebuilding the book. An intuitive approach is to try to join overlapping pairs of reads until you form sufficiently long contigs. This is the de novo approach to sequence assembly."
  },
  {
    "objectID": "notebook/sequence_assembly/sections/intro.html#how-do-we-assemble-genomes",
    "href": "notebook/sequence_assembly/sections/intro.html#how-do-we-assemble-genomes",
    "title": "BioHub",
    "section": "",
    "text": "Currently, technologies with the capacity to sequence human-sized genomes in one go do not exist. This fragments the sequence alignment process into two steps:\n\nSequencing of reads\nAssembly\n\n\n\nIn sequencing, DNA is extracted and purified from an organism. It is then sheared into many short piece and placed into a reaction tube for PCR amplification. Popular platforms for short-read sequencing include Illumina, SOLiD (Life Technologies), and Ion Torrent (ThermoFisher). For long-read sequencing, commonly used platforms include Oxford’s Nanopore sequencer and Pacific Biosciences’ SMRT sequencer.\n\n\nPolymerase Chain Reaction (PCR) is a laboratory technique designed by Kary Mullis to amplify DNA sequences.\n\n\n\nSteps in DNA sequencing. Retrieved from the FJC.\n\n\n\n\n\nThe problem of sequence assembly is synonymous to reconstructing a book that has been blasted into a million pieces. Rebuilding the book from its fragment is not a straightforward task for many reasons. For one, multiple fragments may contain the same word making it hard to exactly know its relative position in the book. It is also possible that the blast completely obliterated some parts of the book resulting to information loss. The difficulty can also be attributed to the intrinsic properties of DNA and its sequencing tools:\n\nDNA is double-stranded: cannot know if the read came from the forward or reverse strand\nSequencing machines can generate errors\nAmplification bias: some regions of the genome may not be covered by any reads\n\n\n\n\nTwo approaches to sequence assembly. Retrieved from Wikipedia.\n\n\nIn an ideal scenario, there exists a copy of the same book which can serve as a reference to guide us in reconstruction. This allows us to reduce the assembly into a mapping problem. Generated reads are mapped to the reference and optimized based on an alignment scoring function. The mapped reads are then collapsed into longer contiguous segments called contigs to simplify the alignment. This is the reference-guided approach to sequence assembly.\nIt can also be the case that the blasted book is the only copy we have, in which case we need to be more clever and design a systematic way to go rebuilding the book. An intuitive approach is to try to join overlapping pairs of reads until you form sufficiently long contigs. This is the de novo approach to sequence assembly."
  },
  {
    "objectID": "notebook/motif_finding/sections/clump_finding.html",
    "href": "notebook/motif_finding/sections/clump_finding.html",
    "title": "Clump Finding",
    "section": "",
    "text": "Instead of finding clumps of a specific k-mer, we can find every k-mer that forms a clump in the genome.\nThe idea is to slide a window of fixed length L along the genome, looking for a region where a k-mer appears several times in short succession. The value of L can vary, however empirical data suggests a length of 500 which reflects the typical length of ori in bacterial genomes.\n\n\n\n\n\n\nClump Finding Problem\n\n\n\nFind patterns forming clumps in a string.\n\nInput: A string text, and integers k, L, and t.\nOutput: All distinct k-mers forming (L, t)-clumps in text.\n\n\n\n\ndef find_clumps(text: str, k: int, L: int, t: int) -&gt; list[str]:\n  \"\"\"Return a list of k-mers that occur at least t times in a region of length L.\"\"\"\n  patterns = set()\n  for i in range(len(text)-L+1):\n    window = text[i:i+L]\n    freq_map = frequency_table(window, k)\n    for kmer in freq_map:\n      if freq_map[kmer] &gt;= t:\n        patterns.add(kmer) \n  return list(patterns)\n\nLet’s find (500,3)-clumps with a k-mer size of 9 in the genome of V cholerae:\n\n# clumps = find_clumps(genome, 9, 500, 3)\n# print(clumps)\n\n\n\n\n Back to top"
  },
  {
    "objectID": "notebook/motif_finding/sections/gc_skew.html",
    "href": "notebook/motif_finding/sections/gc_skew.html",
    "title": "GC Skew",
    "section": "",
    "text": "Prior to replication, DNA is unwound by the helicase enzyme to separate the two sister strands. This allows other proteins such as primase, ligase, and polymerase to interact with each individual strand and carry out the duplication process.\nWhen DNA is single-stranded, cytosine has the tendency to mutate into thymine in a process called deamination.\n\n\n\n\n\n\n\nBorrowed from a thesis by Elsa Call\nThe converted thymine is then paired with an adenine, leading to a discrepancy in the GC content of the reverse half-strand. This is known as the GC skew.\nHow can we use this to identify the ori site of bacteria?\nThe idea stems from the observation that the forward and reverse half-strands of a bacterial genome is demarcated by the origin and terminus of replication. The forward half-strand traverses the genome in the 5’ ⟶ 3’ direction while the reverse half-strand follows the opposite direction (3’ ⟶ 5’). Due to the asymmetry of the replication process, deamination occurs more frequently in the reverse half-strands resulting in an increase in cytosine. By dividing the genome into equally-sized bins and computing the difference in the counts of guanine and cytosine, we can get an idea of where we are in the genome.\n\n\n\n\n\n\n\nBorrowed from Dr. Pavel Pevzner’s course Bioinformatics Algorithms.\n\nThe Skew Array\nThe skew array keeps a running measurement of the skew value while “walking” the length of a genome. The skew value is computed as follows:\n\\[ \\text{skew} = \\text{count}_G - \\text{count}_C \\]\nIf the skew value is increasing, then we guess that we are on the forward half-strand. Otherwise, we guess that we are on the reverse half-strand.\n\n\n\n\n\n\n\nBorrowed from Dr. Pavel Pevzner’s course Bioinformatics Algorithms.\nAs an example, consider the DNA string CATGGGCATCGGCCATACGCC. We compute the skew values starting from the first position up to the length of the string. At position zero, we initialize the skew value as follows:\n\\[\n\\begin{equation}\n\\text{skew}_0 =\n  \\begin{cases}\n    0  & \\text{if $\\text{genome}_0 \\in \\{A,T\\}$} \\\\\n    +1 & \\text{if $\\text{genome}_0 = G$} \\\\  \n    -1 & \\text{if $\\text{genome}_0 = C$} \\\\  \n  \\end{cases}\n\\end{equation}\n\\]\nSince our sequence starts with C, we set \\(\\text{skew}_0 = -1\\). Computing the skew at the \\(i^{th}\\) position can be simplified by considering the skew of the previous position and updating the value based on the following conditions:\n\\[\n\\begin{equation}\n  \\text{skew}_i =\n  \\begin{cases}\n    \\text{skew}_{i-1} & \\text{if $\\text{genome}_i \\in \\{A,T\\}$} \\\\\n    \\text{skew}_{i-1} + 1 & \\text{if $\\text{genome}_i = G$} \\\\\n    \\text{skew}_{i-1} - 1 & \\text{if $\\text{genome}_i = C$} \\\\\n  \\end{cases}\n\\end{equation}\n\\]\nBased on our rules, the first five elements of the skew array can be computed as [-1, -1, -1, 0, 1]. The code implementation is provided below.\n\ndef skew_array(text: str) -&gt; list[int]:\n  \"\"\"Computes the total count difference of G and C.\"\"\"\n  skew_values = [0]\n  for base in text.upper():\n    if base == \"G\":\n      skew_values.append(skew_values[-1]+1)\n    elif base == \"C\":\n      skew_values.append(skew_values[-1]-1)\n    else:\n      skew_values.append(skew_values[-1])\n  return skew_values\n\n\n\nVisualizing the Skew Array\nThe skew array can be visualized by plotting the skew values against the current genome position. Applying this to the linearized genome of E. coli, we get the following graph:\n\n\n\n\n\nBy finding the range where the skew value transitions from decreasing to increasing, we can identify the position of the ori site of E. coli. Another way to think about the problem is to find the genome position corresponding to the global minimum of the skew array.\nSimilarly, the ter region (terminus of replication) can be framed as a maximization problem. To solve this, we find the genomic location at which we observe the highest skew value in the array.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "notebook/motif_finding/sections/approximate_matching.html",
    "href": "notebook/motif_finding/sections/approximate_matching.html",
    "title": "BioHub",
    "section": "",
    "text": "In Progress"
  },
  {
    "objectID": "notebook/motif_finding/sections/approximate_matching.html#approximate-matching",
    "href": "notebook/motif_finding/sections/approximate_matching.html#approximate-matching",
    "title": "BioHub",
    "section": "",
    "text": "In Progress"
  },
  {
    "objectID": "notebook/sequence_alignment/index.html",
    "href": "notebook/sequence_alignment/index.html",
    "title": "Local and Global Sequence Alignment",
    "section": "",
    "text": "Sequence alignment is the process of finding the optimal overlap between two strings with the goal of minimizing a distance metric. Distance is usually measured as:\n\nHamming distance\n\nThe minimum number of base substitutions needed to convert one string into another.\n\nEdit distance\n\nThe minimum number of mutations (substitutions, insertions, deletions) needed to convert one string into another. Also known as the Levenshtein distance.\n\n\n\n\nS: TAGACCCCAC\n\n\nT: TAACCCCACC\n\n\n\nGiven strings S and T of equal length, we can confirm that the hamming distance is equal to 4 by counting the number of subsitutions needed to convert string S to T or vice versa:\n\n\nS: TAGACCCCAC\n\n\nT: TAACCCCACC\n\n\n\nIf we expand our edits to insertions or deletions, less work is needed to convert S to T. This is demonstrated by calculating the edit distance, which in our example is equal to 2:\n\n\nS: TAGACCCCAC-\n\n\nT: TA-ACCCCACC\n\n\n\nIn the context of biology, alignment is done to find regions of high sequence similarity. If two genetic regions are similar or identical, sequence alignment can demonstrate the conserved elements or differences betweenthem. Sequence alignment is usually categorized as either being local or global depending on the scope. Global alignment aims for end-to-end pairing (think genome-to-genome comparisons) while local alignment limits the overlap betweensubstrings of two larger strings (think gene-to-gene comparisons).\n\n\n\n\n\n\n\n\nLocal Alignment\n\n\n\n\n\n\n\nGlobal Alignment.\n\n\n\n\n\n\nFigure 1\n\n\n\nThe two types of alignment can be visualized in Figure 1. A 2-dimensional matrix is generated from the bases that compose the two aligned strings. The x-axis contains the conitugous bases of string S while the y-axis stores that of string T. Blue highlights point to regions of similarity and the path(s) derived by connecting the blue points shows an optimal alignment between regions of the two strings.\n\n\nBiological sequences can be represented as a series of characters thus we can interpret local alignment as a string comparison problem. A well-known approach in solving such problem us through the use of dynamic programming where we attempt to break down the task into smaller subproblems and build an optimal solution from smaller solutions. This approach is preferable when problems have:\n\n\nOptimal substructure\n\nThe optimal solution to an instance of the problem contains optimal solutions to subproblems.\n\n\n\nOverlapping subproblems\n\nThere are a limited number of subproblems thus ensuring tractability. Many, if not most, subproblems are repeated multiple times.\n\n\n\nLike greedy algorithms, dynamic programming is used to solve optimization problems. Unlike greedy algorithms, dynamic programming works on a range of problems in which locally optimal choices do not necessarily produce globally optimal results. In practice, solving a problem using dynamic programming involves two main parts: (1) the setup portion followed by (2) performing computation. Setting up can be generalized in five steps:\n\nFind a ‘matrix’ parameterization of the problem and determin the number of variables/dimensions.\nEnsure the subproblem space is polynomial, not exponential.\nDetermine an effective traversal order by allowing easy access to the solutions of subproblems when needed.\nIdentify a recursive formula and define its base case.\nRemember choices (solutions to subproblems).\n\nAfter the set-up is complete, we proceed to computation which is fairly straightforward:\n\nSystematically fill in the table of results and find an optimal score.\nTraceback from the optimal score through the pointers to determine an optimal solution.\n\nIn the folllowing sections, we’ll illustrate dyanamic programming by exploring a few foundational alignment algorithms.\n\n\n\nLocal alignment is usually done to compare highly conserved regions such as genes and motifs. These regions may indicate functional, structural, and/or evolutionary relationships between two biological sequences. One application of local alignment is the detection of chromosomal rearrangements within the sequenced DNA sample.\n\n\n\nLocal alignments to detect rearrangements.\n\n\n\n\nFirst proposed in 1981, the Smith-Waterman algorithm performs local sequence alignment between two biological strings to find regions of similarity. This method can be outlined in four steps:\n\n\nGeneration of Scoring Scheme\n\nA substion matrix maps each pair of base/amino acid subsitution with a score for matches and mismatches. Additionally, a gap penalty is used to determine the score for opening or extendings gaps within an alignment.\n\n\n\nMatrix Initialization\n\nGiven string S of length m and string T of length n, we initialize an (m+1) x (n+1) matrix, filling in the first row and first column with zeros.\n\n\n\nScoring\n\nElements of the matrix are scored from left-to-right, top-to-bottom. The score of succeeding elements depend on the values of previously-computed cells. Only non-negative values are allowed in the matrix; this is an important property that allows for LOCAL alignment.\n\n\n\nTraceback\n\nUpon filling up the matrix with appropriate scores, we identify the element with the highest score and draw a path that connects high-scoring alignments until a zero is reached.\n\n\n\n\nTo illustrate how the algorithm works, let us go through a simple example.\nI will be using the example DNA strings above where \\(S = \\text{TAGACCCCAC}\\) and \\(T = \\text{TAACCCCACC}\\).\n\n\n\nA simple scoring scheme can be devised by following two rules:\n\nA match gets a score of +3\nA mismatch gets a score of -3\n\nThis can be represented as either a function:\n\\[\n\\begin{equation}\n  s(a_i, b_j) =\n  \\left\\{\n    \\begin{array}{l}\n      +3, && a_i = b_j \\\\\n      -3, && q_i \\ne b_j\n    \\end{array}\n  \\right.\n\\end{equation}\n\\]\nor a substitution matrix:\n\n\n\n\nA\nC\nG\nT\n\n\n\n\nA\n+3\n-3\n-3\n-3\n\n\nC\n-3\n+3\n-3\n-3\n\n\nG\n-3\n-3\n+3\n-3\n\n\nT\n-3\n-3\n-3\n+3\n\n\n\nFor amino acid substitutions, we often use the PAM and BLOSUM schemes.\nIn addition to substitution pairs, we introduce a gap penalty for scoring the opening or extension of gaps (or indels). A linear gap penalty assigns gap openings and extensions with the same score. An affine gap penality uses different scores for gap openings and gap extensions, imposing a larger initial cost for opening a gap and a small incremental cost for each gap extension. Additionally, a general gap penalty allow for any cost function while a frame-aware gap penalty takes into consideration disruptions to the coding frame when specifying the cost function.\nFor the sake of simplicity, we will use a modified linear scheme based on the EMBOSS Water tool where gaps are penalized by subtracting 10 from the score.\n\n\n\n\nGenerate an empty matrix M with (m+1) rows and (n+1) columns where m is the length of S and n is the length of T.\nAn extra row/column is needed to represent the starting point where \\(i=0\\), \\(j=0\\), and \\(F(i, j)=0\\). After creating matrix M, set the value of each element of the first row and column to 0.\nMore formally, given a gap penalty d, we set \\(F_{i,0} = -i × d\\) and \\(F_{0,j} = -j × d\\). The starting matrix is illustrated below:\n\n\n\n\n\n\n\n\n\n\nT\nA\nG\nA\nC\nC\nC\nC\nA\nC\n\n\n\n\n\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nT\n0\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n\n\nA\n0\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n\n\nA\n0\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n\n\nC\n0\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n\n\nC\n0\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n\n\nC\n0\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n\n\nC\n0\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n\n\nA\n0\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n\n\nC\n0\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n\n\nC\n0\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n\n\n\n\n\n\n\n\n\n\n\nFirst label the origin as the element located at (\\(i=1\\), \\(j=1\\)). Then consider the four possibilities when moving across the alignment:\n\nSequence S has a gap at the current alignment position.\nSequence T has a gap at the current alignment position.\nA nucleotide substitution has occured at the current position, resulting to a mismatch.\nThere is a match at the current position.\n\nAll four scenarios can be encapsulated in a single formula as stated below:\n\\[\n\\begin{equation}\n  F(i, j) = max\n  \\left\\{\n    \\begin{array}{l}\n      F(i-1, j) - d, && \\text{insert gap in S} \\\\\n      F(i, j-1) - d, && \\text{insert gap in T} \\\\\n      F(i-1, j-1) + s(S_i, T_j), && \\text{base substitution} \\\\\n    \\end{array}\n  \\right.\n\\end{equation}\n\\]\nWhen traversing the matrix, a downwards shift would indicate a gap introduced at string S while a rightward shift would introduce a gap at string T. Hence, movement from either direction would incur a gap penalty. On the other hand, a diagonal movement would implicate either a match or a mismatch. For this, we consult our substitution matrix and update the score of the current element accordingly.\nFor the first element at (1, 1), we compare the scores derived from moving downwards (\\(↓\\)), rightwards (\\(→\\)), and diagonally (\\(↘\\)):\n\nMoving \\(↓\\) would incur a gap penalty of -10\nMoving \\(→\\) would also incur a gap penalty of -10\nMoving \\(↘\\) would incur a mismatch penalty of -3\n\nDecide which direction to follow by choosing that path that would yield the lowest penalty. By moving diagonally, we maximize the score by taking a ‘hit’ of -3 as compared to the alternative of -10. Take note that a score cannot go below 0, hence we take the maximum between the incurred penalty and zero. To illustrate this step, we update the score at position (1, 1) as follows:\n\\[\nF_{\\text{0,0}} = F(0, 0) = 0\n\\]\n\\[\nF_{1,1} = min(0, F_{0,0} -3) = 0\n\\]\nRepeating the same logic for the first 3 rows and columns would yield the following matrix:\n\n\n\n\n\n\n\n\n\n\nT\nA\nG\nA\nC\nC\nC\nC\nA\nC\n\n\n\n\n\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nT\n0\n3\n0\n0\n-\n-\n-\n-\n-\n-\n-\n\n\nA\n0\n0\n6\n0\n-\n-\n-\n-\n-\n-\n-\n\n\nA\n0\n0\n0\n3\n-\n-\n-\n-\n-\n-\n-\n\n\nC\n0\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n\n\nC\n0\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n\n\nC\n0\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n\n\nC\n0\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n\n\nA\n0\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n\n\nC\n0\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n\n\nC\n0\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n\n\n\n\n\n\n\n\nThe completed matrix is shown below:\n\n\n\n\n\n\n\n\n\n\nT\nA\nG\nA\nC\nC\nC\nC\nA\nC\n\n\n\n\n\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nT\n0\n3\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nA\n0\n0\n6\n0\n3\n0\n0\n0\n0\n0\n0\n\n\nA\n0\n0\n0\n3\n3\n0\n0\n0\n0\n0\n0\n\n\nC\n0\n0\n0\n0\n0\n6\n3\n3\n3\n0\n3\n\n\nC\n0\n0\n0\n0\n0\n3\n9\n6\n6\n0\n3\n\n\nC\n0\n0\n0\n0\n0\n3\n6\n12\n9\n3\n3\n\n\nC\n0\n0\n0\n0\n0\n3\n6\n9\n15\n6\n6\n\n\nA\n0\n0\n0\n0\n3\n0\n0\n3\n6\n18\n3\n\n\nC\n0\n0\n0\n0\n0\n6\n0\n0\n0\n3\n21\n\n\nC\n0\n0\n0\n0\n0\n3\n9\n3\n3\n0\n6\n\n\n\n\n\n\n\n\n\n\n\nFirst locate the element with the largest score. This will serve as the start of the backtrace.\nIn our example, the maximum occurs at position (9, 10) where the score is 21. We then start the backtracking by looking at the scores of the neighbors and following the one with the highest score. We continue this until we’re we encounter a zero.\nFollowing the path would lead to the following alignment:\n\n\nS: TAGACCCCAC\n\n\nT: TAG-CCCCAC\n\n\n\n\n\n\n\n\n\n\n\n\nThe Needleman-Wunsch algorithm is used for the global alignment of biological strings. The technique pre-dates the Smith-Waterman algorithm and actually serves as the basis of their implementation. The key difference between the techniques is in how the subsitutions matrix is created. Smith-Waterman does not allow for negative scores while Needleman-Wunsch does, hence extending its capability to align two sequences from end-to-end.\nThe algorithm also follows four general steps:\n\n\nChoosing a scoring scheme.\n\nFollows the same principle as Smith-Waterman.\n\n\n\nInitializing the matrix.\n\nFollows the same principle as Smith-Waterman.\n\n\n\nComputing alignment scores.\n\nMostly follows the same system as Smith-Waterman, but allows for negative scores.\n\n\n\nPath backtracking.\n\nBacktracking always starts at the last element of the matrix. We only stop extending the path when we reach the origin at position (0,0).\n\n\n\n\n\n\nUsing the reference strings and scoring schemes from the previous example, we generate a scoring matrix as follows:\n\n\n\n\n\n\n\n\n\n\nT\nA\nG\nA\nC\nC\nC\nC\nA\nC\n\n\n\n\n\n0\n-10\n-20\n-30\n-40\n-50\n-60\n-70\n-80\n-90\n-100\n\n\nT\n-10\n3\n-7\n-17\n-27\n-37\n-47\n-57\n-67\n-77\n-87\n\n\nA\n-20\n-7\n6\n-4\n-14\n-27\n-37\n-47\n-57\n-67\n-77\n\n\nA\n-30\n-17\n-4\n3\n-1\n-11\n-22\n-33\n-44\n-54\n-64\n\n\nC\n-40\n-27\n-14\n-7\n0\n2\n-8\n-18\n-28\n-38\n-48\n\n\nC\n-50\n-37\n-24\n-17\n-10\n3\n5\n-5\n-15\n-25\n-35\n\n\nC\n-60\n-47\n-34\n-27\n-20\n-7\n6\n8\n-2\n-12\n-22\n\n\nC\n-70\n-57\n-44\n-37\n-30\n-17\n-4\n9\n11\n1\n-9\n\n\nA\n-80\n-67\n-54\n-47\n-34\n-27\n-14\n-1\n6\n14\n4\n\n\nC\n-90\n-77\n-64\n-57\n-44\n-37\n-24\n-11\n2\n4\n17\n\n\nC\n-100\n-87\n-74\n-67\n-54\n-41\n-34\n-21\n-2\n-6\n7\n\n\n\n\n\n\n\n\n\n\n\nAs stated earlier, we start the backtracing procedure from the last element of the matrix. We then greedily choose the direction pointing towards the highest-scoring neighbor. We repeat this process until we reach the origin.\nThe path corresponds to the following sequence of scores:\n\\[\n7 → 17 → 14 → 11 → 9 → 8 → 6 → 5 → 3 → 0 → 3 → 6 → 3\n\\]\nWhich can be represented as the following optimal global alignment:\n\n\nS:  TAGA C-C-C CA - C \n\n\nT:  TAGA -C-C- CA C C"
  },
  {
    "objectID": "notebook/sequence_alignment/index.html#dynamic-programming",
    "href": "notebook/sequence_alignment/index.html#dynamic-programming",
    "title": "Local and Global Sequence Alignment",
    "section": "",
    "text": "Biological sequences can be represented as a series of characters thus we can interpret local alignment as a string comparison problem. A well-known approach in solving such problem us through the use of dynamic programming where we attempt to break down the task into smaller subproblems and build an optimal solution from smaller solutions. This approach is preferable when problems have:\n\n\nOptimal substructure\n\nThe optimal solution to an instance of the problem contains optimal solutions to subproblems.\n\n\n\nOverlapping subproblems\n\nThere are a limited number of subproblems thus ensuring tractability. Many, if not most, subproblems are repeated multiple times.\n\n\n\nLike greedy algorithms, dynamic programming is used to solve optimization problems. Unlike greedy algorithms, dynamic programming works on a range of problems in which locally optimal choices do not necessarily produce globally optimal results. In practice, solving a problem using dynamic programming involves two main parts: (1) the setup portion followed by (2) performing computation. Setting up can be generalized in five steps:\n\nFind a ‘matrix’ parameterization of the problem and determin the number of variables/dimensions.\nEnsure the subproblem space is polynomial, not exponential.\nDetermine an effective traversal order by allowing easy access to the solutions of subproblems when needed.\nIdentify a recursive formula and define its base case.\nRemember choices (solutions to subproblems).\n\nAfter the set-up is complete, we proceed to computation which is fairly straightforward:\n\nSystematically fill in the table of results and find an optimal score.\nTraceback from the optimal score through the pointers to determine an optimal solution.\n\nIn the folllowing sections, we’ll illustrate dyanamic programming by exploring a few foundational alignment algorithms."
  },
  {
    "objectID": "notebook/sequence_alignment/index.html#local-alignment",
    "href": "notebook/sequence_alignment/index.html#local-alignment",
    "title": "Local and Global Sequence Alignment",
    "section": "",
    "text": "Local alignment is usually done to compare highly conserved regions such as genes and motifs. These regions may indicate functional, structural, and/or evolutionary relationships between two biological sequences. One application of local alignment is the detection of chromosomal rearrangements within the sequenced DNA sample.\n\n\n\nLocal alignments to detect rearrangements.\n\n\n\n\nFirst proposed in 1981, the Smith-Waterman algorithm performs local sequence alignment between two biological strings to find regions of similarity. This method can be outlined in four steps:\n\n\nGeneration of Scoring Scheme\n\nA substion matrix maps each pair of base/amino acid subsitution with a score for matches and mismatches. Additionally, a gap penalty is used to determine the score for opening or extendings gaps within an alignment.\n\n\n\nMatrix Initialization\n\nGiven string S of length m and string T of length n, we initialize an (m+1) x (n+1) matrix, filling in the first row and first column with zeros.\n\n\n\nScoring\n\nElements of the matrix are scored from left-to-right, top-to-bottom. The score of succeeding elements depend on the values of previously-computed cells. Only non-negative values are allowed in the matrix; this is an important property that allows for LOCAL alignment.\n\n\n\nTraceback\n\nUpon filling up the matrix with appropriate scores, we identify the element with the highest score and draw a path that connects high-scoring alignments until a zero is reached.\n\n\n\n\nTo illustrate how the algorithm works, let us go through a simple example.\nI will be using the example DNA strings above where \\(S = \\text{TAGACCCCAC}\\) and \\(T = \\text{TAACCCCACC}\\).\n\n\n\nA simple scoring scheme can be devised by following two rules:\n\nA match gets a score of +3\nA mismatch gets a score of -3\n\nThis can be represented as either a function:\n\\[\n\\begin{equation}\n  s(a_i, b_j) =\n  \\left\\{\n    \\begin{array}{l}\n      +3, && a_i = b_j \\\\\n      -3, && q_i \\ne b_j\n    \\end{array}\n  \\right.\n\\end{equation}\n\\]\nor a substitution matrix:\n\n\n\n\nA\nC\nG\nT\n\n\n\n\nA\n+3\n-3\n-3\n-3\n\n\nC\n-3\n+3\n-3\n-3\n\n\nG\n-3\n-3\n+3\n-3\n\n\nT\n-3\n-3\n-3\n+3\n\n\n\nFor amino acid substitutions, we often use the PAM and BLOSUM schemes.\nIn addition to substitution pairs, we introduce a gap penalty for scoring the opening or extension of gaps (or indels). A linear gap penalty assigns gap openings and extensions with the same score. An affine gap penality uses different scores for gap openings and gap extensions, imposing a larger initial cost for opening a gap and a small incremental cost for each gap extension. Additionally, a general gap penalty allow for any cost function while a frame-aware gap penalty takes into consideration disruptions to the coding frame when specifying the cost function.\nFor the sake of simplicity, we will use a modified linear scheme based on the EMBOSS Water tool where gaps are penalized by subtracting 10 from the score.\n\n\n\n\nGenerate an empty matrix M with (m+1) rows and (n+1) columns where m is the length of S and n is the length of T.\nAn extra row/column is needed to represent the starting point where \\(i=0\\), \\(j=0\\), and \\(F(i, j)=0\\). After creating matrix M, set the value of each element of the first row and column to 0.\nMore formally, given a gap penalty d, we set \\(F_{i,0} = -i × d\\) and \\(F_{0,j} = -j × d\\). The starting matrix is illustrated below:\n\n\n\n\n\n\n\n\n\n\nT\nA\nG\nA\nC\nC\nC\nC\nA\nC\n\n\n\n\n\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nT\n0\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n\n\nA\n0\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n\n\nA\n0\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n\n\nC\n0\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n\n\nC\n0\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n\n\nC\n0\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n\n\nC\n0\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n\n\nA\n0\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n\n\nC\n0\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n\n\nC\n0\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n\n\n\n\n\n\n\n\n\n\n\nFirst label the origin as the element located at (\\(i=1\\), \\(j=1\\)). Then consider the four possibilities when moving across the alignment:\n\nSequence S has a gap at the current alignment position.\nSequence T has a gap at the current alignment position.\nA nucleotide substitution has occured at the current position, resulting to a mismatch.\nThere is a match at the current position.\n\nAll four scenarios can be encapsulated in a single formula as stated below:\n\\[\n\\begin{equation}\n  F(i, j) = max\n  \\left\\{\n    \\begin{array}{l}\n      F(i-1, j) - d, && \\text{insert gap in S} \\\\\n      F(i, j-1) - d, && \\text{insert gap in T} \\\\\n      F(i-1, j-1) + s(S_i, T_j), && \\text{base substitution} \\\\\n    \\end{array}\n  \\right.\n\\end{equation}\n\\]\nWhen traversing the matrix, a downwards shift would indicate a gap introduced at string S while a rightward shift would introduce a gap at string T. Hence, movement from either direction would incur a gap penalty. On the other hand, a diagonal movement would implicate either a match or a mismatch. For this, we consult our substitution matrix and update the score of the current element accordingly.\nFor the first element at (1, 1), we compare the scores derived from moving downwards (\\(↓\\)), rightwards (\\(→\\)), and diagonally (\\(↘\\)):\n\nMoving \\(↓\\) would incur a gap penalty of -10\nMoving \\(→\\) would also incur a gap penalty of -10\nMoving \\(↘\\) would incur a mismatch penalty of -3\n\nDecide which direction to follow by choosing that path that would yield the lowest penalty. By moving diagonally, we maximize the score by taking a ‘hit’ of -3 as compared to the alternative of -10. Take note that a score cannot go below 0, hence we take the maximum between the incurred penalty and zero. To illustrate this step, we update the score at position (1, 1) as follows:\n\\[\nF_{\\text{0,0}} = F(0, 0) = 0\n\\]\n\\[\nF_{1,1} = min(0, F_{0,0} -3) = 0\n\\]\nRepeating the same logic for the first 3 rows and columns would yield the following matrix:\n\n\n\n\n\n\n\n\n\n\nT\nA\nG\nA\nC\nC\nC\nC\nA\nC\n\n\n\n\n\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nT\n0\n3\n0\n0\n-\n-\n-\n-\n-\n-\n-\n\n\nA\n0\n0\n6\n0\n-\n-\n-\n-\n-\n-\n-\n\n\nA\n0\n0\n0\n3\n-\n-\n-\n-\n-\n-\n-\n\n\nC\n0\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n\n\nC\n0\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n\n\nC\n0\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n\n\nC\n0\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n\n\nA\n0\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n\n\nC\n0\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n\n\nC\n0\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n\n\n\n\n\n\n\n\nThe completed matrix is shown below:\n\n\n\n\n\n\n\n\n\n\nT\nA\nG\nA\nC\nC\nC\nC\nA\nC\n\n\n\n\n\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nT\n0\n3\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nA\n0\n0\n6\n0\n3\n0\n0\n0\n0\n0\n0\n\n\nA\n0\n0\n0\n3\n3\n0\n0\n0\n0\n0\n0\n\n\nC\n0\n0\n0\n0\n0\n6\n3\n3\n3\n0\n3\n\n\nC\n0\n0\n0\n0\n0\n3\n9\n6\n6\n0\n3\n\n\nC\n0\n0\n0\n0\n0\n3\n6\n12\n9\n3\n3\n\n\nC\n0\n0\n0\n0\n0\n3\n6\n9\n15\n6\n6\n\n\nA\n0\n0\n0\n0\n3\n0\n0\n3\n6\n18\n3\n\n\nC\n0\n0\n0\n0\n0\n6\n0\n0\n0\n3\n21\n\n\nC\n0\n0\n0\n0\n0\n3\n9\n3\n3\n0\n6\n\n\n\n\n\n\n\n\n\n\n\nFirst locate the element with the largest score. This will serve as the start of the backtrace.\nIn our example, the maximum occurs at position (9, 10) where the score is 21. We then start the backtracking by looking at the scores of the neighbors and following the one with the highest score. We continue this until we’re we encounter a zero.\nFollowing the path would lead to the following alignment:\n\n\nS: TAGACCCCAC\n\n\nT: TAG-CCCCAC"
  },
  {
    "objectID": "notebook/sequence_alignment/index.html#global-alignment",
    "href": "notebook/sequence_alignment/index.html#global-alignment",
    "title": "Local and Global Sequence Alignment",
    "section": "",
    "text": "The Needleman-Wunsch algorithm is used for the global alignment of biological strings. The technique pre-dates the Smith-Waterman algorithm and actually serves as the basis of their implementation. The key difference between the techniques is in how the subsitutions matrix is created. Smith-Waterman does not allow for negative scores while Needleman-Wunsch does, hence extending its capability to align two sequences from end-to-end.\nThe algorithm also follows four general steps:\n\n\nChoosing a scoring scheme.\n\nFollows the same principle as Smith-Waterman.\n\n\n\nInitializing the matrix.\n\nFollows the same principle as Smith-Waterman.\n\n\n\nComputing alignment scores.\n\nMostly follows the same system as Smith-Waterman, but allows for negative scores.\n\n\n\nPath backtracking.\n\nBacktracking always starts at the last element of the matrix. We only stop extending the path when we reach the origin at position (0,0).\n\n\n\n\n\n\nUsing the reference strings and scoring schemes from the previous example, we generate a scoring matrix as follows:\n\n\n\n\n\n\n\n\n\n\nT\nA\nG\nA\nC\nC\nC\nC\nA\nC\n\n\n\n\n\n0\n-10\n-20\n-30\n-40\n-50\n-60\n-70\n-80\n-90\n-100\n\n\nT\n-10\n3\n-7\n-17\n-27\n-37\n-47\n-57\n-67\n-77\n-87\n\n\nA\n-20\n-7\n6\n-4\n-14\n-27\n-37\n-47\n-57\n-67\n-77\n\n\nA\n-30\n-17\n-4\n3\n-1\n-11\n-22\n-33\n-44\n-54\n-64\n\n\nC\n-40\n-27\n-14\n-7\n0\n2\n-8\n-18\n-28\n-38\n-48\n\n\nC\n-50\n-37\n-24\n-17\n-10\n3\n5\n-5\n-15\n-25\n-35\n\n\nC\n-60\n-47\n-34\n-27\n-20\n-7\n6\n8\n-2\n-12\n-22\n\n\nC\n-70\n-57\n-44\n-37\n-30\n-17\n-4\n9\n11\n1\n-9\n\n\nA\n-80\n-67\n-54\n-47\n-34\n-27\n-14\n-1\n6\n14\n4\n\n\nC\n-90\n-77\n-64\n-57\n-44\n-37\n-24\n-11\n2\n4\n17\n\n\nC\n-100\n-87\n-74\n-67\n-54\n-41\n-34\n-21\n-2\n-6\n7\n\n\n\n\n\n\n\n\n\n\n\nAs stated earlier, we start the backtracing procedure from the last element of the matrix. We then greedily choose the direction pointing towards the highest-scoring neighbor. We repeat this process until we reach the origin.\nThe path corresponds to the following sequence of scores:\n\\[\n7 → 17 → 14 → 11 → 9 → 8 → 6 → 5 → 3 → 0 → 3 → 6 → 3\n\\]\nWhich can be represented as the following optimal global alignment:\n\n\nS:  TAGA C-C-C CA - C \n\n\nT:  TAGA -C-C- CA C C"
  },
  {
    "objectID": "notebook/retrieving_biological_data/sections/genbank.html",
    "href": "notebook/retrieving_biological_data/sections/genbank.html",
    "title": "Accessing Genbank",
    "section": "",
    "text": "GenBank is the NIH genetic sequence database, an annotated collection of publicly available DNA sequences. If your data has a GenBank accession number such as AF086833, use the bio fetch command. By default, data is printed to stdout. Override this behavior by specifying the output filename with the -o flag or redirect the output to a file with the &gt; operator.\n# Accession id pointing to the record.\n1ACC=AF086833\n\n# Specify output with a flag.\n2bio fetch ${ACC} --format fasta -o ${ACC}.fa\n\n# Redirect the output to a file.\n3bio fetch ${ACC} --format gff &gt; ${ACC}.gff\n\n1\n\nStore accesssion ID as a variable.\n\n2\n\nDownload the sequence (FASTA) file.\n\n3\n\nDownload the annotation (GFF) file.\n\n\nLet us verify the download by viewing the first ten lines of the annotation file:\n\n\n##gff-version 3\n#!gff-spec-version 1.21\n#!processor NCBI annotwriter\n##sequence-region AF086833.2 1 18959\n##species https://www.ncbi.nlm.nih.gov/Taxonomy/Browser/wwwtax.cgi?id=128952\nAF086833.2  Genbank region  1   18959   .   +   .   ID=AF086833.2:1..18959;Dbxref=taxon:128952;gb-acronym=EBOV-May;gbkey=Src;mol_type=viral cRNA;strain=Mayinga\nAF086833.2  Genbank five_prime_UTR  1   55  .   +   .   ID=id-AF086833.2:1..55;Note=putative leader region;function=regulation or initiation of RNA replication;gbkey=5'UTR\nAF086833.2  Genbank gene    56  3026    .   +   .   ID=gene-NP;Name=NP;gbkey=Gene;gene=NP;gene_biotype=protein_coding\nAF086833.2  Genbank mRNA    56  3026    .   +   .   ID=rna-NP;Parent=gene-NP;gbkey=mRNA;gene=NP;product=nucleoprotein\nAF086833.2  Genbank exon    56  3026    .   +   .   ID=exon-NP-1;Parent=rna-NP;gbkey=mRNA;gene=NP;product=nucleoprotein\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "notebook/retrieving_biological_data/sections/genomepy.html",
    "href": "notebook/retrieving_biological_data/sections/genomepy.html",
    "title": "Using genomepy",
    "section": "",
    "text": "How to use genomepy\ngenomepy is another tool designed for searching and downloading genomic data. It can be used to:\n\nsearch available data\nshow the available metadata\nautomatically download, preprocess\ngenerate optional aligned indexes\n\nCurrently, genomepy supports Ensembl, UCSC, NCBI, and GENCODE.\n\n\n\n\n\n\nflowchart LR\n  genomepy[\"`**genomepy**`\"]\n  commands{\"`**search**\n  **install**\n  **annotation**`\"}\n  storage[\"`files stored in\n  **$home/local/share/genomes**`\"]\n  \n  genomepy --&gt; commands --&gt; storage\n\n\n\n\n\n\n\nSee also: genomepy documentation\n\n\nInstallation\nInstall using micromamba:\nmicromamba install genomepy\nInstall using pip or pipx:\npipx install genomepy\n\n\nUsing genomepy\nUse the search command to query genomes by name or accession:\ngenomepy search ecoli &gt; ecoli_query_results.txt\nA genome index will be downloaded upon invoking the search command for the first time. Hence, the initial search may take a while depending on your connection speed. As seen from the log below, assembly summaries are fetched from multiple databases (GENCODE, UCSC, Ensembl, NCBI).\n05:28:31 | INFO | Downloading assembly summaries from GENCODE\n05:29:54 | INFO | Downloading assembly summaries from UCSC\n05:30:05 | INFO | Downloading assembly summaries from Ensembl\n05:30:43 | INFO | Downloading assembly summaries from NCBI, this will take a while...\ngenbank_historical: 73.0k genomes [00:06, 11.1k genomes/s]\nrefseq_historical: 85.6k genomes [00:05, 16.6k genomes/s]\ngenbank: 2.39M genomes [02:11, 18.1k genomes/s]\nrefseq: 378k genomes [00:28, 13.0k genomes/s] \nThe results look like so:\nname                 provider accession         tax_id annotation species                                  other_info                              \n                                                        n r e k   &lt;- UCSC options (see help)                                                       \nEcoliT22_2.0         NCBI     GCF_000247665.3      562     ✓      Escherichia coli O157:H43 str. T22       BAYGEN                                  \nEcoli.O104:H4.LB226692_2.0 NCBI     GCA_000215685.3      562     ✓      Escherichia coli O104:H4 str. LB226692   Life Technologies                       \nEcoli.O104:H4.01-09591_1.0 NCBI     GCA_000221065.2      562     ✓      Escherichia coli O104:H4 str. 01-09591   Life Technologies                       \nEcoli_C227-11_1.0    NCBI     GCA_000220805.2      562     ✓      Escherichia coli O104:H4 str. C227-11    PacBio                                  \necoli009             NCBI     GCA_900607665.1      562     ✓      Escherichia coli                         BIOZENTRUM, UNIVERSITY OF BASEL         \necoli006             NCBI     GCA_900607465.1      562     ✓      Escherichia coli                         BIOZENTRUM, UNIVERSITY OF BASEL         \necoli008             NCBI     GCA_900607535.1      562     ✓      Escherichia coli                         BIOZENTRUM, UNIVERSITY OF BASEL         \necoli017             NCBI     GCA_900608025.1      562     ✓      Escherichia coli                         BIOZENTRUM, UNIVERSITY OF BASEL         \necoli025             NCBI     GCA_900608175.1      562     ✓      Escherichia coli                         BIOZENTRUM, UNIVERSITY OF BASEL         \necoli022             NCBI     GCA_900608105.1      562     ✓      Escherichia coli                         BIOZENTRUM, UNIVERSITY OF BASEL         \necoli015             NCBI     GCA_900607975.1      562     ✓      Escherichia coli                         BIOZENTRUM, UNIVERSITY OF BASEL         \necoli013             NCBI     GCA_900607805.1      562     ✓      Escherichia coli                         BIOZENTRUM, UNIVERSITY OF BASEL       \nEntries under the name field can be used to download the genome:\ngenomepy install ecoli009\nBy default, the downloaded genomes will be found in the ~/.local/share/genomes directory. For our example, the directory named ecoli009 contains the genome data and other relevant files:\n\n\n/home/dagsdags/.local/share/genomes\n└── ecoli009\n    ├── assembly_report.txt\n    ├── ecoli009.fa\n    ├── ecoli009.fa.fai\n    ├── ecoli009.fa.sizes\n    ├── ecoli009.gaps.bed\n    └── README.txt\n\n1 directory, 6 files\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "notebook/retrieving_biological_data/sections/retrieving_fastq.html",
    "href": "notebook/retrieving_biological_data/sections/retrieving_fastq.html",
    "title": "How to get FASTQ data",
    "section": "",
    "text": "Publibashed FASTQ files are stored in the bashort Read Archive (SRA). Access to SRA can be diagrammed like so:\n\n\n\n\n\n\nflowchart LR\n  fq[\"`**FASTQ FILES**`\"]\n  srr[\"`**SRR number**`\"] --&gt; srr2[\"`Find URL and metadata\n  web, **bio**, **ffq**`\"]\n  sra[\"`**SRA**\n  bashort Read Archive`\"] --&gt; sra2[\"`Use SRR number\n  **fastq-dump**`\"] --&gt; fq\n  ensembl[\"`**Ensembl**\n  Sequence archive`\"] --&gt; ensembl2[\"`Find URL\n  **curl**, **wget**, **aria2**`\"]\n  com[\"`**Commercial**\n  Google, Amazon\n  Users pay to download`\"] --&gt; com2[\"`Custom tools\n  **gsutil**, **aws**`\"] --&gt; fq\n\n\n\n\n\n\n\nThe sratools suite from NCBI provides fastq-dump and fasterq-dump to download read data from SRA accessions. In later versions of sratools, fasterq-dump is the preferred tool for fetching read data as demonstrated below:\n# Store accession number and number of reads\nACC=SRR1553425\nN=10000\n\n# Create reads directory\nmkdir reads\n\n# Fetch reads from accession\nfasterq-dump --split-3 -X ${N} -O reads ${ACC}\nHowever this method is clunky and fragile, often failing to fetch the required data due to errors that are cryptically communicated to the user. An alternative method is to retrieve the URLs that point to the data and download locally using wget, curl or aria2. Use bio search to retrieve metadata on the SRA accession and parse using jq.\n\n! bio search SRR1553425 | jq -r '.[].fastq_url[]'\n\nhttps://ftp.sra.ebi.ac.uk/vol1/fastq/SRR155/005/SRR1553425/SRR1553425_1.fastq.gz\nhttps://ftp.sra.ebi.ac.uk/vol1/fastq/SRR155/005/SRR1553425/SRR1553425_2.fastq.gz\n\n\n\nUsing the SRA Explorer\nThe SRA Explorer is a web-based tool developed by Phil Lewis aimed to make SRA data more accessible. It allows you to search for datasets and view metadata. The link can be accessed here:\n\nhttps://sra-explorer.info/\n\n\n\nUsing the NCBI website\nYou can also visit NCBI’s SRA repository here to download sequencing read data.\n\n\nHow to download multiple runs\nAll data from a project can be queried using bio search, parsed using csvcut, and concurrently downloaded using parallel or aria2c:\nThe project id encapsulates all the details in a sequencing experiment. Pass the project id as an argument to the bio search command to view the SRR accessions.\n# Access the project metadata and save as a CSV file\nbio search PRJNA257197 --csv &gt; project.csv\nThe truncated output is as follows:\n\n\nSRR1972602,SAMN03253748,G3769.5,Zaire ebolavirus genome sequencing from 2014 outbreak in Sierra Leone,2015-06-05,Sierra Leone,Zaire ebolavirus,64700200;62048586,177030780,876390,G3769.5.l1,RNA-Seq,TRANSCRIPTOMIC,PAIRED,ILLUMINA,Illumina HiSeq 2500,Zaire ebolavirus Genome sequencing,\"['https://ftp.sra.ebi.ac.uk/vol1/fastq/SRR197/002/SRR1972602/SRR1972602_1.fastq.gz', 'https://ftp.sra.ebi.ac.uk/vol1/fastq/SRR197/002/SRR1972602/SRR1972602_2.fastq.gz']\",\"65 MB, 62 MB files; 0.9 million reads; 177.0 million sequenced bases\"\nSRR1972603,SAMN03253750,G3789.7,Zaire ebolavirus genome sequencing from 2014 outbreak in Sierra Leone,2015-06-05,Sierra Leone,Zaire ebolavirus,48479951;46951197,136769352,677076,G3789.7.l1,RNA-Seq,TRANSCRIPTOMIC,PAIRED,ILLUMINA,Illumina HiSeq 2500,Zaire ebolavirus Genome sequencing,\"['https://ftp.sra.ebi.ac.uk/vol1/fastq/SRR197/003/SRR1972603/SRR1972603_1.fastq.gz', 'https://ftp.sra.ebi.ac.uk/vol1/fastq/SRR197/003/SRR1972603/SRR1972603_2.fastq.gz']\",\"48 MB, 47 MB files; 0.7 million reads; 136.8 million sequenced bases\"\nSRR1972604,SAMN03253751,G3794.3,Zaire ebolavirus genome sequencing from 2014 outbreak in Sierra Leone,2015-06-05,Sierra Leone,Zaire ebolavirus,51272726;46805204,141510494,700547,G3794.3.l1,RNA-Seq,TRANSCRIPTOMIC,PAIRED,ILLUMINA,Illumina HiSeq 2500,Zaire ebolavirus Genome sequencing,\"['https://ftp.sra.ebi.ac.uk/vol1/fastq/SRR197/004/SRR1972604/SRR1972604_1.fastq.gz', 'https://ftp.sra.ebi.ac.uk/vol1/fastq/SRR197/004/SRR1972604/SRR1972604_2.fastq.gz']\",\"51 MB, 47 MB files; 0.7 million reads; 141.5 million sequenced bases\"\nSRR1972608,SAMN03253757,G3809.2,Zaire ebolavirus genome sequencing from 2014 outbreak in Sierra Leone,2015-06-05,Sierra Leone,Zaire ebolavirus,44413580;40807337,114095256,564828,G3809.2.l1,RNA-Seq,TRANSCRIPTOMIC,PAIRED,ILLUMINA,Illumina HiSeq 2500,Zaire ebolavirus Genome sequencing,\"['https://ftp.sra.ebi.ac.uk/vol1/fastq/SRR197/008/SRR1972608/SRR1972608_1.fastq.gz', 'https://ftp.sra.ebi.ac.uk/vol1/fastq/SRR197/008/SRR1972608/SRR1972608_2.fastq.gz']\",\"44 MB, 41 MB files; 0.6 million reads; 114.1 million sequenced bases\"\nSRR1972610,SAMN03253763,G3821.3,Zaire ebolavirus genome sequencing from 2014 outbreak in Sierra Leone,2015-06-05,Sierra Leone,Zaire ebolavirus,51084211;47577227,146658666,726033,G3821.3.l1,RNA-Seq,TRANSCRIPTOMIC,PAIRED,ILLUMINA,Illumina HiSeq 2500,Zaire ebolavirus Genome sequencing,\"['https://ftp.sra.ebi.ac.uk/vol1/fastq/SRR197/000/SRR1972610/SRR1972610_1.fastq.gz', 'https://ftp.sra.ebi.ac.uk/vol1/fastq/SRR197/000/SRR1972610/SRR1972610_2.fastq.gz']\",\"51 MB, 48 MB files; 0.7 million reads; 146.7 million sequenced bases\"\n\n\nOnly the accession numbers are needed to download the reads. From the project file, we extract the first column corresponding to the accession, and use this as input to fastq-dump. The parallel tool enables us to simultaneously download multiple accession at once.\n# Extract the first column and download concurrently using parallel\n1cat project.csv | \\\n2    csvcut -c 1 | \\\n3    head -n 3 | \\\n4    parallel \"fastq-dump -F --split-files -O data {}\"\n\n1\n\nPrint to standard output.\n\n2\n\nFilter only the first column.\n\n3\n\nFilter first three rows.\n\n4\n\nDownload reads for each of the three accessions.\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "notebook/retrieving_biological_data/sections/refgenie.html",
    "href": "notebook/retrieving_biological_data/sections/refgenie.html",
    "title": "How to use refgenie",
    "section": "",
    "text": "Refgenie is a command-line tool that can be used to download and manage reference genomes, and to build and manage custom genome assets. It also provides a Python interface for programmatic access to genome assets.\n\n\n\n\n\n\nflowchart LR\n  list[\"`**refgenie**\n  list`\"]\n  pull[\"`**refgenie**\n  pull`\"]\n  seek[\"`**refgenie**\n  seek`\"]\n  list --&gt; pull --&gt; seek\n\n\n\n\n\n\n\n\nInstallation\nRefgenie can be installed as a Python package using pip:\n# Install using pip.\npip install refgenie\n\n# Install using pipx.\npipx install refgenie\nor conda:\nconda install -c conda-forge refgenie\n\n# Use mamba/micromamba instead of conda\nmicromamba install refgenie\n\n\nCreate a config file\nrefgenie requires a configuration file that lists the resources in the form of a yaml file. For that you need to select a directory that will store the downloaded resources. The path to the config file is saved as the REFGENIE shell environment variable which will be used for initialization:\n# Path pointing to refgenie config file.\nexport REFGENIE=~/refs/config.yaml\n\n# Load the REFGENIE variable when launching a shell instance\necho \"export REFGENIE=~/refs/config.yml\" &gt;&gt; ~/.bashrc\nsource ~/.bashrc\n\n# Run initialization\nrefgenie init\nThe refgenie tool is now ready to be used to download and manage reference genomes.\n\n\nUsing refgenie\nA list of pre-built assets from a remote server can be displayed with listr:\n\n! refgenie listr\n\nzsh:1: command not found: refgenie\n\n\nGenome data is fetched using the pull command:\nrefgenie pull hg38/bwa_index\nThe seek command displays the path of the downloaded file:\nrefgenie seek hg38/bwa_index\nList local genome assets:\nrefgenie list\nUse command substitution to store the genome path to a variable:\n# Retrieve the human reference genome\nrefgenie pull hg38/fasta\n\n# Save path of reference genome to a variable\nREF=$(refgenie seek hg38/fasta)\nThen use the resulting path in downstream tools:\n# Generate statistics for the human reference genome\nseqkit stats ${REF}\nSubscribe to the iGenomes server which hosts additional reference genomes and genome assets.\nrefgenie subscribe -s http://igenomes.databio.org/\n\n\n\n\n Back to top"
  },
  {
    "objectID": "notebook/phylogenetic_analysis/index.html",
    "href": "notebook/phylogenetic_analysis/index.html",
    "title": "Modeling Evolutionary Relationships",
    "section": "",
    "text": "Note\n\n\n\nThis article is inspired by a lecture series from Prof. Greg Kellog. Full playlist can be accessed here."
  },
  {
    "objectID": "notebook/phylogenetic_analysis/index.html#evolution",
    "href": "notebook/phylogenetic_analysis/index.html#evolution",
    "title": "Modeling Evolutionary Relationships",
    "section": "Evolution",
    "text": "Evolution\n\nWhat drives change?\nMutations in the genome create new alleles and change allele frequencies in a population, and as such drive the process of evolution. A simplified model of evolution has two rules:\n\nMutations that confer a fitness advantage are maintained and inherited across generations.\nMutations that deter fitness are selected against and purged from the population.\n\nThis simplified model does not account for population dynamics and environmental factors that may possible affect the overall fitness of a population. From previous studies, we know that there are plenty of other factors that drive evolution such as:\n\nNegative selection\nPositive selection\nGenetic drift\nGenetic expansion (duplication)\nGenetic loss (deletions)\nHorizontal gene transfer\n\n\nNegative Selection\n\nThe natural selection against deleterious mutations, preserving functionally important sites.\n\nPostive Selection\n\nThe process of Darwinian selection by which new, advantageous variants sweep a population.\n\nGenetic Drift\n\nMost mutations the occurred within a population are selectively neutral (Neutral Theory by Kimura).\n\n\n\n\nHow do we observe selection?\nSimplify the model by enforcing a few assumptions:\n\nSome changes are not subject to selection (neutral mutations)\nNon-synonymous mutations in amino acids are subjected to greater selection pressure\nThe overall rate of synonymous mutation is a good approximation of neutral mutations\n\nSelection in protein-coding regions is then infered by:\n\\[\n\\frac{\\text{non-synonymous mutation rate}}{\\text{synonymous mutation rate}}\n\\]\n\n\nSynonymous vs. non-synonymous mutations\n\nThe degeneracy of th genetic code means some nucleotide mutations will not lead to protein change.\nTransition/transversion occur at unequal rates\nPositive selection protein sequences can be inferred from elevated rate of non-synonymous mutations over a neutral model"
  },
  {
    "objectID": "notebook/phylogenetic_analysis/index.html#phylogenetics",
    "href": "notebook/phylogenetic_analysis/index.html#phylogenetics",
    "title": "Modeling Evolutionary Relationships",
    "section": "Phylogenetics",
    "text": "Phylogenetics\nThe study of evolutionary relationships using genetic data, quantitative traits, characters, and morphological data matrices. The subfield of molecular phylogenetics focuses on the use of molecular data such as DNA and protein sequences to infer phylogenies. The end result of doing this type of analysis is usually a phylogenetic tree which is a branching diagram showing inferred evolutionary relationships between samples and species.\nA phylogenetic tree is composed of:\n\nInternal nodes to represent ancestor species;\nExternal nodes (or leaves) to represent extant, known species; and\nBranches whose lengths represent the evolutionary distance between species\n\n\n\n\nComponents of a Phylogenetic Tree.\n\n\n\n\nRetrieved from Your Genome.\n\nRooted and unrooted trees\n\nIf a tree is rooted, the nodes repreent inferred most recent common ancestor\nSometimes, the lengths of branches on a rooted tree can be meaningful\n\nIn a Phylogram, length represents the amount of character change\nIn a Chronogram, length represents evolutionary time\n\nunrooted trees may look rooted because of they way they are displayed\n\nA tree is deemed bifurcating if two descendants arise from every interior node in a phylogenetic tree, akin to the structure of binary trees. A more general term for diagrams that illustrate evolutionary relationships is the dendogram. All trees are dendograms, but not all dendograms are trees.\n\n\nHow do we generate a tree?\n\n1. Decide what sequences to include in the tree\n\nThis is the task of deciding the tips of the trees\nIt if often the most time-consuming\nThink about the question your are trying to answer\nPerform queries using BLAST\n\n\n\n2. Align the sequences using an MSA tool.\n\nUse tools like CLUSTAL, MUSCLE, T-Coffee, etc.\nThere are scenarios where produced alignments need to be edited (e.g. end trimming) due to how tree inference algorithms work.\n\n\n\n3. Estimate the tree.\n\nDistance-based method\n\nAfter alignment, aligned sequences are converted into distances representing pairwise differences between sequences. A distance matix yields a single tree.\n\nCharacter-based method\n\nAfter alignment, each position in the alignment is compared. Homologous positions must be aligned. Multiple trees are generated and compared.\n\n\n\n\n4. Root the tree if possible.\n\n\n\n\n\n\n\nMethod\nDescription\n\n\n\n\nOutgroup\nPlace root between distant sequence and the rest of the tree\n\n\nMidpoint\nPlace root at midpoint of longest path between two leaves\n\n\nGene duplication\nPlace root bewteen paralogous gene copies\n\n\nUPGMA\nAssume a molecular clocl\n\n\n\n\n\n5. Draw the tree to allow interpretation.\n\n\n\nSubstitution Models\nAttempts to estimate the evolutionary divergence from observed substitutions.\n\nEach such model forms a Markov process (similar to PAM), with two components:\n\nA stationary distribution of nucleotide frequencies, \\(\\pi_A\\), \\(\\pi_G\\), \\(\\pi_C\\), \\(\\pi_T\\), \\(\\sum_\\pi = 1\\)\nA transition rate matrix \\(Q\\) that represent transtions that would occur by substitution\n\nA substitution model can be used to estimate evolutionary distance between two sequences.\nA substitution model can also be used to simulate divergence.\nNucleotide substitution models range widely in complexity.\nPhylogenetic tools may provide statistical tests (based on nucleotide sequence alignments and simulations) to help determine the most appropriate substitution model.\n\n\nJukes-Cantor Model\n\nAll mutations are equal with an overall mutation rate \\(α\\).\nNucleotide frequencies are equally distributed, i.e., \\(\\pi_A\\) = \\(\\pi_G\\) = \\(\\pi_C\\) = \\(\\pi_T\\) = 0.25\nConsiders one parameters: \\(α\\).\n\n\n\nMutation probability matrix\n\\[\\begin{matrix}\n  \\quad & A & G & C & T \\\\\n  A & 1-3α & α & α & α \\\\\n  G & α & 1-3α & α & α \\\\\n  C & α & α & 1-3α & α \\\\\n  T & α & α & α & 1-3α \\\\\n\\end{matrix}\\]\n\nTransition rate matrix\n\\[\\begin{matrix}\n  \\quad & A & G & C & T \\\\\n  A & -3α & α & α & α \\\\\n  G & α & -3α & α & α \\\\\n  C & α & α & -3α & α \\\\\n  T & α & α & α & -3α \\\\\n\\end{matrix}\\]\n\n\nEvolutionary distance can be computed as:\n\\[\nD_{xy} = -\\frac{3}{4} ln(1-\\frac{4}{3}p)\n\\]\nwhere \\(D_{xy}\\) is the distance between sequences \\(x\\) and \\(y\\) in terms of number of changes per site, an \\(p\\) is the observed proportion of sites that differ between them.\n\n\nKimura Two-Parameter Model\n\n\n\nAssigned different mutation rates for transitions and transversions\n\n\\(α\\) for transitions\n\\(β\\) for transversions\n\nall base frequencies are equal\n\ni.e., \\(\\pi_A\\) = \\(\\pi_G\\) = \\(\\pi_C\\) = \\(\\pi_T\\) = 0.25\n\n\n\nK2P Transition rate matrix\n\\[\\begin{matrix}\n  \\quad & A & G & C & T \\\\\n  A & -α-2β & α & α & α \\\\\n  G & α & -α-2β & α & α \\\\\n  C & α & α & -α-2β & α \\\\\n  T & α & α & α & -α-2β \\\\\n\\end{matrix}\\]\n\n\n\n\nFalsenstein-Tajima-Nei Model\n\nInstead of varying transition/transversion rates, Falsenstein allowed varying nucleotide frequences:\n\n\\[π_A \\ne π_G \\ne π_C \\ne π_T, \\sum_{\\pi} = 1\\]\n\nThe relative probability of mutating to a particular base \\(i\\) was given by its frequence \\(\\pi_i\\).\nIntroduced a maximum likelihood method for inferring phylogenies.\n\n\n\nHasegawa, Kishino, and Yano 1985\n\nCombined K2P (\\(α\\) and \\(β\\)) with Felsenstein (varying \\(π\\))\n\n\n\nOther Models\n\nMore complicated models up to allowing all substitution frequencies and nucleotide frequencies (i.e., Generalized Time-Reversible) can be derived (Tavare et al. 1986)\nIt is also possible to have different base rates at different positions (which might be expected in coding sequnces) with a different estimator \\(γ\\).\nMore complicated models have more parameters.\nThe more complicated models do not necessarily perform better. The most appropriate model may be assessed by simulation of different models and statistical tests to compare them.\nGoal is to choose the model that is most appropriate for the data."
  },
  {
    "objectID": "notebook/phylogenetic_analysis/index.html#distance-based-methods",
    "href": "notebook/phylogenetic_analysis/index.html#distance-based-methods",
    "title": "Modeling Evolutionary Relationships",
    "section": "Distance-based Methods",
    "text": "Distance-based Methods\nWhen aligning sequence \\(S_i\\) with sequence \\(S_j\\),\n\nCalculate a dissimilarity matrix that has some measure of distance between sequences. We call this \\(D_{ij}\\).\nFor \\(D_{ij}\\) in guide tree construction, we often use the % mismatch between \\(S_i\\) and \\(S_j\\) at aligned positions.\nFor \\(D_{ij}\\) in phylogenetics, we may estimate a distance from the observed differences and a substitution model.\n\nMetric distances always object the triangle inequality formula:\n\\[ D_{ik} \\le D_{ij} + D_{jk} \\]\nA uniform molecular clock assumes an ultrametic distance:\n\\[ D_{ik} \\le \\text{max}(D_{ij}, D_{jk}) \\]\n\nUPGMA\nUnweighted Pair Group Mean Average\n\nThe pair (row and column) with the smallest distance in the matrix is joined.\nThe pair becomes a cluster, with an interior node positioned halfway between the entries.\nA new distance matrix is calculated with the cluster replacing the individual entries using an average of all contributing pairs\nRepeat until done.\n\nThe algorithm runs at \\(O(n^2 \\log n)\\) time complexity and produces a unique, rooted tree.\n\nIntro1a1b2a2b2c3a3b3c4a4b56End\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNeighbour Joining\nNeighbour joining is guaranteed to yield a unique tree from a set of additive distances. The model does not assume equal rates of evolution on all lineages and thus leads to an unrooted tree. Its goal is to minimize the total branch length of a tree using an objective function.\nThe steps of the algorithm are as follows:\n\nStart with a distance matrix \\(D\\) among sequences.\nCalculate the net divergence \\(Q_{ij}\\) of each taxon pair among the \\(n\\) sequences:\n\n\n\n\\[R_i = \\sum_{k=1}^{n} D_{ik}\\]\nwhere \\(R_i\\) is the total distance from a sequence \\(S_i\\) to all others.\n\\[ Q_{ij} = (n-2)D_{ij} - R_i - R_j \\]\n\nCompute the distances between the two points (\\(f\\) and \\(g\\)) and a new node labeled as \\(u\\).\n\n\\[ δ(f,u) = \\frac{1}{2} D_{fg} + \\frac{1}{2(n-2)} [R_f - R_g] \\]\n\\[ δ(g,u) = δ(f,g) - δ(f,u) \\]\n\nCompute the distance between \\(u\\) and the remaining nodes \\(k\\).\n\n\\[ D_{uk} = \\frac{1}{2}[D_{fk} + D_{gk} - D_{fg}] \\]\n\nRecompute the distance matrix and repeat until done."
  },
  {
    "objectID": "notebook/phylogenetic_analysis/index.html#character-based-methods",
    "href": "notebook/phylogenetic_analysis/index.html#character-based-methods",
    "title": "Modeling Evolutionary Relationships",
    "section": "Character-based Methods",
    "text": "Character-based Methods\nLooks at each column in the alignment.\n\nParsimony\n\nSeeks the minimum number of changes and often results in several equivalent trees.\nFound in a variety of computer science algorithms:\n\nheuristic methods\nbranch and bound\nnearest neighbor interchange\n\n\n\nWhat happens when a character evolves rapidly?\n\nin DNA, a rapidly evolving site might be inferred to be unchanged, when in reality it will have changed twice\n\nA → C → A\n\nThis can lead to statistically inconsistent results\nMaximum Parsimony is particularly prone to this behavior\nThe more data, the more likely to be wrong\nVarious detection and solution methods, all imperfect\n\n\n\n\nMaximum Likelihood\n\nUnder a model of evolution, generates a tree that maximizes the likelihood of observing the data.\nProduces different trees with varying likelihood (measured as \\(L\\))."
  },
  {
    "objectID": "lab/edirect/sections/accessory_programs.html",
    "href": "lab/edirect/sections/accessory_programs.html",
    "title": "BioHub",
    "section": "",
    "text": "Additional helper programs are included in the EDirect package. They help eliminate the need to write custom scripts to further filter out query results.\n\nnquire\n\nRetrieves data from remote servers with URLs constructed from command line arguments.\n\ntramsute\n\nConverts a concatenate stream of JSON objects or other structured formats into XML.\n\nxtract\n\nUses waypoints to navigate a complex XML hierarchy and obtain data values by field name."
  },
  {
    "objectID": "lab/edirect/sections/accessory_programs.html#accessory-programs",
    "href": "lab/edirect/sections/accessory_programs.html#accessory-programs",
    "title": "BioHub",
    "section": "",
    "text": "Additional helper programs are included in the EDirect package. They help eliminate the need to write custom scripts to further filter out query results.\n\nnquire\n\nRetrieves data from remote servers with URLs constructed from command line arguments.\n\ntramsute\n\nConverts a concatenate stream of JSON objects or other structured formats into XML.\n\nxtract\n\nUses waypoints to navigate a complex XML hierarchy and obtain data values by field name."
  },
  {
    "objectID": "lab/edirect/sections/eutils.html",
    "href": "lab/edirect/sections/eutils.html",
    "title": "BioHub",
    "section": "",
    "text": "Accessed through the einfo command to display a list of NCBI databases. It can also display summary statistics and information about indexed fields and links in a specific database.\n\n\nThe einfo command allows the following arguments:\n\n-dbs: Outputs a text list of NCBI databases which can be accessed by E-utilities.\n-db: Allow you to specify a database name about which you wish to receive information.\n-fields: Overrides the default XML output and prints a text list of indexed fields for the database specified by the -db parameter.\n-links: Override the default XML output and prints a text list of links (for use with the ELink utility) for the database specified in the -db parameter.\n\n\n\n\nGet a list of all available databases (truncated to the first 10), sorted alphabetically:\neinfo -dbs | sort | head -n 10\nDisplay a list of PubMed indexed fields:\neinfo -db pubmed -fields"
  },
  {
    "objectID": "lab/edirect/sections/eutils.html#einfo",
    "href": "lab/edirect/sections/eutils.html#einfo",
    "title": "BioHub",
    "section": "",
    "text": "Accessed through the einfo command to display a list of NCBI databases. It can also display summary statistics and information about indexed fields and links in a specific database.\n\n\nThe einfo command allows the following arguments:\n\n-dbs: Outputs a text list of NCBI databases which can be accessed by E-utilities.\n-db: Allow you to specify a database name about which you wish to receive information.\n-fields: Overrides the default XML output and prints a text list of indexed fields for the database specified by the -db parameter.\n-links: Override the default XML output and prints a text list of links (for use with the ELink utility) for the database specified in the -db parameter.\n\n\n\n\nGet a list of all available databases (truncated to the first 10), sorted alphabetically:\neinfo -dbs | sort | head -n 10\nDisplay a list of PubMed indexed fields:\neinfo -db pubmed -fields"
  },
  {
    "objectID": "lab/edirect/sections/eutils.html#esearch",
    "href": "lab/edirect/sections/eutils.html#esearch",
    "title": "BioHub",
    "section": "ESearch",
    "text": "ESearch\nUses the esearch command to search an NCBI database for a query and finds the unique identifiers for all records that match the search query.\n\nArguments\nThe esearch command allows the following arguments:\n\n-db: The database to be searched.\n-query: The search string enclosed in double quotes.\n-sort: Specifies the order in which your results will be sorted.\n-datetype: When limiting by date, specifies which of the several date fields on a record is used to limit.\n-days: Limits results to records with dates no more than the specified number of days in the past, based on the date field specified in the -datetype argument.\n-mindate/-maxdate: Limits results to records with dates in a certain range, based on the date field specified in the -datetype argument.\n-spell: Corrects misspellings in your search query.\n-log: Also show the full E-utilities URL and the full query translation.\n\n\n\nExamples\nSearch for articles on the African Swine Fever Virus:\n\nesearch -db pubmed -query \"African Swine Fever Virus\"\n\n&lt;ENTREZ_DIRECT&gt;\n  &lt;Db&gt;pubmed&lt;/Db&gt;\n  &lt;Count&gt;3064&lt;/Count&gt;\n  &lt;Query&gt;African Swine Fever Virus&lt;/Query&gt;\n  &lt;Step&gt;1&lt;/Step&gt;\n  &lt;Elapsed&gt;1&lt;/Elapsed&gt;\n&lt;/ENTREZ_DIRECT&gt;\n\n\nThe query returns XML-formatted output where the &lt;Count&gt;3064&lt;/Count&gt; tag indicates the number of relevant results.\nAdd a date filter and sort by the Relevance field:\n\nesearch -db pubmed -query \"African Swine Fever Virus\" \\\n  -datetype PDAT -mindate 2020 -maxdate 2024 \\\n  -sort \"Relevance\"\n\n&lt;ENTREZ_DIRECT&gt;\n  &lt;Db&gt;pubmed&lt;/Db&gt;\n  &lt;Count&gt;1636&lt;/Count&gt;\n  &lt;Query&gt;(African Swine Fever Virus) AND 2020:2024 [PDAT]&lt;/Query&gt;\n  &lt;Step&gt;1&lt;/Step&gt;\n  &lt;Elapsed&gt;2&lt;/Elapsed&gt;\n&lt;/ENTREZ_DIRECT&gt;"
  },
  {
    "objectID": "lab/edirect/sections/eutils.html#efetch",
    "href": "lab/edirect/sections/eutils.html#efetch",
    "title": "BioHub",
    "section": "EFetch",
    "text": "EFetch\nThe efetch command is used by the EFetch utility to download records from an NCBI database in a specified format.\n\nArguments\nThe efetch command allows the following arguments:\n\n-db: The database from which to retrieve records.\n-id: One or more UIDs, separated by commas.\n-format: Specifies the format in which you wish to display the records.\n\nValid formats include:\n\nuid: display a list of UIDs\nabstract: displays the Abstract view, formatted in plain text\nmedline: displays the MEDLINE view, including the field indicators\nxml: displays the full PubMed XML\n\n\n\n\n\nExamples\nRetrieve 10 articles on ASFV:\n\nesearch -db pubmed -query \"African Swine Fever Virus\" \\\n  | efetch -format uid | head -n 20 \\\n  | esummary -db pubmed \\\n  | xtract -pattern DocumentSummary -element Id Title -year PubDate \\\n  | align-columns\n\n78480   Negative staining of a non-haemadsorbing strain of African swine fever virus.                                                                                                                                              1977\n99488   Electron microscopic observation of African swine fever virus development in Vero cells.                                                                                                                                   1978\n117788  Synthesis of DNA in cells infected with African swine fever virus.                                                                                                                                                         1979\n122128  African swine fever antibody detection in warthogs.                                                                                                                                                                        1979\n158850  Veterinary experiences as a Japanese prisoner of war and ex-POW along the Burma railroad from 1942 to January 1946.                                                                                                        1979\n170316  Design and construction of an apparatus for the growth of micro cell cultures on standard glass microscope slides and its application for screening large numbers of sera by the indirect fluorescent antibody technique.  1975\n224564  Stability of three animal viruses in thyroid tissue from infected swine.                                                                                                                                                   1979\n225989  Glutaraldehyde inactivation of exotic animal viruses in swine heart tissue.                                                                                                                                                1979\n228092  Susceptibility of mink to certain viral animal diseases foreign to the United States.                                                                                                                                      1979\n287093  Some characteristics of african swine fever viruses isolated from Brazil and the Dominican Republic.                                                                                                                       1978\n298918  Additional characteristics of disease caused by the African swine fever viruses isolated from Brazil and the Dominican Republic.                                                                                           1979\n340610  The growth of virulent African swine fever virus in pig monocytes and macrophages.                                                                                                                                         1978\n345892  African swine fever: microplaque assay by an immunoperoxidase method.                                                                                                                                                      1978\n378573  Effect of disodium phosphonoacetate and iododeoxyuridine on the multiplication of African swine fever virus in vitro.                                                                                                      1979\n385771  A solid-phase enzyme linked immunosorbent assay for the detection of African swine fever virus antigen and antibody.                                                                                                       1979\n394467  Enzyme-linked immunosorbent assay for the diagnosis of African swine fever.                                                                                                                                                1979\n403300  Requirement of cell nucleus for African swine fever virus replication in Vero cells.                                                                                                                                       1977\n407329  Crossed immunoelectrophoretic characterization of virus-specified antigens in cells infected with African swine fever (ASF) virus.                                                                                         1977\n489964  Solid-phase radioimmunoassay techniques for the detection of African swine fever antigen and antibody.                                                                                                                     1979\n496644  The replication of virulent and attenuated strains of African swine fever virus in porcine macrophages.                                                                                                                    1979\n\n\nRetrieve a first 20 UIDs of available ASFV genomes:\nesearch -db nuccore -query \"African Swine Fever Virus genome\" \\\n  | efetch -format uid | head\nxtract -pattern PubmedArticle -element PMID ArticleTitle\n-block DateCompleted -sep “-” Year,Month,Day"
  },
  {
    "objectID": "lab/edirect/sections/intro.html",
    "href": "lab/edirect/sections/intro.html",
    "title": "BioHub",
    "section": "",
    "text": "The Entrez Programming Utilities (E-utilities) are a set of server-side programs that provide a stable interface into the Entrez query and database system at the NCBI. It provides a REST API for programmatic access to data hosted in the 38 databases covering a variety of biomedical data, including nucleotide and protein sequences, gene records, molecular structures, and biomedical literature.\n\n\n\n\n\n\n\nThe official help manual can be access here."
  },
  {
    "objectID": "lab/edirect/sections/intro.html#introduction-to-the-e-utilities",
    "href": "lab/edirect/sections/intro.html#introduction-to-the-e-utilities",
    "title": "BioHub",
    "section": "",
    "text": "The Entrez Programming Utilities (E-utilities) are a set of server-side programs that provide a stable interface into the Entrez query and database system at the NCBI. It provides a REST API for programmatic access to data hosted in the 38 databases covering a variety of biomedical data, including nucleotide and protein sequences, gene records, molecular structures, and biomedical literature.\n\n\n\n\n\n\n\nThe official help manual can be access here."
  },
  {
    "objectID": "algorithms/z_algorithm/index.html",
    "href": "algorithms/z_algorithm/index.html",
    "title": "The Z-Algorithm",
    "section": "",
    "text": "A few extact-matching algorithms rely on a fundamental preprocessing step to compute the maximum number of allowable skips in string-to-substring comparisons. Text preprocessing is usually done to only the query string, making it ideal for applications where the query is smaller than the text itself. Such is the case when trying to align short reads to a significantly larger reference genome.\nAlgorithms that preprocess only the query string are termed online while those that preprocess the larger text are offline. Some examples of online algorithms include:\n\nKnuth-Morris-Pratt\nBoyer-Moore\n\nOnline algorithms preprocess the query string using the Z function. Let’s look at how the algorithm works.\n\n\nIn pattern matching, we are interested in two separate strings where one string is ‘smaller’ than the other. The context of string size is limited to the number of characters used in building the string. I will be using notation for representing such string.\n\n\n\nReference string (S)\n\nThe references string or the larger of the two strings.\n\nQuery string (T)\n\nThe query string or the shorter of the two strings.\n\nk\n\nIndex position of the first character of a substring, relative to the start of the reference string. Given \\(S = \\text{abracadabra}\\), the value of \\(k\\) for substring \\(\\text{adab}\\) is 6.\n\n\n\n\n|S|\n\nThe length of the reference string. This can be generalized to other stings as well. Any variable enclosed between pipes (|) refers to the length of that string.\n\nS[i:j]\n\nA subset of the reference string that stars at index i and ends at index j, inclusive. Also generalizable to other strings. For example, if \\(S = \\text{abracadabra}\\) then S[6:9] is just the substring \\(\\text{adab}\\).\n\n\n\n\n\n\n\nGiven a string S and a position i &gt; 1 let \\(Z_i(S)\\) be the length of the longest substring of S that execute: cache: true starts at i and matches a prefix of S.\nIn other words, \\(Z_i(S)\\) is the length of the longest prefix of S[i..|S|] that matches a prefix of S. For example, when \\(S = \\text{aabcaabxaaz}\\) then\n\\[\n\\begin{aligned}\nZ_5(S) = 3 \\\\\nZ_6(S) = 1 \\\\\nZ_7(S) = 0 \\\\\nZ_8(S) = 0 \\\\\nZ_9(S) = 2 \\\\\n\\end{aligned}\n\\]\nThe length of S can be represented as a 1-dimensional line as seen in the figure below. Each span of each z-box should be within the line.\n\nEach solid box represents a substring of S that matches a prefix of S and that starts between positions 2 and i. The right-most end of any Z-box is denoted as \\(r_i\\) and the left-most end is denoted as \\(l_i\\). The substring that occurs between \\(r_i\\) and \\(l_i\\) is labelled as substring \\(α\\).\n\n\n\nGiven a string S with length |S|, we compute \\(Z_i\\), \\(r_i\\), and \\(l_i\\) for each successive position i, starting from \\(i=2\\). An empty array of length |S| will be initialized to store each compute value of \\(Z_i\\).\nFor any iteration i, the algorithm would only need to update the values of \\(r_i\\) and \\(l_i\\) based on the values for the previous iteration j. So for any iteration i where a new Z-box is discovered, the variable r will be incremeted to the end of the Z-box and variable l is set to the current value of i. Given:\n\\[ S = \\text{CAGTCAGCA} \\]\nWe compute for the Z values of the substrings highlighted in blue:\nC A G T C A G C A  C A G T C A G C A  C A G T C A G C A  C A G T C A G C A  C A G T C A G C A C A G T C A G C A\nWe first compute for Z at the second index of the string (\\(Z_2\\)). Since this is the first iteration, we explicitly compare the characters of \\(S[2..|S|]\\) and \\(S[1..|S|]\\) from left to right, keeping count of the number of characters that exactly match:\n\n\n\n\n\n\n\nC A G T C A G C A  C A G T C A G C A  C A G T C A G C A \n\n\nC A G T C A G C A  C A G T C A G C A  C A G T  C A G C A \n\n\n\n\nThe first three iterations show that there are not common characters between the \\(\\text{substring}_i\\) and the prefix of S, hence the compute a \\(Z_i = 0\\). We continue with the next few iterations.\n\n\n\n\n\n\n\nC A G T C A G C A  C A G T C A G C A  C A G T C A G C A \n\n\nC A G T C  A G C A  C A G T C A  G C A  C A G T C A G  C A \n\n\n\n\nAt \\(i = 5\\), we see that there is a common substring of length 3 indicated that we found an occurrence of a new z-box. We denote the start of the substring as k and update the values of r and l as follows:\n\\[\nk = 5; z_k = 3\n\\] \\[\nr = k + z_k - 1\n\\] \\[\nl = k\n\\]\nWe visualize the current z-box as a range within string S bound between indices l and r which in our case is a substring that starts at index 5 and ends at index 7. We represent this substring as \\(Z[l..r]\\) and is color in red below:\n\nC A G T C A G C A  1 2 3 4 5 6 7 8 9\n\nSince we know that \\(S[5..7]\\) occurs at \\(S[1..3]\\), we can also be certain that \\(S[6..7]\\) is found at \\(S[2..3]\\). The same can be said for \\(S[7]\\) and \\(S[3]\\). This subtlety allows us to update the values of l and r based on previously computed values of Z. We update these variables based on the following conditions:\n\nIf \\(k &gt; r\\), explicitly compare characters starting at position k to the character starting at position 1 of string S until a mistmatch is found and assign the count to \\(Z_k\\). If \\(Z_k &gt; 0\\), set \\(r = k + Z_k -1\\) and \\(l = k\\). Otherwise, set \\(r = 0\\) and \\(l = 0\\).\n\n\n\n\nString S[k..r] is labelled \\(β\\) and also occurs starting at position k’ of S.\n\n\n\nIf \\(k \\le r\\), compute for k’ as \\(k' = k - l + 1\\) and the length of S[k..r] (denoted as \\(\\beta\\)). Then following the subconditions:\n\nIf \\(Z_{k'} \\lt |β|\\), set \\(Z_k = Z_{k'}\\) and leave the values of l and r unchanged.\n\n\n\n\nThe longest string starting at k’ that matches a prefix of S is shorter than |\\(β\\)|. In this case, \\(Z_k = Z_{k'}\\).\n\n\n\nIf \\(Z_{k'} \\ge |β|\\), start comparing the characters starting at positions \\(r + 1\\) and \\(|β|+1\\) of S until a mismatch occurs, storing the index where the first mistmatch occurred at variable q. Set \\(Z_k = q - k\\), \\(r = q - 1\\) and \\(l = k\\).\n\n\n\n\nThe longest string starting at k’ that matches a prefix of S is at least |\\(β\\)|.\n\n\n\nContinuing our example, we proceed to iteration 6 (\\(k=6\\)) and follow the condition provided. Since \\(k \\le r\\) (6 &lt; 7), we compute k’ and the length of \\(β\\) as 2 and 3, respectively. We backtrack to the Z-value at index k’ which is seen to be \\(Z_2 = 0\\). This information points us to the first subcondition since \\(0 &lt; 3\\). Hence we set \\(Z_6 = Z_2 = 0\\) and keep l and r unchanged.\nFollowing the next iteration would point us to the second subcondition where we have to explicitly compare substrings starting from indices 4 and 8 of string S. This immediately leads to a mismatch hence we set \\(q = 0\\) and update the variables \\(Z_k = 8 - 70\\), \\(r = 8-1 = 7\\), and \\(l=7\\)\n\n\n\ndef z(s):\n  \"\"\"Use Z-algorithm to preprocess a given string.\"\"\"\n1  L = len(s)\n  Z = [L] + [0] * L\n\n2  for i in range(1, L):\n    if s[i] == s[i-1]:\n      Z[1] += 1\n    else:\n      break\n\n3  r, l = 0, 0\n  if Z[1] &gt; 0:\n    r, l = Z[1], 1\n\n  for k in range(2, L):\n    assert Z[k] == 0\n4    if k &gt; r:\n      for i in range(k, len(s)):\n        if s[i] == s[i-k]:\n          Z[k] += 1\n        else:\n          break\n      r, l = k + Z[k] - 1, k\n    else:\n5      n_beta = r - k + 1\n      Zkp = Z[k-l]\n6      if n_beta &gt; Zkp:\n        Z[k] = Zkp\n      else:\n7        n_match = 0\n        for i in range(r+1, len(s)):\n          if s[i ] == s[i-k]:\n            n_match += 1\n          else:\n            break\n        l, r = k, r + n_match\n        Z[k] = r - k + 1\n  return Z\n\n1\n\nInitialize array for storing Z-values.\n\n2\n\nNaive comparison for first substring.\n\n3\n\nInitialize bounds of the Z-box.\n\n4\n\nCase 1.\n\n5\n\nCase 2.\n\n6\n\nCase 2a.\n\n7\n\nCase 2b."
  },
  {
    "objectID": "algorithms/z_algorithm/index.html#terminology",
    "href": "algorithms/z_algorithm/index.html#terminology",
    "title": "The Z-Algorithm",
    "section": "",
    "text": "In pattern matching, we are interested in two separate strings where one string is ‘smaller’ than the other. The context of string size is limited to the number of characters used in building the string. I will be using notation for representing such string.\n\n\n\nReference string (S)\n\nThe references string or the larger of the two strings.\n\nQuery string (T)\n\nThe query string or the shorter of the two strings.\n\nk\n\nIndex position of the first character of a substring, relative to the start of the reference string. Given \\(S = \\text{abracadabra}\\), the value of \\(k\\) for substring \\(\\text{adab}\\) is 6.\n\n\n\n\n|S|\n\nThe length of the reference string. This can be generalized to other stings as well. Any variable enclosed between pipes (|) refers to the length of that string.\n\nS[i:j]\n\nA subset of the reference string that stars at index i and ends at index j, inclusive. Also generalizable to other strings. For example, if \\(S = \\text{abracadabra}\\) then S[6:9] is just the substring \\(\\text{adab}\\)."
  },
  {
    "objectID": "algorithms/z_algorithm/index.html#the-z-box",
    "href": "algorithms/z_algorithm/index.html#the-z-box",
    "title": "The Z-Algorithm",
    "section": "",
    "text": "Given a string S and a position i &gt; 1 let \\(Z_i(S)\\) be the length of the longest substring of S that execute: cache: true starts at i and matches a prefix of S.\nIn other words, \\(Z_i(S)\\) is the length of the longest prefix of S[i..|S|] that matches a prefix of S. For example, when \\(S = \\text{aabcaabxaaz}\\) then\n\\[\n\\begin{aligned}\nZ_5(S) = 3 \\\\\nZ_6(S) = 1 \\\\\nZ_7(S) = 0 \\\\\nZ_8(S) = 0 \\\\\nZ_9(S) = 2 \\\\\n\\end{aligned}\n\\]\nThe length of S can be represented as a 1-dimensional line as seen in the figure below. Each span of each z-box should be within the line.\n\nEach solid box represents a substring of S that matches a prefix of S and that starts between positions 2 and i. The right-most end of any Z-box is denoted as \\(r_i\\) and the left-most end is denoted as \\(l_i\\). The substring that occurs between \\(r_i\\) and \\(l_i\\) is labelled as substring \\(α\\)."
  },
  {
    "objectID": "algorithms/z_algorithm/index.html#computing-z_i",
    "href": "algorithms/z_algorithm/index.html#computing-z_i",
    "title": "The Z-Algorithm",
    "section": "",
    "text": "Given a string S with length |S|, we compute \\(Z_i\\), \\(r_i\\), and \\(l_i\\) for each successive position i, starting from \\(i=2\\). An empty array of length |S| will be initialized to store each compute value of \\(Z_i\\).\nFor any iteration i, the algorithm would only need to update the values of \\(r_i\\) and \\(l_i\\) based on the values for the previous iteration j. So for any iteration i where a new Z-box is discovered, the variable r will be incremeted to the end of the Z-box and variable l is set to the current value of i. Given:\n\\[ S = \\text{CAGTCAGCA} \\]\nWe compute for the Z values of the substrings highlighted in blue:\nC A G T C A G C A  C A G T C A G C A  C A G T C A G C A  C A G T C A G C A  C A G T C A G C A C A G T C A G C A\nWe first compute for Z at the second index of the string (\\(Z_2\\)). Since this is the first iteration, we explicitly compare the characters of \\(S[2..|S|]\\) and \\(S[1..|S|]\\) from left to right, keeping count of the number of characters that exactly match:\n\n\n\n\n\n\n\nC A G T C A G C A  C A G T C A G C A  C A G T C A G C A \n\n\nC A G T C A G C A  C A G T C A G C A  C A G T  C A G C A \n\n\n\n\nThe first three iterations show that there are not common characters between the \\(\\text{substring}_i\\) and the prefix of S, hence the compute a \\(Z_i = 0\\). We continue with the next few iterations.\n\n\n\n\n\n\n\nC A G T C A G C A  C A G T C A G C A  C A G T C A G C A \n\n\nC A G T C  A G C A  C A G T C A  G C A  C A G T C A G  C A \n\n\n\n\nAt \\(i = 5\\), we see that there is a common substring of length 3 indicated that we found an occurrence of a new z-box. We denote the start of the substring as k and update the values of r and l as follows:\n\\[\nk = 5; z_k = 3\n\\] \\[\nr = k + z_k - 1\n\\] \\[\nl = k\n\\]\nWe visualize the current z-box as a range within string S bound between indices l and r which in our case is a substring that starts at index 5 and ends at index 7. We represent this substring as \\(Z[l..r]\\) and is color in red below:\n\nC A G T C A G C A  1 2 3 4 5 6 7 8 9\n\nSince we know that \\(S[5..7]\\) occurs at \\(S[1..3]\\), we can also be certain that \\(S[6..7]\\) is found at \\(S[2..3]\\). The same can be said for \\(S[7]\\) and \\(S[3]\\). This subtlety allows us to update the values of l and r based on previously computed values of Z. We update these variables based on the following conditions:\n\nIf \\(k &gt; r\\), explicitly compare characters starting at position k to the character starting at position 1 of string S until a mistmatch is found and assign the count to \\(Z_k\\). If \\(Z_k &gt; 0\\), set \\(r = k + Z_k -1\\) and \\(l = k\\). Otherwise, set \\(r = 0\\) and \\(l = 0\\).\n\n\n\n\nString S[k..r] is labelled \\(β\\) and also occurs starting at position k’ of S.\n\n\n\nIf \\(k \\le r\\), compute for k’ as \\(k' = k - l + 1\\) and the length of S[k..r] (denoted as \\(\\beta\\)). Then following the subconditions:\n\nIf \\(Z_{k'} \\lt |β|\\), set \\(Z_k = Z_{k'}\\) and leave the values of l and r unchanged.\n\n\n\n\nThe longest string starting at k’ that matches a prefix of S is shorter than |\\(β\\)|. In this case, \\(Z_k = Z_{k'}\\).\n\n\n\nIf \\(Z_{k'} \\ge |β|\\), start comparing the characters starting at positions \\(r + 1\\) and \\(|β|+1\\) of S until a mismatch occurs, storing the index where the first mistmatch occurred at variable q. Set \\(Z_k = q - k\\), \\(r = q - 1\\) and \\(l = k\\).\n\n\n\n\nThe longest string starting at k’ that matches a prefix of S is at least |\\(β\\)|.\n\n\n\nContinuing our example, we proceed to iteration 6 (\\(k=6\\)) and follow the condition provided. Since \\(k \\le r\\) (6 &lt; 7), we compute k’ and the length of \\(β\\) as 2 and 3, respectively. We backtrack to the Z-value at index k’ which is seen to be \\(Z_2 = 0\\). This information points us to the first subcondition since \\(0 &lt; 3\\). Hence we set \\(Z_6 = Z_2 = 0\\) and keep l and r unchanged.\nFollowing the next iteration would point us to the second subcondition where we have to explicitly compare substrings starting from indices 4 and 8 of string S. This immediately leads to a mismatch hence we set \\(q = 0\\) and update the variables \\(Z_k = 8 - 70\\), \\(r = 8-1 = 7\\), and \\(l=7\\)"
  },
  {
    "objectID": "algorithms/z_algorithm/index.html#implementation",
    "href": "algorithms/z_algorithm/index.html#implementation",
    "title": "The Z-Algorithm",
    "section": "",
    "text": "def z(s):\n  \"\"\"Use Z-algorithm to preprocess a given string.\"\"\"\n1  L = len(s)\n  Z = [L] + [0] * L\n\n2  for i in range(1, L):\n    if s[i] == s[i-1]:\n      Z[1] += 1\n    else:\n      break\n\n3  r, l = 0, 0\n  if Z[1] &gt; 0:\n    r, l = Z[1], 1\n\n  for k in range(2, L):\n    assert Z[k] == 0\n4    if k &gt; r:\n      for i in range(k, len(s)):\n        if s[i] == s[i-k]:\n          Z[k] += 1\n        else:\n          break\n      r, l = k + Z[k] - 1, k\n    else:\n5      n_beta = r - k + 1\n      Zkp = Z[k-l]\n6      if n_beta &gt; Zkp:\n        Z[k] = Zkp\n      else:\n7        n_match = 0\n        for i in range(r+1, len(s)):\n          if s[i ] == s[i-k]:\n            n_match += 1\n          else:\n            break\n        l, r = k, r + n_match\n        Z[k] = r - k + 1\n  return Z\n\n1\n\nInitialize array for storing Z-values.\n\n2\n\nNaive comparison for first substring.\n\n3\n\nInitialize bounds of the Z-box.\n\n4\n\nCase 1.\n\n5\n\nCase 2.\n\n6\n\nCase 2a.\n\n7\n\nCase 2b."
  },
  {
    "objectID": "lab/quality_control/index.html",
    "href": "lab/quality_control/index.html",
    "title": "Quality Control and Adapter Trimming",
    "section": "",
    "text": "Objective\n\n\n\nIn this tutorial, you will learn some important steps in preparing sequencing reads for downstream analysis. This includes:\n\nexploring the raw data\nremoving adapters\nfiltering reads based on quality score\nsummarizing and exploring filtered data"
  },
  {
    "objectID": "lab/quality_control/index.html#prerequisites",
    "href": "lab/quality_control/index.html#prerequisites",
    "title": "Quality Control and Adapter Trimming",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nDownloading Sequencing Reads\nIn a typical workflow, sequencing reads are downloaded from the Sequencing Read Archive. To learn more about fetching data from the SRA, refer to this article. In this tutorial, we will be processing a dataset containing 100000 reads stored as a single FASTQ file.\nCreate an empty directory and download the sequencing data in FASTQ format using git:\n# Create working directory and navigate to it\nmkdir qc && cd qc\n\n# Download sequencing reads into the 'data' directory\ngit clone https://github.com/bioinfo-dirty-jobs/Data.git data/\n\n\nProject Structure\nI often create an output directory to store all files generated by the tools I used in the analysis, with each tool having its own subdirectory. For this tutorial, we will be using fastqc, cutadapt, and fastp thus there will be a subdirectory for each tool in the output directory:\n\n\n output\n├──  cutadapt\n├──  fastp\n└──  fastqc\n    ├──  raw\n    └──  trimmed\n\n\n\n\nSoftware\nThe listed tools can be downloaded from the Bioconda repository:\n\nfastqc\ncutadapt\nfastp\nbiostrings"
  },
  {
    "objectID": "lab/quality_control/index.html#explore-the-raw-sequencing-reads",
    "href": "lab/quality_control/index.html#explore-the-raw-sequencing-reads",
    "title": "Quality Control and Adapter Trimming",
    "section": "Explore the Raw Sequencing Reads",
    "text": "Explore the Raw Sequencing Reads\nUnzip the FASTQ file using gunzip:\ngunzip data/f010_raw_mirna.fastq\nThe file f010_raw_mirna.fastq.fz contains sequencing reads from a microRNA sequencing experiment. View the first 12 rows using the head command:\n\n!head -n 12 data/f010_raw_mirna.fastq\n\n@K2N32:24:83\nGCTTACGGAACTTTTTTTAAAAAAAAAAACTAAAAAAAAAAATTTCTTCTTGCTTTT\n+\n,.,%*-*(*.1111111%*---------%**----------%*1+-*(*0+.**---\n@K2N32:26:77\nCTTCGGAATAACCTAAAAAAAA\n+\n.4(.0-//11+.(000000000\n@K2N32:28:90\nCTTACGTAATAAA\n+\n+0%-0,11-/.0'"
  },
  {
    "objectID": "lab/quality_control/index.html#workflow",
    "href": "lab/quality_control/index.html#workflow",
    "title": "Quality Control and Adapter Trimming",
    "section": "Workflow",
    "text": "Workflow\n\nExplore the Raw Sequencing Reads\nUnzip the FASTQ file using gunzip:\ngunzip data/f010_raw_mirna.fastq\nThe file f010_raw_mirna.fastq.fz contains sequencing reads from a microRNA sequencing experiment. View the first 12 rows using the head command:\n\n!head -n 12 data/f010_raw_mirna.fastq\n\n@K2N32:24:83\nGCTTACGGAACTTTTTTTAAAAAAAAAAACTAAAAAAAAAAATTTCTTCTTGCTTTT\n+\n,.,%*-*(*.1111111%*---------%**----------%*1+-*(*0+.**---\n@K2N32:26:77\nCTTCGGAATAACCTAAAAAAAA\n+\n.4(.0-//11+.(000000000\n@K2N32:28:90\nCTTACGTAATAAA\n+\n+0%-0,11-/.0'\n\n\nCount the number of reads in the FASTQ file using wc and dividing the output by 4:\n\n!wc -l data/f010_raw_mirna.fastq\n\n400000 data/f010_raw_mirna.fastq\n\n\nThe number of reads can be computed directly with the use of cut and expr:\n\n!expr $(wc -l data/f010_raw_mirna.fastq | cut -d \" \" -f 1) / 4\n\n100000\n\n\nThere are a total of 100000 reads in the FASTQ file. Information that summarizes the distribution of quality reads, base composition, and sequence representation can be generated using fastqc. We pass in the FASTQ file as the first positional argument and specify the -o flag to store our results in a designated directory.\n# Target directory for FastQC report of raw reads\nmkdir -p output/fastqc/raw\n\n# Run fastq\nfastqc data/f010_raw_mirna.fastq -o output/fastqc/raw\nThe result of running fastqc on the unprocessed reads is provided below.\n\n\n\n\nFrom initial inspection, we observe a wide spread in the distribution of quality scores. The lowest quality score is 9 and the highest is 26, with the majority of reads having a quality score of 26. The output suggests that there are issues with the following metrics:\n\nPer base sequence quality\nPer base sequence content\nPer sequence GC content\nSequence Duplication Levels\nOverrepresented sequences\n\n\n\nChecking for Adapters\nThere are 2 known adapters used in this experiment:\nCTGGGAAATCACCATAAACGTGAAATGTCTTTGGATTTGGGAATCTTATAAGTTCTGTATGAGACCACTCTAAAAA\nCTTTTTTTCGTCCTTTCCACAAGATATATAAAGCCAAGAAATCGAAATACTTTCAAGTTACGGTAAGC\nWe can verify this by querying our reads against the two adapters using grep and counting the number of their occurrence:\n\n!grep \"CTGGGAAATCACCATAAACGTGAAATGTCTTTGGATTTGGGAATCTTATAAGTTCTGTATGAGACCACTCTAAAAA\" data/f010_raw_mirna.fastq | wc -l\n\n25514\n\n\n\n!grep \"CTTTTTTTCGTCCTTTCCACAAGATATATAAAGCCAAGAAATCGAAATACTTTCAAGTTACGGTAAGC\" data/f010_raw_mirna.fastq | wc -l\n\n5708\n\n\nThe adapters seem to systematically appear at the flanking ends of the reads, suggesting that these sequences were artificially ligated. We must also check if the same set of adapters are found in the reverse, complement, or reverse complement of each read. For this, we compute the reversed and completemented reads using the R package biostrings.\nlibrary(Biostrings)\n\nadapter1 &lt;- DNAString(\"CTGGGAAATCACCATAAACGTGAAATGTCTTTGGATTTGGGAATCTTATAAGTTCTGTATGAGACCACTCTAAAAA\")\npaste(\"Adapter 1 reverse:\", reverse(adapter1))\npaste(\"Adapter 1 complement:\", complement(adapter1))\npaste(\"Adapter 1 reverse complement:\", reverseComplement(adapter1))\n\nadapter2 &lt;- DNAString(\"CTTTTTTTCGTCCTTTCCACAAGATATATAAAGCCAAGAAATCGAAATACTTTCAAGTTACGGTAAGC\")\npaste(\"Adapter 2 reverse:\", reverse(adapter2))\npaste(\"Adapter 2 complement:\", complement(adapter2))\npaste(\"Adapter 2 reverse complement:\", reverseComplement(adapter2))\nThere are two scenarios: (1) the adapter appears as a complete sequence within the read or (2) a substring of the adapter appears within the read. Hence it is usually good practice to systematically query the reads with substrings of the adapter set. This will give us information on which positions to trim the reads.\n\n\nTrimming Adapters\ncutadapt is a tool for trimming adapter sequences from reads. We specify the 3’ adapter using the -a flag and the 5’ adapter using the -g flag. The FASTQ file to be processed is passed as a positional argument.\n# Trim 3' and 5' adapters\ncutadapt data/f010_raw_mirna.fastq \\\n1  -a TTTTTAGAGTGGTCTCATACAGAACTTATAAGATTCCCAAATCCAAAGACATTTCACGTTTATGGTGATTTCCCAG \\\n2  -g GCTTACCGTAACTTGAAAGTATTTCGATTTCTTGGCTTTATATATCTTGTGGAAAGGACGAAAAAAAG \\\n3  -o output/cutadapt/f010_mirna.trimmed.1.fastq \\\n\n1\n\nTrim 3’ adapters\n\n2\n\nTrim 5’ adapters\n\n3\n\nSave trimmed reads to destination file\n\n\nWe then filter by read length by specifying the minimum length (-m) and maximum length (-M). In the same command, provide a threshold for filtering out reads by quality score (-q).\n# Filter reads by lengths and quality score\ncutadapt output/cutadapt/f010_mirna.trimmed.1.fastq \\\n1  -m 17 -M 30 \\\n2  -q 20 \\\n3  -o output/cutadapt/f010_mirna.trimmed.2.fastq \\\n4  --json output/cutadapt/report.json\n\n1\n\nFilter by minimum and maximum lengths, respectively\n\n2\n\nFilter by minimum quality score\n\n3\n\nSave trimmed reads to destination file\n\n4\n\nSave report in JSON format\n\n\nAs a form of due dilligence, we ensure that the adapter set (along with its substrings) are not present in the trimmed dataset using grep.\n# Check for the presence of adapter 1 and its substrings\ngrep CTGGGAAATCACCATAAACGTGAAATGTCTTTGGATTTGGGAATCTTATAAGTTCTGTATGAGACCACTCTAAAAA output/cutadapt/f010_mirna.trimmed.fastq | wc -l\ngrep GACCCTTTAGTGGTATTTGCACTTTACAGAAACCTAAACCCTTAGAATATTCAAGACATACTCTGGTGAGATTTTT output/cutadapt/f010_mirna.trimmed.fastq | wc -l \ngrep TTTTTAGAGTGGTCTCATACAGAACTTATAAGATTCCCAAATCCAAAGACATTTCACGTTTATGGTGATTTCCCAG output/cutadapt/f010_mirna.trimmed.fastq | wc -l\ngrep AAAAATCTCACCAGAGTATGTCTTGAATATTCTAAGGGTTTAGGTTTCTGTAAAGTGCAAATACCACTAAAGGGTC output/cutadapt/f010_mirna.trimmed.fastq | wc -l\n# Check for the presence of adapter 2 and its substrings\ngrep CTTTTTTTCGTCCTTTCCACAAGATATATAAAGCCAAGAAATCGAAATACTTTCAAGTTACGGTAAGC output/cutadapt/f010_mirna.trimmed.fastq | wc -l\ngrep GAAAAAAAGCAGGAAAGGTGTTCTATATATTTCGGTTCTTTAGCTTTATGAAAGTTCAATGCCATTCG output/cutadapt/f010_mirna.trimmed.fastq | wc -l \ngrep GCTTACCGTAACTTGAAAGTATTTCGATTTCTTGGCTTTATATATCTTGTGGAAAGGACGAAAAAAAG output/cutadapt/f010_mirna.trimmed.fastq | wc -l\ngrep CGAATGGCATTGAACTTTCATAAAGCTAAAGAACCGAAATATATAGAACACCTTTCCTGCTTTTTTTC output/cutadapt/f010_mirna.trimmed.fastq | wc -l \nA summary report is printed to the standard output of the terminal. It should resemble the following:\n=== Summary ===\n\nTotal reads processed:                 100,000\nReads with adapters:                    80,297 (80.3%)\nReverse-complemented:                   42,949 (42.9%)\n\n== Read fate breakdown ==\nReads that were too short:              28,068 (28.1%)\nReads that were too long:               63,771 (63.8%)\nReads written (passing filters):         8,161 (8.2%)\n\nTotal basepairs processed:    11,428,754 bp\nQuality-trimmed:                  66,003 bp (0.6%)\nTotal written (filtered):        180,401 bp (1.6%)\n\n\nExplore the Trimmed Sequencing Reads\nWe then re-run fastqc on the trimmed reads to verify if the matched adapter sets were removed.\n# Output directory for report on trimmed reads\nmkdir -p output/fastqc/trimmed\n\n# Run fastqc on trimmed reads\nfastqc -o output/fastqc/trimmed output/cutadapt/f010_mirna.trimmed.fastq \nThe resulting report can be viewed below:\n\n\n\n\nThe report confirms that reads with quality scores below 20 have been removed and only the first 30 bases were kept per read. The overrepresented sequences still persist and represent the miRNA sequences. Confirm this by querying the overrepresented sequences against an updated miRNA database.\n\n\nRead Trimming and Filtering Using fastp\nfastp is an all-in-one tool designed for preprocessing and quality control of FASTQ data. It automatically detects adapter sequences in the 3’ and 5’ ends of the reads and performs filtering based on quality score and read length. Furthermore, it supports preprocessing of long reads (e.g., PacBio, Nanopore) and implements parallel compute. Similar to fastqc, the program generates a summary report in HTML format.\nSimply provide the raw read data as input and specify an output directory to store the report. We could explicitly provide the adapter sequences using the -a flag. The minimum length, maximum length, and quality score threshold are given by the --length_required, --length_limit, and -q flags, respectively.\n1fastp -i data/f010_raw_mirna.fastq -o output/fastp/f010_mirna.trimmed.fastq \\\n2  --length_required 30 --length_limit 60 \\\n3  -q 20 \\\n4  --html output/fastp/fastp_report.html \\\n5  --json output/fastp/fastp_report.json\n\n1\n\nProcess raw reads and save output to specified path\n\n2\n\nSpecify the minimum and maximum read length, respectively\n\n3\n\nSpecift quality score threshold\n\n4\n\nGenerate an HTML report\n\n5\n\nGenerate a JSON report\n\n\nThe summary report generated by fastp is minimal and simplistic. An example is provided below, giving us information on both raw and processed data:"
  },
  {
    "objectID": "lab/quality_control/index.html#bringing-it-all-together",
    "href": "lab/quality_control/index.html#bringing-it-all-together",
    "title": "Quality Control and Adapter Trimming",
    "section": "Bringing It All Together",
    "text": "Bringing It All Together\nThe entire workflow can be replicated by running the bash script provided below.\n# Print commands and exit on error\nset -uex\n\n# Create working directory and navigate to it\nmkdir qc && cd qc\n\n# Download sequencing reads into the 'data' directory\ngit clone https://github.com/bioinfo-dirty-jobs/Data.git data/\n\n# Decompress reads with gunzip\ngunzip data/QUALITY/quality_control/f010_raw_mirna.fastqc.gz\n\n# Store path to reads as variable\nREADS=data/QUALITY/quality_control/f010_raw_mirna.fastqc\n\n# Create subdirectories for outputs\nmkdir -p output/{fastqc,cutadapt,fastp}\nmkdir -o output/fasqc/{raw,trimmed}\n\n# Run fastq on raw reads\nfastqc ${READS} -o output/fastqc/raw\n\n# Trim adapters using cutadapt\ncutadapt data/f010_raw_mirna.fastq \\\n  -a TTTTTAGAGTGGTCTCATACAGAACTTATAAGATTCCCAAATCCAAAGACATTTCACGTTTATGGTGATTTCCCAG \\\n  -g GCTTACCGTAACTTGAAAGTATTTCGATTTCTTGGCTTTATATATCTTGTGGAAAGGACGAAAAAAAG \\\n  -o output/cutadapt/f010_mirna.trimmed.1.fastq\n\n# Filter reads by length and quality score using cutadapt\ncutadapt output/cutadapt/f010_mirna.trimmed.1.fastq \\\n  -m 17 -M 30 \\\n  -q 20 \\\n  -o output/cutadapt/f010_mirna.trimmed.2.fastq \\\n  --json output/cutadapt/report.json\n\n# Re-run fastqc on trimmed reads\nfastqc -o output/fastqc/trimmed output/cutadapt/f010_mirna.trimmed.fastq \n\n# Trim and filter using fastp\nfastp -i data/f010_raw_mirna.fastq -o output/fastp/f010_mirna.trimmed.fastq \\\n  --length_required 30 --length_limit 60 \\\n  -q 20 \\\n  --html output/fastp/fastp_report.html \\\n  --json output/fastp/fastp_report.json"
  },
  {
    "objectID": "lab/quality_control/index.html#complete-preprocessing-using-fastp",
    "href": "lab/quality_control/index.html#complete-preprocessing-using-fastp",
    "title": "Quality Control and Adapter Trimming",
    "section": "Complete Preprocessing using fastp",
    "text": "Complete Preprocessing using fastp\nfastp is an all-in-one tool designed for preprocessing and quality control of FASTQ data. It automatically detects adapter sequences in the 3’ and 5’ ends of the reads and performs filtering based on quality score and read length. Furthermore, it supports preprocessing of long reads (e.g., PacBio, Nanopore) and implements parallel compute. Similar to fastqc, the program generates a summary report in HTML format.\nSimply provide the raw read data as input and specify an output directory to store the report. We could explicitly provide the adapter sequences using the -a flag. The minimum length, maximum length, and quality score threshold are given by the --length_required, --length_limit, and -q flags, respectively.\n1fastp -i data/f010_raw_mirna.fastq -o output/fastp/f010_mirna.trimmed.fastq \\\n2  --length_required 30 --length_limit 60 \\\n3  -q 20 \\\n4  --html output/fastp/fastp_report.html \\\n5  --json output/fastp/fastp_report.json\n\n1\n\nProcess raw reads and save output to specified path\n\n2\n\nSpecify the minimum and maximum read length, respectively\n\n3\n\nSpecift quality score threshold\n\n4\n\nGenerate an HTML report\n\n5\n\nGenerate a JSON report\n\n\nThe summary report generated by fastp is minimal and simplistic. An example is provided below, giving us information on both raw and processed data:"
  },
  {
    "objectID": "lab/quality_control/index.html#read-trimming-and-filtering-using-fastp",
    "href": "lab/quality_control/index.html#read-trimming-and-filtering-using-fastp",
    "title": "Quality Control and Adapter Trimming",
    "section": "Read Trimming and Filtering Using fastp",
    "text": "Read Trimming and Filtering Using fastp\nfastp is an all-in-one tool designed for preprocessing and quality control of FASTQ data. It automatically detects adapter sequences in the 3’ and 5’ ends of the reads and performs filtering based on quality score and read length. Furthermore, it supports preprocessing of long reads (e.g., PacBio, Nanopore) and implements parallel compute. Similar to fastqc, the program generates a summary report in HTML format.\nSimply provide the raw read data as input and specify an output directory to store the report. We could explicitly provide the adapter sequences using the -a flag. The minimum length, maximum length, and quality score threshold are given by the --length_required, --length_limit, and -q flags, respectively.\n1fastp -i data/f010_raw_mirna.fastq -o output/fastp/f010_mirna.trimmed.fastq \\\n2  --length_required 30 --length_limit 60 \\\n3  -q 20 \\\n4  --html output/fastp/fastp_report.html \\\n5  --json output/fastp/fastp_report.json\n\n1\n\nProcess raw reads and save output to specified path\n\n2\n\nSpecify the minimum and maximum read length, respectively\n\n3\n\nSpecift quality score threshold\n\n4\n\nGenerate an HTML report\n\n5\n\nGenerate a JSON report\n\n\nThe summary report generated by fastp is minimal and simplistic. An example is provided below, giving us information on both raw and processed data:"
  }
]